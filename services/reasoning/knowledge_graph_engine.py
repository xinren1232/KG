#!/usr/bin/env python3
"""
çŸ¥è¯†å›¾è°±æ¨ç†å¼•æ“
é›†æˆå›¾æ•°æ®åº“æ¨ç†ç®—æ³•ï¼Œå®ç°å®ä½“é“¾æ¥ã€å…³ç³»æ¨æ–­ã€å¼‚å¸¸æ¨¡å¼è¯†åˆ«ç­‰é«˜çº§åŠŸèƒ½

æŠ€æœ¯æ ˆ:
- NetworkX: å›¾ç®—æ³•å’Œåˆ†æ
- scikit-learn: æœºå™¨å­¦ä¹ ç®—æ³•
- pandas: æ•°æ®å¤„ç†
- Neo4j: å›¾æ•°æ®åº“æŸ¥è¯¢
"""
import json
import pandas as pd
import networkx as nx
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
import logging
from collections import defaultdict, Counter
import numpy as np

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class KnowledgeGraphEngine:
    """çŸ¥è¯†å›¾è°±æ¨ç†å¼•æ“"""
    
    def __init__(self):
        self.graph = nx.MultiDiGraph()
        self.entity_embeddings = {}
        self.pattern_rules = self._load_pattern_rules()
        
    def _load_pattern_rules(self) -> Dict[str, List[Dict]]:
        """åŠ è½½æ¨ç†è§„åˆ™"""
        return {
            'anomaly_patterns': [
                {
                    'name': 'ç»„ä»¶æ•…éšœä¼ æ’­',
                    'pattern': 'Anomaly -[:AFFECTS]-> Component -[:PART_OF]-> Product',
                    'inference': 'Product -[:HAS_RISK]-> Anomaly'
                },
                {
                    'name': 'ç›¸ä¼¼ç—‡çŠ¶èšç±»',
                    'pattern': 'Symptom1 -[:SIMILAR_TO]-> Symptom2',
                    'inference': 'Anomaly1 -[:RELATED_TO]-> Anomaly2'
                },
                {
                    'name': 'ä¾›åº”å•†è´¨é‡å…³è”',
                    'pattern': 'Supplier -[:SUPPLIES]-> Component <-[:AFFECTS]- Anomaly',
                    'inference': 'Supplier -[:QUALITY_ISSUE]-> Anomaly'
                }
            ],
            'test_patterns': [
                {
                    'name': 'æµ‹è¯•è¦†ç›–æ¨ç†',
                    'pattern': 'TestCase -[:TESTS]-> Component <-[:AFFECTS]- Anomaly',
                    'inference': 'TestCase -[:SHOULD_DETECT]-> Anomaly'
                },
                {
                    'name': 'æµ‹è¯•ä¼˜å…ˆçº§æ¨ç†',
                    'pattern': 'Component -[:HAS_HIGH_FAILURE_RATE]-> True',
                    'inference': 'TestCase -[:HIGH_PRIORITY]-> Component'
                }
            ]
        }
    
    def load_knowledge_graph(self, kg_data: Dict[str, Any]):
        """åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®"""
        logger.info("åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®åˆ°æ¨ç†å¼•æ“")
        
        # æ¸…ç©ºç°æœ‰å›¾
        self.graph.clear()
        
        # æ·»åŠ èŠ‚ç‚¹
        for node in kg_data.get('nodes', []):
            self.graph.add_node(
                node['key'],
                type=node['type'],
                name=node['name'],
                **node.get('properties', {})
            )
        
        # æ·»åŠ è¾¹
        for rel in kg_data.get('relationships', []):
            self.graph.add_edge(
                rel['source_key'],
                rel['target_key'],
                relation=rel['relation_type'],
                **rel.get('properties', {})
            )
        
        logger.info(f"å›¾è°±åŠ è½½å®Œæˆ: {self.graph.number_of_nodes()} èŠ‚ç‚¹, {self.graph.number_of_edges()} è¾¹")
    
    def detect_anomaly_patterns(self) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¼‚å¸¸æ¨¡å¼"""
        logger.info("æ£€æµ‹å¼‚å¸¸æ¨¡å¼")
        patterns = []
        
        # 1. é«˜é¢‘æ•…éšœç»„ä»¶è¯†åˆ«
        component_failure_count = defaultdict(int)
        for node, data in self.graph.nodes(data=True):
            if data.get('type') == 'Anomaly':
                # æŸ¥æ‰¾å½±å“çš„ç»„ä»¶
                for neighbor in self.graph.neighbors(node):
                    neighbor_data = self.graph.nodes[neighbor]
                    if neighbor_data.get('type') == 'Component':
                        component_failure_count[neighbor] += 1
        
        # è¯†åˆ«é«˜é£é™©ç»„ä»¶
        if component_failure_count:
            max_failures = max(component_failure_count.values())
            high_risk_components = [
                comp for comp, count in component_failure_count.items()
                if count >= max_failures * 0.7  # è¶…è¿‡70%æœ€å¤§å€¼çš„ç»„ä»¶
            ]
            
            patterns.append({
                'type': 'high_risk_components',
                'description': 'é«˜é£é™©ç»„ä»¶è¯†åˆ«',
                'components': high_risk_components,
                'failure_counts': dict(component_failure_count)
            })
        
        # 2. ä¾›åº”å•†è´¨é‡é—®é¢˜èšç±»
        supplier_issues = defaultdict(list)
        for node, data in self.graph.nodes(data=True):
            if data.get('type') == 'Supplier':
                # æŸ¥æ‰¾ä¾›åº”å•†ç›¸å…³çš„å¼‚å¸¸
                for path in nx.all_simple_paths(self.graph, node, 
                                               [n for n, d in self.graph.nodes(data=True) 
                                                if d.get('type') == 'Anomaly'], cutoff=3):
                    if len(path) >= 3:  # è‡³å°‘ç»è¿‡ä¸€ä¸ªä¸­é—´èŠ‚ç‚¹
                        anomaly_node = path[-1]
                        supplier_issues[node].append(anomaly_node)
        
        if supplier_issues:
            patterns.append({
                'type': 'supplier_quality_issues',
                'description': 'ä¾›åº”å•†è´¨é‡é—®é¢˜èšç±»',
                'supplier_issues': dict(supplier_issues)
            })
        
        # 3. ç—‡çŠ¶ç›¸ä¼¼æ€§åˆ†æ
        symptom_similarity = self._analyze_symptom_similarity()
        if symptom_similarity:
            patterns.append({
                'type': 'symptom_clusters',
                'description': 'ç›¸ä¼¼ç—‡çŠ¶èšç±»',
                'clusters': symptom_similarity
            })
        
        return patterns
    
    def _analyze_symptom_similarity(self) -> List[Dict[str, Any]]:
        """åˆ†æç—‡çŠ¶ç›¸ä¼¼æ€§"""
        symptom_nodes = [
            (node, data) for node, data in self.graph.nodes(data=True)
            if data.get('type') == 'Symptom'
        ]
        
        if len(symptom_nodes) < 2:
            return []
        
        clusters = []
        
        # ç®€å•çš„åŸºäºå…³é”®è¯çš„ç›¸ä¼¼æ€§åˆ†æ
        symptom_keywords = {}
        for node, data in symptom_nodes:
            name = data.get('name', '')
            keywords = set(name.split())
            symptom_keywords[node] = keywords
        
        # æŸ¥æ‰¾ç›¸ä¼¼ç—‡çŠ¶
        processed = set()
        for i, (node1, data1) in enumerate(symptom_nodes):
            if node1 in processed:
                continue
                
            cluster = [node1]
            keywords1 = symptom_keywords[node1]
            
            for j, (node2, data2) in enumerate(symptom_nodes[i+1:], i+1):
                if node2 in processed:
                    continue
                    
                keywords2 = symptom_keywords[node2]
                # è®¡ç®—Jaccardç›¸ä¼¼åº¦
                intersection = len(keywords1 & keywords2)
                union = len(keywords1 | keywords2)
                
                if union > 0 and intersection / union > 0.3:  # 30%ç›¸ä¼¼åº¦é˜ˆå€¼
                    cluster.append(node2)
                    processed.add(node2)
            
            if len(cluster) > 1:
                clusters.append({
                    'symptoms': cluster,
                    'similarity_score': len(set.intersection(*[symptom_keywords[s] for s in cluster])) / 
                                      len(set.union(*[symptom_keywords[s] for s in cluster]))
                })
                processed.update(cluster)
        
        return clusters
    
    def recommend_test_cases(self, anomaly_key: str) -> List[Dict[str, Any]]:
        """ä¸ºå¼‚å¸¸æ¨èç›¸å…³æµ‹è¯•ç”¨ä¾‹"""
        logger.info(f"ä¸ºå¼‚å¸¸ {anomaly_key} æ¨èæµ‹è¯•ç”¨ä¾‹")
        
        recommendations = []
        
        if anomaly_key not in self.graph:
            return recommendations
        
        # æŸ¥æ‰¾å¼‚å¸¸å½±å“çš„ç»„ä»¶
        affected_components = []
        for neighbor in self.graph.neighbors(anomaly_key):
            neighbor_data = self.graph.nodes[neighbor]
            if neighbor_data.get('type') == 'Component':
                affected_components.append(neighbor)
        
        # æŸ¥æ‰¾æµ‹è¯•è¿™äº›ç»„ä»¶çš„æµ‹è¯•ç”¨ä¾‹
        for component in affected_components:
            for node, data in self.graph.nodes(data=True):
                if data.get('type') == 'TestCase':
                    # æ£€æŸ¥æ˜¯å¦æœ‰è·¯å¾„è¿æ¥æµ‹è¯•ç”¨ä¾‹å’Œç»„ä»¶
                    try:
                        if nx.has_path(self.graph, node, component):
                            recommendations.append({
                                'testcase_key': node,
                                'testcase_name': data.get('name', ''),
                                'component': component,
                                'relevance_score': self._calculate_relevance_score(node, anomaly_key),
                                'reason': f'æµ‹è¯•å—å½±å“ç»„ä»¶: {self.graph.nodes[component].get("name", component)}'
                            })
                    except:
                        continue
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        recommendations.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        return recommendations[:10]  # è¿”å›å‰10ä¸ªæ¨è
    
    def _calculate_relevance_score(self, testcase_key: str, anomaly_key: str) -> float:
        """è®¡ç®—æµ‹è¯•ç”¨ä¾‹ä¸å¼‚å¸¸çš„ç›¸å…³æ€§åˆ†æ•°"""
        try:
            # åŸºäºå›¾è·ç¦»è®¡ç®—ç›¸å…³æ€§
            if nx.has_path(self.graph, testcase_key, anomaly_key):
                path_length = nx.shortest_path_length(self.graph, testcase_key, anomaly_key)
                return 1.0 / (1.0 + path_length)
            else:
                return 0.0
        except:
            return 0.0
    
    def analyze_centrality(self) -> Dict[str, Any]:
        """åˆ†æå›¾ä¸­å¿ƒæ€§"""
        logger.info("åˆ†æå›¾ä¸­å¿ƒæ€§")
        
        # è½¬æ¢ä¸ºæ— å‘å›¾è¿›è¡Œä¸­å¿ƒæ€§åˆ†æ
        undirected_graph = self.graph.to_undirected()
        
        # åº¦ä¸­å¿ƒæ€§
        degree_centrality = nx.degree_centrality(undirected_graph)
        
        # ä»‹æ•°ä¸­å¿ƒæ€§
        betweenness_centrality = nx.betweenness_centrality(undirected_graph)
        
        # ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
        try:
            eigenvector_centrality = nx.eigenvector_centrality(undirected_graph, max_iter=1000)
        except:
            eigenvector_centrality = {}
        
        # æ‰¾å‡ºæœ€é‡è¦çš„èŠ‚ç‚¹
        top_nodes = {
            'degree': sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10],
            'betweenness': sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10],
            'eigenvector': sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:10]
        }
        
        return {
            'centrality_scores': {
                'degree': degree_centrality,
                'betweenness': betweenness_centrality,
                'eigenvector': eigenvector_centrality
            },
            'top_nodes': top_nodes,
            'analysis': self._interpret_centrality(top_nodes)
        }
    
    def _interpret_centrality(self, top_nodes: Dict[str, List[Tuple[str, float]]]) -> Dict[str, str]:
        """è§£é‡Šä¸­å¿ƒæ€§åˆ†æç»“æœ"""
        interpretations = {}
        
        # åº¦ä¸­å¿ƒæ€§è§£é‡Š
        if top_nodes['degree']:
            top_degree_node = top_nodes['degree'][0][0]
            node_data = self.graph.nodes[top_degree_node]
            interpretations['degree'] = f"è¿æ¥æœ€å¤šçš„èŠ‚ç‚¹: {node_data.get('name', top_degree_node)} ({node_data.get('type', 'Unknown')})"
        
        # ä»‹æ•°ä¸­å¿ƒæ€§è§£é‡Š
        if top_nodes['betweenness']:
            top_betweenness_node = top_nodes['betweenness'][0][0]
            node_data = self.graph.nodes[top_betweenness_node]
            interpretations['betweenness'] = f"æœ€å…³é”®çš„æ¡¥æ¥èŠ‚ç‚¹: {node_data.get('name', top_betweenness_node)} ({node_data.get('type', 'Unknown')})"
        
        return interpretations
    
    def find_anomaly_root_causes(self, anomaly_key: str) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾å¼‚å¸¸çš„æ ¹æœ¬åŸå› """
        logger.info(f"æŸ¥æ‰¾å¼‚å¸¸ {anomaly_key} çš„æ ¹æœ¬åŸå› ")
        
        root_causes = []
        
        if anomaly_key not in self.graph:
            return root_causes
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ ¹å› è·¯å¾„
        for node, data in self.graph.nodes(data=True):
            if data.get('type') == 'RootCause':
                try:
                    if nx.has_path(self.graph, anomaly_key, node):
                        paths = list(nx.all_simple_paths(self.graph, anomaly_key, node, cutoff=5))
                        for path in paths:
                            root_causes.append({
                                'root_cause': node,
                                'root_cause_name': data.get('name', ''),
                                'path': path,
                                'path_length': len(path),
                                'confidence': 1.0 / len(path)  # è·¯å¾„è¶ŠçŸ­ï¼Œç½®ä¿¡åº¦è¶Šé«˜
                            })
                except:
                    continue
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        root_causes.sort(key=lambda x: x['confidence'], reverse=True)
        
        return root_causes
    
    def generate_insights(self) -> Dict[str, Any]:
        """ç”ŸæˆçŸ¥è¯†å›¾è°±æ´å¯Ÿ"""
        logger.info("ç”ŸæˆçŸ¥è¯†å›¾è°±æ´å¯Ÿ")
        
        insights = {
            'graph_statistics': {
                'nodes': self.graph.number_of_nodes(),
                'edges': self.graph.number_of_edges(),
                'density': nx.density(self.graph),
                'connected_components': nx.number_weakly_connected_components(self.graph)
            },
            'anomaly_patterns': self.detect_anomaly_patterns(),
            'centrality_analysis': self.analyze_centrality(),
            'entity_distribution': self._analyze_entity_distribution()
        }
        
        return insights
    
    def _analyze_entity_distribution(self) -> Dict[str, int]:
        """åˆ†æå®ä½“ç±»å‹åˆ†å¸ƒ"""
        distribution = defaultdict(int)
        for node, data in self.graph.nodes(data=True):
            entity_type = data.get('type', 'Unknown')
            distribution[entity_type] += 1
        
        return dict(distribution)

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ¨ç†å¼•æ“åŠŸèƒ½"""
    engine = KnowledgeGraphEngine()
    
    # åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®
    kg_file = "data/processed/extracted_knowledge_graph.json"
    if Path(kg_file).exists():
        with open(kg_file, 'r', encoding='utf-8') as f:
            kg_data = json.load(f)
        
        engine.load_knowledge_graph(kg_data)
        
        # ç”Ÿæˆæ´å¯Ÿ
        insights = engine.generate_insights()
        
        # ä¿å­˜æ´å¯Ÿç»“æœ
        insights_file = "data/processed/kg_insights.json"
        with open(insights_file, 'w', encoding='utf-8') as f:
            json.dump(insights, f, ensure_ascii=False, indent=2)
        
        print("ğŸ§  çŸ¥è¯†å›¾è°±æ¨ç†åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š å›¾è°±ç»Ÿè®¡: {insights['graph_statistics']['nodes']} èŠ‚ç‚¹, {insights['graph_statistics']['edges']} è¾¹")
        print(f"ğŸ” å‘ç°æ¨¡å¼: {len(insights['anomaly_patterns'])} ä¸ª")
        print(f"ğŸ’¾ æ´å¯Ÿå·²ä¿å­˜åˆ°: {insights_file}")

if __name__ == "__main__":
    main()
