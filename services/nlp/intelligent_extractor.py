#!/usr/bin/env python3
"""
æ™ºèƒ½æ•°æ®æŠ½å–å™¨
é›†æˆå¤šä¸ªå¼€æºNLPæŠ€æœ¯æ ˆå®ç°è‡ªåŠ¨åŒ–æ•°æ®æŠ½å–å’ŒçŸ¥è¯†å›¾è°±æ„å»º

æŠ€æœ¯æ ˆ:
- spaCy: ä¸­æ–‡NERå’Œæ–‡æœ¬å¤„ç†
- transformers: BERTå®ä½“æŠ½å–
- sentence-transformers: è¯­ä¹‰ç›¸ä¼¼åº¦
- LangChain: çŸ¥è¯†å›¾è°±æ„å»º
"""
import re
import json
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntelligentExtractor:
    """æ™ºèƒ½æ•°æ®æŠ½å–å™¨"""
    
    def __init__(self):
        self.entity_patterns = self._load_entity_patterns()
        self.field_mappings = self._load_field_mappings()
        self.nlp_models = {}
        
    def _load_entity_patterns(self) -> Dict[str, List[str]]:
        """åŠ è½½å®ä½“è¯†åˆ«æ¨¡å¼"""
        return {
            'AnomalyID': [
                r'[A-Z]{2,4}-\d{4}-\d{3}',  # IQC-2024-001
                r'é—®é¢˜ç¼–å·',
                r'ç¼ºé™·.*ç¼–å·',
                r'å¼‚å¸¸.*ç¼–å·'
            ],
            'Product': [
                r'MyPhone[A-Z]',
                r'äº§å“.*å‹å·',
                r'æœºå‹',
                r'å½±å“.*äº§å“'
            ],
            'Component': [
                r'æ‘„åƒå¤´|ç›¸æœº|é•œå¤´',
                r'ç”µæ± |ç”µèŠ¯',
                r'æ˜¾ç¤ºå±|å±å¹•|LCD|OLED',
                r'è§¦æ‘¸å±|è§¦æ§',
                r'æ‰¬å£°å™¨|å–‡å­',
                r'ç‰©æ–™.*åç§°',
                r'ç»„ä»¶|æ¨¡å—|éƒ¨ä»¶'
            ],
            'Supplier': [
                r'.*æœ‰é™å…¬å¸',
                r'.*è‚¡ä»½.*å…¬å¸',
                r'.*ç§‘æŠ€.*å…¬å¸',
                r'ä¾›åº”å•†'
            ],
            'Severity': [
                r'S[1-4]',
                r'ä¸¥é‡.*ç¨‹åº¦',
                r'ä¼˜å…ˆçº§',
                r'P[1-4]'
            ],
            'Status': [
                r'å¤„ç†ä¸­|å·²å…³é—­|åˆ†æä¸­|å¾…å¤„ç†',
                r'å¤„ç†.*çŠ¶æ€',
                r'çŠ¶æ€'
            ],
            'Symptom': [
                r'å¯¹ç„¦.*å¼‚å¸¸|å¯¹ç„¦.*å¤±è´¥',
                r'å……ç”µ.*æ…¢|å……ç”µ.*å¼‚å¸¸',
                r'è‰²å½©.*åå·®|æ˜¾ç¤º.*å¼‚å¸¸',
                r'è§¦æ‘¸.*ä¸çµæ•|å“åº”.*å¼‚å¸¸',
                r'éŸ³è´¨.*å¼‚å¸¸|æ‚éŸ³',
                r'é—®é¢˜.*æè¿°',
                r'ç—‡çŠ¶|ç°è±¡'
            ],
            'RootCause': [
                r'å·¥è‰º.*é—®é¢˜',
                r'å†…é˜».*åé«˜',
                r'è‰²æ¸©.*åå·®',
                r'å¯¼ç”µå±‚.*ç¼ºé™·',
                r'ç£åŠ›.*ä¸è¶³',
                r'æ ¹æœ¬.*åŸå› ',
                r'æ ¹å› '
            ]
        }
    
    def _load_field_mappings(self) -> Dict[str, str]:
        """åŠ è½½å­—æ®µæ˜ å°„å…³ç³»"""
        return {
            # ä¸­æ–‡å­—æ®µ -> æ ‡å‡†è‹±æ–‡å­—æ®µ
            'é—®é¢˜ç¼–å·': 'anomaly_key',
            'æ¥æ–™æ‰¹æ¬¡': 'batch_number',
            'ä¾›åº”å•†': 'supplier',
            'ç‰©æ–™åç§°': 'component',
            'ç‰©æ–™å‹å·': 'component_model',
            'é—®é¢˜æè¿°': 'symptom',
            'é—®é¢˜åˆ†ç±»': 'category',
            'ä¸¥é‡ç¨‹åº¦': 'severity',
            'å‘ç°æ—¶é—´': 'discovered_at',
            'å‘ç°äººå‘˜': 'discovered_by',
            'å½±å“äº§å“': 'product',
            'å½±å“æ•°é‡': 'affected_quantity',
            'å¤„ç†çŠ¶æ€': 'status',
            'æ ¹æœ¬åŸå› ': 'root_cause',
            'çº æ­£æªæ–½': 'corrective_action',
            'é¢„é˜²æªæ–½': 'preventive_action',
            'å…³é—­æ—¶é—´': 'closed_at',
            'å¤‡æ³¨': 'notes',
            
            # æµ‹è¯•ç”¨ä¾‹å­—æ®µ
            'ç”¨ä¾‹ç¼–å·': 'testcase_key',
            'ç”¨ä¾‹åç§°': 'title',
            'æµ‹è¯•æ¨¡å—': 'component',
            'ä¼˜å…ˆçº§': 'priority',
            'æµ‹è¯•æ­¥éª¤': 'steps',
            'é¢„æœŸç»“æœ': 'expected_result',
            'å…³è”äº§å“': 'related_products'
        }
    
    def extract_entities_from_text(self, text: str) -> Dict[str, List[str]]:
        """ä»æ–‡æœ¬ä¸­æŠ½å–å®ä½“"""
        if not text or pd.isna(text):
            return {}
            
        entities = {}
        text_str = str(text)
        
        for entity_type, patterns in self.entity_patterns.items():
            matches = []
            for pattern in patterns:
                found = re.findall(pattern, text_str, re.IGNORECASE)
                matches.extend(found)
            
            if matches:
                entities[entity_type] = list(set(matches))  # å»é‡
                
        return entities
    
    def normalize_field_names(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–å­—æ®µåç§°"""
        normalized_df = df.copy()
        
        # é‡å‘½ååˆ—
        rename_mapping = {}
        for col in df.columns:
            if col in self.field_mappings:
                rename_mapping[col] = self.field_mappings[col]
            else:
                # å°è¯•æ¨¡ç³ŠåŒ¹é…
                normalized_name = self._fuzzy_match_field(col)
                if normalized_name:
                    rename_mapping[col] = normalized_name
        
        if rename_mapping:
            normalized_df = normalized_df.rename(columns=rename_mapping)
            logger.info(f"å­—æ®µé‡å‘½å: {rename_mapping}")
        
        return normalized_df
    
    def _fuzzy_match_field(self, field_name: str) -> Optional[str]:
        """æ¨¡ç³ŠåŒ¹é…å­—æ®µå"""
        field_lower = field_name.lower()
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        if any(keyword in field_lower for keyword in ['ç¼–å·', 'id', 'key']):
            if 'é—®é¢˜' in field_lower or 'å¼‚å¸¸' in field_lower:
                return 'anomaly_key'
            elif 'ç”¨ä¾‹' in field_lower or 'test' in field_lower:
                return 'testcase_key'
        
        if any(keyword in field_lower for keyword in ['äº§å“', 'product', 'æœºå‹']):
            return 'product'
        
        if any(keyword in field_lower for keyword in ['ç»„ä»¶', 'component', 'æ¨¡å—', 'éƒ¨ä»¶']):
            return 'component'
        
        if any(keyword in field_lower for keyword in ['ç—‡çŠ¶', 'symptom', 'é—®é¢˜', 'æè¿°']):
            return 'symptom'
        
        if any(keyword in field_lower for keyword in ['ä¸¥é‡', 'severity', 'çº§åˆ«']):
            return 'severity'
        
        return None
    
    def extract_relationships(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """æŠ½å–å®ä½“é—´å…³ç³»"""
        relationships = []
        
        for _, row in df.iterrows():
            # å¼‚å¸¸ -> ç»„ä»¶å…³ç³»
            if 'anomaly_key' in row and 'component' in row:
                if pd.notna(row['anomaly_key']) and pd.notna(row['component']):
                    relationships.append({
                        'source': row['anomaly_key'],
                        'source_type': 'Anomaly',
                        'target': row['component'],
                        'target_type': 'Component',
                        'relation': 'AFFECTS'
                    })
            
            # å¼‚å¸¸ -> ç—‡çŠ¶å…³ç³»
            if 'anomaly_key' in row and 'symptom' in row:
                if pd.notna(row['anomaly_key']) and pd.notna(row['symptom']):
                    relationships.append({
                        'source': row['anomaly_key'],
                        'source_type': 'Anomaly',
                        'target': row['symptom'],
                        'target_type': 'Symptom',
                        'relation': 'HAS_SYMPTOM'
                    })
            
            # äº§å“ -> ç»„ä»¶å…³ç³»
            if 'product' in row and 'component' in row:
                if pd.notna(row['product']) and pd.notna(row['component']):
                    # å¤„ç†å¤šä¸ªäº§å“çš„æƒ…å†µ
                    products = str(row['product']).split(',')
                    for product in products:
                        product = product.strip()
                        if product:
                            relationships.append({
                                'source': product,
                                'source_type': 'Product',
                                'target': row['component'],
                                'target_type': 'Component',
                                'relation': 'INCLUDES'
                            })
            
            # å¼‚å¸¸ -> æ ¹å› å…³ç³»
            if 'anomaly_key' in row and 'root_cause' in row:
                if pd.notna(row['anomaly_key']) and pd.notna(row['root_cause']):
                    relationships.append({
                        'source': row['anomaly_key'],
                        'source_type': 'Anomaly',
                        'target': row['root_cause'],
                        'target_type': 'RootCause',
                        'relation': 'CAUSED_BY'
                    })
        
        return relationships
    
    def process_excel_file(self, file_path: str) -> Dict[str, Any]:
        """å¤„ç†Excelæ–‡ä»¶çš„å®Œæ•´æµç¨‹"""
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
        
        try:
            # 1. è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(file_path)
            logger.info(f"è¯»å–åˆ° {len(df)} è¡Œæ•°æ®ï¼Œ{len(df.columns)} åˆ—")
            
            # 2. æ ‡å‡†åŒ–å­—æ®µå
            normalized_df = self.normalize_field_names(df)
            
            # 3. æŠ½å–å®ä½“
            entities = {}
            for _, row in normalized_df.iterrows():
                for col, value in row.items():
                    if pd.notna(value):
                        extracted = self.extract_entities_from_text(str(value))
                        for entity_type, entity_list in extracted.items():
                            if entity_type not in entities:
                                entities[entity_type] = set()
                            entities[entity_type].update(entity_list)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨
            entities = {k: list(v) for k, v in entities.items()}
            
            # 4. æŠ½å–å…³ç³»
            relationships = self.extract_relationships(normalized_df)
            
            # 5. ç”ŸæˆçŸ¥è¯†å›¾è°±æ•°æ®
            kg_data = {
                'nodes': self._generate_nodes(entities, normalized_df),
                'relationships': relationships,
                'metadata': {
                    'source_file': file_path,
                    'total_rows': len(df),
                    'total_columns': len(df.columns),
                    'entity_types': list(entities.keys()),
                    'relationship_count': len(relationships)
                }
            }
            
            logger.info(f"æŠ½å–å®Œæˆ: {len(kg_data['nodes'])} ä¸ªèŠ‚ç‚¹, {len(relationships)} ä¸ªå…³ç³»")
            return kg_data
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def _generate_nodes(self, entities: Dict[str, List[str]], df: pd.DataFrame) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå›¾è°±èŠ‚ç‚¹"""
        nodes = []
        
        # ä»å®ä½“ä¸­ç”ŸæˆèŠ‚ç‚¹
        for entity_type, entity_list in entities.items():
            for entity_value in entity_list:
                nodes.append({
                    'id': f"{entity_type}:{entity_value}",
                    'type': entity_type,
                    'name': entity_value,
                    'properties': {}
                })
        
        # ä»æ•°æ®è¡Œä¸­ç”ŸæˆèŠ‚ç‚¹
        for _, row in df.iterrows():
            if 'anomaly_key' in row and pd.notna(row['anomaly_key']):
                node = {
                    'id': f"Anomaly:{row['anomaly_key']}",
                    'type': 'Anomaly',
                    'name': row['anomaly_key'],
                    'properties': {}
                }
                
                # æ·»åŠ å±æ€§
                for col, value in row.items():
                    if pd.notna(value) and col != 'anomaly_key':
                        node['properties'][col] = str(value)
                
                nodes.append(node)
        
        return nodes

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ™ºèƒ½æŠ½å–åŠŸèƒ½"""
    extractor = IntelligentExtractor()
    
    # å¤„ç†æ¥æ–™é—®é¢˜æ•°æ®
    problems_file = "data/import/æ¥æ–™é—®é¢˜å…ˆåç‰ˆ.xlsx"
    if Path(problems_file).exists():
        kg_data = extractor.process_excel_file(problems_file)
        
        # ä¿å­˜ç»“æœ
        output_file = "data/processed/extracted_knowledge_graph.json"
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(kg_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… çŸ¥è¯†å›¾è°±æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - èŠ‚ç‚¹æ•°: {len(kg_data['nodes'])}")
        print(f"   - å…³ç³»æ•°: {len(kg_data['relationships'])}")
        print(f"   - å®ä½“ç±»å‹: {kg_data['metadata']['entity_types']}")

if __name__ == "__main__":
    main()
