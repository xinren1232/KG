#!/usr/bin/env python3
"""
å¢å¼ºæ–‡æ¡£æŠ½å–å™¨
æ”¯æŒExcelã€PDFã€Wordç­‰å¤šç§æ ¼å¼çš„æ™ºèƒ½æŠ½å–
é›†æˆspaCy NLPå’ŒLLMèƒ½åŠ›è¿›è¡Œå®ä½“å…³ç³»æŠ½å–
"""
import re
import json
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Union
from pathlib import Path
import logging
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedDocumentExtractor:
    """å¢å¼ºæ–‡æ¡£æŠ½å–å™¨"""
    
    def __init__(self):
        self.supported_formats = ['.xlsx', '.xls', '.csv', '.pdf', '.docx', '.txt']
        self.nlp_models = {}
        self.extraction_rules = self._load_extraction_rules()
        self.vocabulary_mappings = self._load_vocabulary_mappings()
        
    def _load_extraction_rules(self) -> Dict[str, Any]:
        """åŠ è½½æŠ½å–è§„åˆ™"""
        return {
            'entity_patterns': {
                'anomaly_id': [
                    r'[A-Z]{2,4}-\d{4}-\d{3,4}',  # QA-2024-001
                    r'IQC-\d{4}-\d{3}',           # IQC-2024-001
                    r'å¼‚å¸¸ç¼–å·[:ï¼š]\s*([A-Z0-9-]+)',
                    r'é—®é¢˜ç¼–å·[:ï¼š]\s*([A-Z0-9-]+)'
                ],
                'product_model': [
                    r'MyPhone[A-Z]',
                    r'[A-Z]{2,4}\d{2,4}[A-Z]?',   # BG6, MP30A
                    r'æœºå‹[:ï¼š]\s*([A-Z0-9]+)',
                    r'äº§å“[:ï¼š]\s*([A-Z0-9]+)'
                ],
                'component': [
                    r'æ‘„åƒå¤´|ç›¸æœº|é•œå¤´|Camera',
                    r'ç”µæ± |ç”µèŠ¯|Battery',
                    r'æ˜¾ç¤ºå±|å±å¹•|LCD|OLED|Display',
                    r'è§¦æ‘¸å±|è§¦æ§|Touch',
                    r'æ‰¬å£°å™¨|å–‡å­|Speaker',
                    r'ç»„ä»¶[:ï¼š]\s*([^ï¼Œã€‚\n]+)',
                    r'æ¨¡å—[:ï¼š]\s*([^ï¼Œã€‚\n]+)'
                ],
                'severity': [
                    r'S[1-4]',
                    r'ä¸¥é‡ç¨‹åº¦[:ï¼š]\s*(S[1-4]|é«˜|ä¸­|ä½)',
                    r'ä¼˜å…ˆçº§[:ï¼š]\s*(P[1-4]|é«˜|ä¸­|ä½)'
                ],
                'symptom': [
                    r'å¯¹ç„¦.*å¤±è´¥|å¯¹ç„¦.*å¼‚å¸¸',
                    r'å……ç”µ.*æ…¢|å……ç”µ.*å¼‚å¸¸',
                    r'è‰²å½©.*åå·®|æ˜¾ç¤º.*å¼‚å¸¸',
                    r'è§¦æ‘¸.*ä¸çµæ•|å“åº”.*å¼‚å¸¸',
                    r'éŸ³è´¨.*å¼‚å¸¸|æ‚éŸ³',
                    r'è£‚çº¹|ç ´æŸ|å˜å½¢|åˆ’ä¼¤',
                    r'ç—‡çŠ¶[:ï¼š]\s*([^ï¼Œã€‚\n]+)',
                    r'ç°è±¡[:ï¼š]\s*([^ï¼Œã€‚\n]+)'
                ],
                'root_cause': [
                    r'.*å¯¼è‡´.*',
                    r'.*åŸå› .*',
                    r'å·¥è‰º.*é—®é¢˜',
                    r'å†…é˜».*åé«˜',
                    r'è‰²æ¸©.*åå·®',
                    r'æ ¹å› [:ï¼š]\s*([^ï¼Œã€‚\n]+)',
                    r'åŸå› [:ï¼š]\s*([^ï¼Œã€‚\n]+)'
                ],
                'countermeasure': [
                    r'æ›´æ¢.*',
                    r'è°ƒæ•´.*',
                    r'å¢åŠ .*',
                    r'æ”¹è¿›.*',
                    r'ä¼˜åŒ–.*',
                    r'å¯¹ç­–[:ï¼š]\s*([^ï¼Œã€‚\n]+)',
                    r'æªæ–½[:ï¼š]\s*([^ï¼Œã€‚\n]+)'
                ],
                'supplier': [
                    r'.*æœ‰é™å…¬å¸',
                    r'.*è‚¡ä»½.*å…¬å¸',
                    r'.*ç§‘æŠ€.*å…¬å¸',
                    r'.*åˆ¶é€ .*å…¬å¸',
                    r'ä¾›åº”å•†[:ï¼š]\s*([^ï¼Œã€‚\n]+)'
                ],
                'owner': [
                    r'[\u4e00-\u9fa5]{2,4}',  # ä¸­æ–‡å§“å
                    r'è´£ä»»äºº[:ï¼š]\s*([\u4e00-\u9fa5]{2,4})',
                    r'å¤„ç†äºº[:ï¼š]\s*([\u4e00-\u9fa5]{2,4})'
                ]
            },
            'relation_patterns': [
                {
                    'pattern': r'(.*å¼‚å¸¸).*å½±å“.*(ç»„ä»¶|æ¨¡å—)',
                    'relation': 'AFFECTS',
                    'source_type': 'Anomaly',
                    'target_type': 'Component'
                },
                {
                    'pattern': r'(.*ç—‡çŠ¶).*ç”±äº.*(åŸå› )',
                    'relation': 'CAUSED_BY',
                    'source_type': 'Symptom',
                    'target_type': 'RootCause'
                },
                {
                    'pattern': r'(.*åŸå› ).*é€šè¿‡.*(æªæ–½).*è§£å†³',
                    'relation': 'RESOLVED_BY',
                    'source_type': 'RootCause',
                    'target_type': 'Countermeasure'
                }
            ]
        }
    
    def _load_vocabulary_mappings(self) -> Dict[str, Dict[str, str]]:
        """åŠ è½½è¯æ±‡æ˜ å°„è¡¨"""
        return {
            'severity_mapping': {
                'é«˜': 'S1', 'ä¸¥é‡': 'S1', 'High': 'S1', 'Critical': 'S1',
                'ä¸­': 'S2', 'ä¸€èˆ¬': 'S2', 'Medium': 'S2', 'Normal': 'S2',
                'ä½': 'S3', 'è½»å¾®': 'S3', 'Low': 'S3', 'Minor': 'S3'
            },
            'component_mapping': {
                'ç›¸æœº': 'æ‘„åƒå¤´', 'Camera': 'æ‘„åƒå¤´',
                'ç”µèŠ¯': 'ç”µæ± ', 'Battery': 'ç”µæ± ',
                'å±å¹•': 'æ˜¾ç¤ºå±', 'Display': 'æ˜¾ç¤ºå±', 'LCD': 'æ˜¾ç¤ºå±',
                'è§¦æ§': 'è§¦æ‘¸å±', 'Touch': 'è§¦æ‘¸å±',
                'å–‡å­': 'æ‰¬å£°å™¨', 'Speaker': 'æ‰¬å£°å™¨'
            },
            'symptom_mapping': {
                'å¯¹ç„¦å¤±è´¥': 'å¯¹ç„¦å¼‚å¸¸',
                'å……ç”µæ…¢': 'å……ç”µå¼‚å¸¸',
                'æ˜¾ç¤ºå¼‚å¸¸': 'è‰²å½©åå·®',
                'è§¦æ§ä¸çµæ•': 'è§¦æ‘¸å¼‚å¸¸',
                'éŸ³è´¨å·®': 'éŸ³è´¨å¼‚å¸¸'
            }
        }
    
    def extract_from_file(self, file_path: str) -> Dict[str, Any]:
        """ä»æ–‡ä»¶æŠ½å–æ•°æ®ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if file_path.suffix.lower() not in self.supported_formats:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
        
        logger.info(f"å¼€å§‹æŠ½å–æ–‡ä»¶: {file_path}")
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©æŠ½å–æ–¹æ³•
        if file_path.suffix.lower() in ['.xlsx', '.xls']:
            return self._extract_from_excel(file_path)
        elif file_path.suffix.lower() == '.csv':
            return self._extract_from_csv(file_path)
        elif file_path.suffix.lower() == '.pdf':
            return self._extract_from_pdf(file_path)
        elif file_path.suffix.lower() == '.docx':
            return self._extract_from_docx(file_path)
        elif file_path.suffix.lower() == '.txt':
            return self._extract_from_text(file_path)
        else:
            raise ValueError(f"æœªå®ç°çš„æ–‡ä»¶æ ¼å¼å¤„ç†: {file_path.suffix}")
    
    def _extract_from_excel(self, file_path: Path) -> Dict[str, Any]:
        """ä»Excelæ–‡ä»¶æŠ½å–æ•°æ®"""
        try:
            df = pd.read_excel(file_path)
            logger.info(f"Excelæ–‡ä»¶è¯»å–æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
            
            entities = []
            relations = []
            
            # ç»“æ„åŒ–æ•°æ®æŠ½å–
            for index, row in df.iterrows():
                row_entities, row_relations = self._extract_from_structured_row(row, index)
                entities.extend(row_entities)
                relations.extend(row_relations)
            
            # æ–‡æœ¬å†…å®¹æŠ½å–ï¼ˆä»æè¿°å­—æ®µï¼‰
            text_columns = [col for col in df.columns if any(keyword in col.lower() 
                           for keyword in ['æè¿°', 'è¯´æ˜', 'å¤‡æ³¨', 'description', 'note'])]
            
            for col in text_columns:
                for text in df[col].dropna():
                    text_entities, text_relations = self._extract_from_text_content(str(text))
                    entities.extend(text_entities)
                    relations.extend(text_relations)
            
            return self._build_extraction_result(entities, relations, file_path, 'Excel')
            
        except Exception as e:
            logger.error(f"ExcelæŠ½å–å¤±è´¥: {e}")
            raise
    
    def _extract_from_csv(self, file_path: Path) -> Dict[str, Any]:
        """ä»CSVæ–‡ä»¶æŠ½å–æ•°æ®"""
        try:
            df = pd.read_csv(file_path, encoding='utf-8')
            logger.info(f"CSVæ–‡ä»¶è¯»å–æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
            
            # ä½¿ç”¨ä¸Excelç›¸åŒçš„é€»è¾‘
            return self._extract_from_excel(file_path)
            
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            try:
                df = pd.read_csv(file_path, encoding='gbk')
                return self._extract_from_excel(file_path)
            except Exception as e:
                logger.error(f"CSVæŠ½å–å¤±è´¥: {e}")
                raise
    
    def _extract_from_pdf(self, file_path: Path) -> Dict[str, Any]:
        """ä»PDFæ–‡ä»¶æŠ½å–æ•°æ®"""
        try:
            # å°è¯•å¯¼å…¥pdfplumber
            try:
                import pdfplumber
            except ImportError:
                logger.warning("pdfplumberæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–PDFå¤„ç†")
                return self._extract_from_pdf_simple(file_path)
            
            entities = []
            relations = []
            
            with pdfplumber.open(file_path) as pdf:
                full_text = ""
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        full_text += text + "\n"
                
                logger.info(f"PDFæ–‡æœ¬æŠ½å–å®Œæˆ: {len(full_text)} å­—ç¬¦")
                
                # ä»æ–‡æœ¬ä¸­æŠ½å–å®ä½“å’Œå…³ç³»
                text_entities, text_relations = self._extract_from_text_content(full_text)
                entities.extend(text_entities)
                relations.extend(text_relations)
            
            return self._build_extraction_result(entities, relations, file_path, 'PDF')
            
        except Exception as e:
            logger.error(f"PDFæŠ½å–å¤±è´¥: {e}")
            raise
    
    def _extract_from_pdf_simple(self, file_path: Path) -> Dict[str, Any]:
        """ç®€åŒ–PDFå¤„ç†ï¼ˆæ— pdfplumberä¾èµ–ï¼‰"""
        # è¿”å›ç©ºç»“æœï¼Œæç¤ºéœ€è¦å®‰è£…ä¾èµ–
        logger.warning("PDFå¤„ç†éœ€è¦å®‰è£…pdfplumber: pip install pdfplumber")
        return self._build_extraction_result([], [], file_path, 'PDF')
    
    def _extract_from_docx(self, file_path: Path) -> Dict[str, Any]:
        """ä»Wordæ–‡æ¡£æŠ½å–æ•°æ®"""
        try:
            # å°è¯•å¯¼å…¥python-docx
            try:
                from docx import Document
            except ImportError:
                logger.warning("python-docxæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–Wordå¤„ç†")
                return self._extract_from_docx_simple(file_path)
            
            entities = []
            relations = []
            
            doc = Document(file_path)
            full_text = ""
            
            # æŠ½å–æ®µè½æ–‡æœ¬
            for paragraph in doc.paragraphs:
                full_text += paragraph.text + "\n"
            
            # æŠ½å–è¡¨æ ¼æ–‡æœ¬
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        full_text += cell.text + " "
                    full_text += "\n"
            
            logger.info(f"Wordæ–‡æ¡£æ–‡æœ¬æŠ½å–å®Œæˆ: {len(full_text)} å­—ç¬¦")
            
            # ä»æ–‡æœ¬ä¸­æŠ½å–å®ä½“å’Œå…³ç³»
            text_entities, text_relations = self._extract_from_text_content(full_text)
            entities.extend(text_entities)
            relations.extend(text_relations)
            
            return self._build_extraction_result(entities, relations, file_path, 'Word')
            
        except Exception as e:
            logger.error(f"Wordæ–‡æ¡£æŠ½å–å¤±è´¥: {e}")
            raise
    
    def _extract_from_docx_simple(self, file_path: Path) -> Dict[str, Any]:
        """ç®€åŒ–Wordå¤„ç†ï¼ˆæ— python-docxä¾èµ–ï¼‰"""
        logger.warning("Wordå¤„ç†éœ€è¦å®‰è£…python-docx: pip install python-docx")
        return self._build_extraction_result([], [], file_path, 'Word')
    
    def _extract_from_text(self, file_path: Path) -> Dict[str, Any]:
        """ä»æ–‡æœ¬æ–‡ä»¶æŠ½å–æ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            logger.info(f"æ–‡æœ¬æ–‡ä»¶è¯»å–æˆåŠŸ: {len(text)} å­—ç¬¦")
            
            # ä»æ–‡æœ¬ä¸­æŠ½å–å®ä½“å’Œå…³ç³»
            entities, relations = self._extract_from_text_content(text)
            
            return self._build_extraction_result(entities, relations, file_path, 'Text')
            
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    text = f.read()
                entities, relations = self._extract_from_text_content(text)
                return self._build_extraction_result(entities, relations, file_path, 'Text')
            except Exception as e:
                logger.error(f"æ–‡æœ¬æ–‡ä»¶æŠ½å–å¤±è´¥: {e}")
                raise
    
    def _extract_from_structured_row(self, row: pd.Series, row_index: int) -> Tuple[List[Dict], List[Dict]]:
        """ä»ç»“æ„åŒ–æ•°æ®è¡ŒæŠ½å–å®ä½“å’Œå…³ç³»"""
        entities = []
        relations = []
        
        # è¿™é‡Œå¯ä»¥å¤ç”¨ä¹‹å‰çš„material_anomaly_extractoré€»è¾‘
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸»è¦æŠ½å–å…³é”®å®ä½“
        
        for col, value in row.items():
            if pd.isna(value):
                continue
                
            value_str = str(value)
            
            # æ ¹æ®åˆ—åå’Œå€¼æŠ½å–å®ä½“
            if any(keyword in col.lower() for keyword in ['å¼‚å¸¸', 'anomaly', 'é—®é¢˜']):
                entities.append({
                    'key': f"Anomaly:ROW-{row_index}",
                    'type': 'Anomaly',
                    'name': value_str,
                    'properties': {'title': value_str}
                })
            elif any(keyword in col.lower() for keyword in ['ç»„ä»¶', 'component', 'æ¨¡å—']):
                entities.append({
                    'key': f"Component:{value_str}",
                    'type': 'Component',
                    'name': value_str,
                    'properties': {}
                })
            elif any(keyword in col.lower() for keyword in ['ç—‡çŠ¶', 'symptom']):
                entities.append({
                    'key': f"Symptom:{value_str}",
                    'type': 'Symptom',
                    'name': value_str,
                    'properties': {}
                })
        
        return entities, relations
    
    def _extract_from_text_content(self, text: str) -> Tuple[List[Dict], List[Dict]]:
        """ä»æ–‡æœ¬å†…å®¹æŠ½å–å®ä½“å’Œå…³ç³»"""
        entities = []
        relations = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŠ½å–å®ä½“
        for entity_type, patterns in self.extraction_rules['entity_patterns'].items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    entity_value = match.group(1) if match.groups() else match.group(0)
                    entity_value = entity_value.strip()
                    
                    if entity_value:
                        # æ ‡å‡†åŒ–å®ä½“å€¼
                        normalized_value = self._normalize_entity_value(entity_type, entity_value)
                        
                        entity = {
                            'key': f"{entity_type.title()}:{normalized_value}",
                            'type': entity_type.title(),
                            'name': normalized_value,
                            'properties': {'original_text': entity_value}
                        }
                        entities.append(entity)
        
        # æŠ½å–å…³ç³»ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        for relation_rule in self.extraction_rules['relation_patterns']:
            pattern = relation_rule['pattern']
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match.groups()) >= 2:
                    source_entity = match.group(1).strip()
                    target_entity = match.group(2).strip()
                    
                    relation = {
                        'source_key': f"{relation_rule['source_type']}:{source_entity}",
                        'target_key': f"{relation_rule['target_type']}:{target_entity}",
                        'relation_type': relation_rule['relation'],
                        'properties': {'confidence': 0.7}
                    }
                    relations.append(relation)
        
        return entities, relations
    
    def _normalize_entity_value(self, entity_type: str, value: str) -> str:
        """æ ‡å‡†åŒ–å®ä½“å€¼"""
        # æ ¹æ®å®ä½“ç±»å‹å’Œè¯æ±‡æ˜ å°„è¡¨è¿›è¡Œæ ‡å‡†åŒ–
        if entity_type == 'severity' and value in self.vocabulary_mappings['severity_mapping']:
            return self.vocabulary_mappings['severity_mapping'][value]
        elif entity_type == 'component' and value in self.vocabulary_mappings['component_mapping']:
            return self.vocabulary_mappings['component_mapping'][value]
        elif entity_type == 'symptom' and value in self.vocabulary_mappings['symptom_mapping']:
            return self.vocabulary_mappings['symptom_mapping'][value]
        
        return value
    
    def _build_extraction_result(self, entities: List[Dict], relations: List[Dict], 
                                file_path: Path, file_type: str) -> Dict[str, Any]:
        """æ„å»ºæŠ½å–ç»“æœ"""
        # å»é‡
        unique_entities = self._deduplicate_entities(entities)
        unique_relations = self._deduplicate_relations(relations)
        
        return {
            'entities': unique_entities,
            'relations': unique_relations,
            'metadata': {
                'source_file': str(file_path),
                'file_type': file_type,
                'entity_count': len(unique_entities),
                'relation_count': len(unique_relations),
                'extracted_at': datetime.now().isoformat()
            }
        }
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """å®ä½“å»é‡"""
        seen_keys = set()
        unique_entities = []
        
        for entity in entities:
            if entity['key'] not in seen_keys:
                seen_keys.add(entity['key'])
                unique_entities.append(entity)
        
        return unique_entities
    
    def _deduplicate_relations(self, relations: List[Dict]) -> List[Dict]:
        """å…³ç³»å»é‡"""
        seen_relations = set()
        unique_relations = []
        
        for relation in relations:
            relation_tuple = (relation['source_key'], relation['target_key'], relation['relation_type'])
            if relation_tuple not in seen_relations:
                seen_relations.add(relation_tuple)
                unique_relations.append(relation)
        
        return unique_relations

def main():
    """ä¸»å‡½æ•° - æµ‹è¯•å¢å¼ºæŠ½å–å™¨"""
    extractor = EnhancedDocumentExtractor()
    
    # æµ‹è¯•ä¸åŒæ ¼å¼çš„æ–‡ä»¶
    test_files = [
        "data/import/æ¥æ–™é—®é¢˜å…ˆåç‰ˆ.xlsx",
        "data/import/ç›¸å…³æµ‹è¯•ç”¨ä¾‹.xlsx"
    ]
    
    for test_file in test_files:
        if Path(test_file).exists():
            try:
                result = extractor.extract_from_file(test_file)
                
                # ä¿å­˜ç»“æœ
                output_file = f"data/processed/enhanced_extracted_{Path(test_file).stem}.json"
                Path(output_file).parent.mkdir(parents=True, exist_ok=True)
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… {test_file} æŠ½å–å®Œæˆ!")
                print(f"ğŸ“Š ç»“æœ: {result['metadata']['entity_count']} ä¸ªå®ä½“, {result['metadata']['relation_count']} ä¸ªå…³ç³»")
                print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
                print()
                
            except Exception as e:
                print(f"âŒ {test_file} æŠ½å–å¤±è´¥: {e}")
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")

if __name__ == "__main__":
    main()
