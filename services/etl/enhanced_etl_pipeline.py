#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆETLç®¡çº¿
é›†æˆæ™ºèƒ½æŠ½å–å™¨ï¼Œæ”¯æŒå¤šç§Excelæ ¼å¼çš„è‡ªåŠ¨é€‚é…å’ŒçŸ¥è¯†å›¾è°±æ„å»º
"""
import sys
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from services.nlp.intelligent_extractor import IntelligentExtractor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedETLPipeline:
    """å¢å¼ºç‰ˆETLç®¡çº¿"""
    
    def __init__(self):
        self.extractor = IntelligentExtractor()
        self.supported_formats = ['.xlsx', '.xls', '.csv']
        
    def detect_file_type(self, file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç±»å‹å’Œå†…å®¹ç‰¹å¾"""
        path = Path(file_path)
        
        if path.suffix.lower() not in self.supported_formats:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")
        
        # è¯»å–æ–‡ä»¶å¹¶åˆ†æå†…å®¹
        try:
            if path.suffix.lower() in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path)
            else:
                df = pd.read_csv(file_path)
            
            # åŸºäºåˆ—åå’Œå†…å®¹åˆ¤æ–­æ–‡ä»¶ç±»å‹
            columns = [col.lower() for col in df.columns]
            
            # å¼‚å¸¸/é—®é¢˜æ•°æ®ç‰¹å¾
            anomaly_keywords = ['é—®é¢˜', 'å¼‚å¸¸', 'ç¼ºé™·', 'æ•…éšœ', 'anomaly', 'issue', 'defect']
            if any(keyword in ' '.join(columns) for keyword in anomaly_keywords):
                return 'anomaly_data'
            
            # æµ‹è¯•ç”¨ä¾‹æ•°æ®ç‰¹å¾
            testcase_keywords = ['ç”¨ä¾‹', 'æµ‹è¯•', 'test', 'case', 'tc-']
            if any(keyword in ' '.join(columns) for keyword in testcase_keywords):
                return 'testcase_data'
            
            # ä¾›åº”å•†æ•°æ®ç‰¹å¾
            supplier_keywords = ['ä¾›åº”å•†', 'æ¥æ–™', 'æ‰¹æ¬¡', 'supplier', 'vendor']
            if any(keyword in ' '.join(columns) for keyword in supplier_keywords):
                return 'supplier_data'
            
            # é»˜è®¤ä¸ºé€šç”¨æ•°æ®
            return 'generic_data'
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶ç±»å‹æ£€æµ‹å¤±è´¥: {e}")
            return 'unknown'
    
    def process_file(self, file_path: str, file_type: Optional[str] = None) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
        
        # è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç±»å‹
        if not file_type:
            file_type = self.detect_file_type(file_path)
        
        logger.info(f"æ–‡ä»¶ç±»å‹: {file_type}")
        
        # ä½¿ç”¨æ™ºèƒ½æŠ½å–å™¨å¤„ç†
        kg_data = self.extractor.process_excel_file(file_path)
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œç‰¹å®šå¤„ç†
        processed_data = self._process_by_type(kg_data, file_type)
        
        # æ•°æ®æ ‡å‡†åŒ–
        normalized_data = self._normalize_data(processed_data)
        
        # ç”Ÿæˆå…¥åº“è„šæœ¬
        cypher_scripts = self._generate_cypher_scripts(normalized_data)
        
        return {
            'file_path': file_path,
            'file_type': file_type,
            'raw_data': kg_data,
            'processed_data': processed_data,
            'normalized_data': normalized_data,
            'cypher_scripts': cypher_scripts,
            'metadata': {
                'processed_at': datetime.now().isoformat(),
                'node_count': len(normalized_data.get('nodes', [])),
                'relationship_count': len(normalized_data.get('relationships', []))
            }
        }
    
    def _process_by_type(self, kg_data: Dict[str, Any], file_type: str) -> Dict[str, Any]:
        """æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œç‰¹å®šå¤„ç†"""
        processed = kg_data.copy()
        
        if file_type == 'anomaly_data':
            processed = self._process_anomaly_data(kg_data)
        elif file_type == 'testcase_data':
            processed = self._process_testcase_data(kg_data)
        elif file_type == 'supplier_data':
            processed = self._process_supplier_data(kg_data)
        
        return processed
    
    def _process_anomaly_data(self, kg_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å¼‚å¸¸æ•°æ®"""
        logger.info("å¤„ç†å¼‚å¸¸æ•°æ®ç‰¹å®šé€»è¾‘")
        
        # å¢å¼ºå¼‚å¸¸æ•°æ®çš„å…³ç³»æŠ½å–
        enhanced_relationships = []
        
        for rel in kg_data.get('relationships', []):
            enhanced_relationships.append(rel)
            
            # ä¸ºå¼‚å¸¸æ•°æ®æ·»åŠ é¢å¤–çš„æ¨ç†å…³ç³»
            if rel['relation'] == 'AFFECTS' and rel['source_type'] == 'Anomaly':
                # æ·»åŠ ä¸¥é‡ç¨‹åº¦å…³ç³»
                enhanced_relationships.append({
                    'source': rel['source'],
                    'source_type': 'Anomaly',
                    'target': 'S1',  # ä»æ•°æ®ä¸­æå–å®é™…ä¸¥é‡ç¨‹åº¦
                    'target_type': 'Severity',
                    'relation': 'HAS_SEVERITY'
                })
        
        kg_data['relationships'] = enhanced_relationships
        return kg_data
    
    def _process_testcase_data(self, kg_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†æµ‹è¯•ç”¨ä¾‹æ•°æ®"""
        logger.info("å¤„ç†æµ‹è¯•ç”¨ä¾‹æ•°æ®ç‰¹å®šé€»è¾‘")
        
        # ä¸ºæµ‹è¯•ç”¨ä¾‹æ·»åŠ æ‰§è¡Œå…³ç³»
        enhanced_relationships = kg_data.get('relationships', []).copy()
        
        for node in kg_data.get('nodes', []):
            if node['type'] == 'TestCase':
                # æ·»åŠ æµ‹è¯•ç”¨ä¾‹ä¸äº§å“çš„å…³ç³»
                if 'related_products' in node.get('properties', {}):
                    products = node['properties']['related_products'].split(',')
                    for product in products:
                        product = product.strip()
                        if product:
                            enhanced_relationships.append({
                                'source': node['name'],
                                'source_type': 'TestCase',
                                'target': product,
                                'target_type': 'Product',
                                'relation': 'TESTS'
                            })
        
        kg_data['relationships'] = enhanced_relationships
        return kg_data
    
    def _process_supplier_data(self, kg_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†ä¾›åº”å•†æ•°æ®"""
        logger.info("å¤„ç†ä¾›åº”å•†æ•°æ®ç‰¹å®šé€»è¾‘")
        
        # ä¸ºä¾›åº”å•†æ•°æ®æ·»åŠ ä¾›åº”å…³ç³»
        enhanced_relationships = kg_data.get('relationships', []).copy()
        
        for rel in kg_data.get('relationships', []):
            if rel['source_type'] == 'Supplier' and rel['target_type'] == 'Component':
                # æ·»åŠ ä¾›åº”å…³ç³»
                enhanced_relationships.append({
                    'source': rel['source'],
                    'source_type': 'Supplier',
                    'target': rel['target'],
                    'target_type': 'Component',
                    'relation': 'SUPPLIES'
                })
        
        kg_data['relationships'] = enhanced_relationships
        return kg_data
    
    def _normalize_data(self, processed_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ•°æ®æ ‡å‡†åŒ–"""
        logger.info("æ‰§è¡Œæ•°æ®æ ‡å‡†åŒ–")
        
        normalized = processed_data.copy()
        
        # æ ‡å‡†åŒ–èŠ‚ç‚¹
        normalized_nodes = []
        for node in processed_data.get('nodes', []):
            normalized_node = {
                'key': self._generate_node_key(node),
                'type': node['type'],
                'name': node['name'],
                'properties': node.get('properties', {})
            }
            normalized_nodes.append(normalized_node)
        
        # æ ‡å‡†åŒ–å…³ç³»
        normalized_relationships = []
        for rel in processed_data.get('relationships', []):
            normalized_rel = {
                'source_key': self._generate_entity_key(rel['source'], rel['source_type']),
                'target_key': self._generate_entity_key(rel['target'], rel['target_type']),
                'relation_type': rel['relation'],
                'properties': rel.get('properties', {})
            }
            normalized_relationships.append(normalized_rel)
        
        normalized['nodes'] = normalized_nodes
        normalized['relationships'] = normalized_relationships
        
        return normalized
    
    def _generate_node_key(self, node: Dict[str, Any]) -> str:
        """ç”ŸæˆèŠ‚ç‚¹å”¯ä¸€é”®"""
        return f"{node['type']}:{node['name']}"
    
    def _generate_entity_key(self, name: str, entity_type: str) -> str:
        """ç”Ÿæˆå®ä½“å”¯ä¸€é”®"""
        return f"{entity_type}:{name}"
    
    def _generate_cypher_scripts(self, normalized_data: Dict[str, Any]) -> List[str]:
        """ç”ŸæˆCypherå…¥åº“è„šæœ¬"""
        scripts = []
        
        # ç”ŸæˆèŠ‚ç‚¹åˆ›å»ºè„šæœ¬
        for node in normalized_data.get('nodes', []):
            cypher = f"""
MERGE (n:Entity:{node['type']} {{key: '{node['key']}'}})
SET n.name = '{node['name']}'
"""
            # æ·»åŠ å±æ€§
            for prop_key, prop_value in node.get('properties', {}).items():
                if prop_value:
                    cypher += f", n.{prop_key} = '{str(prop_value).replace("'", "\\'")}'"
            
            scripts.append(cypher.strip())
        
        # ç”Ÿæˆå…³ç³»åˆ›å»ºè„šæœ¬
        for rel in normalized_data.get('relationships', []):
            cypher = f"""
MATCH (a:Entity {{key: '{rel['source_key']}'}})
MATCH (b:Entity {{key: '{rel['target_key']}'}})
MERGE (a)-[r:{rel['relation_type']}]->(b)
"""
            scripts.append(cypher.strip())
        
        return scripts
    
    def process_directory(self, directory_path: str) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ–‡ä»¶"""
        logger.info(f"æ‰¹é‡å¤„ç†ç›®å½•: {directory_path}")
        
        directory = Path(directory_path)
        if not directory.exists():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        
        results = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        for file_path in directory.rglob('*'):
            if file_path.suffix.lower() in self.supported_formats:
                try:
                    result = self.process_file(str(file_path))
                    results[str(file_path)] = result
                    logger.info(f"âœ… å¤„ç†å®Œæˆ: {file_path}")
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                    results[str(file_path)] = {'error': str(e)}
        
        return results
    
    def save_results(self, results: Dict[str, Any], output_dir: str):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = output_path / 'etl_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜Cypherè„šæœ¬
        cypher_file = output_path / 'import_scripts.cypher'
        with open(cypher_file, 'w', encoding='utf-8') as f:
            for file_path, result in results.items():
                if 'cypher_scripts' in result:
                    f.write(f"// Scripts for {file_path}\n")
                    for script in result['cypher_scripts']:
                        f.write(script + ";\n\n")
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    pipeline = EnhancedETLPipeline()
    
    # å¤„ç†å¯¼å…¥ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    import_dir = "data/import"
    results = pipeline.process_directory(import_dir)
    
    # ä¿å­˜ç»“æœ
    pipeline.save_results(results, "data/processed/etl_output")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("ğŸ‰ ETLå¤„ç†å®Œæˆ!")
    print(f"ğŸ“ å¤„ç†æ–‡ä»¶æ•°: {len(results)}")
    
    total_nodes = 0
    total_relationships = 0
    
    for file_path, result in results.items():
        if 'metadata' in result:
            total_nodes += result['metadata'].get('node_count', 0)
            total_relationships += result['metadata'].get('relationship_count', 0)
            print(f"   ğŸ“„ {Path(file_path).name}: {result['metadata'].get('node_count', 0)} èŠ‚ç‚¹, {result['metadata'].get('relationship_count', 0)} å…³ç³»")
    
    print(f"ğŸ“Š æ€»è®¡: {total_nodes} èŠ‚ç‚¹, {total_relationships} å…³ç³»")

if __name__ == "__main__":
    main()
