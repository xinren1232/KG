#!/usr/bin/env python3
"""
å¯¼å…¥æ–°æ‰¹æ¬¡è¯å…¸æ•°æ®åˆ°dictionary.jsonå’ŒNeo4jå›¾è°±
"""
import json
import csv
from datetime import datetime

def load_existing_dictionary():
    """åŠ è½½ç°æœ‰è¯å…¸"""
    with open('api/data/dictionary.json', 'r', encoding='utf-8') as f:
        return json.load(f)

def load_csv_data(csv_file):
    """ä»CSVåŠ è½½æ–°æ•°æ®"""
    new_entries = []
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # è§£æåˆ«åï¼ˆåˆ†å·åˆ†éš”ï¼‰
            aliases = [a.strip() for a in row['åˆ«å'].split(';') if a.strip()]
            
            # è§£ææ ‡ç­¾ï¼ˆåˆ†å·åˆ†éš”ï¼‰
            tags = [t.strip() for t in row['å¤šæ ‡ç­¾'].split(';') if t.strip()]
            
            entry = {
                "term": row['æœ¯è¯­'].strip(),
                "aliases": aliases,
                "category": row['ç±»åˆ«'].strip(),
                "tags": tags,
                "description": row['å¤‡æ³¨'].strip(),
                "sub_category": "",
                "source": "manual_supplement_2025",
                "status": "",
                "original_category": row['ç±»åˆ«'].strip()
            }
            new_entries.append(entry)
    
    return new_entries

def check_duplicates(existing_data, new_entries):
    """æ£€æŸ¥é‡å¤"""
    existing_terms = set(e['term'] for e in existing_data)
    duplicates = []
    unique_entries = []
    
    for entry in new_entries:
        if entry['term'] in existing_terms:
            duplicates.append(entry['term'])
        else:
            unique_entries.append(entry)
    
    return unique_entries, duplicates

def merge_and_save(existing_data, new_entries, output_file):
    """åˆå¹¶å¹¶ä¿å­˜"""
    merged_data = existing_data + new_entries
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    return merged_data

def generate_import_report(existing_count, new_count, duplicates):
    """ç”Ÿæˆå¯¼å…¥æŠ¥å‘Š"""
    report = f"""
{'='*80}
ğŸ“Š è¯å…¸æ•°æ®å¯¼å…¥æŠ¥å‘Š
{'='*80}

å¯¼å…¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“‹ æ•°æ®ç»Ÿè®¡:
  åŸæœ‰è¯å…¸: {existing_count}æ¡
  æ–°å¢æ•°æ®: {new_count}æ¡
  å¯¼å…¥åæ€»æ•°: {existing_count + new_count}æ¡
  å¢é•¿ç‡: {new_count/existing_count*100:.1f}%

"""
    
    if duplicates:
        report += f"""
âš ï¸ é‡å¤æœ¯è¯­ ({len(duplicates)}æ¡):
"""
        for dup in duplicates:
            report += f"  - {dup}\n"
    else:
        report += "âœ… æ— é‡å¤æœ¯è¯­\n"
    
    report += f"""
{'='*80}
âœ… å¯¼å…¥å®Œæˆ
{'='*80}
"""
    
    return report

def main():
    print("=" * 80)
    print("ğŸ“¥ å¼€å§‹å¯¼å…¥æ–°æ‰¹æ¬¡è¯å…¸æ•°æ®")
    print("=" * 80)
    
    # 1. åŠ è½½ç°æœ‰è¯å…¸
    print("\n1ï¸âƒ£ åŠ è½½ç°æœ‰è¯å…¸...")
    existing_data = load_existing_dictionary()
    print(f"   ç°æœ‰è¯å…¸: {len(existing_data)}æ¡")
    
    # 2. åŠ è½½æ–°æ•°æ®
    print("\n2ï¸âƒ£ åŠ è½½æ–°æ‰¹æ¬¡æ•°æ®...")
    csv_file = 'batch_60_corrected.csv'
    new_entries = load_csv_data(csv_file)
    print(f"   æ–°æ‰¹æ¬¡æ•°æ®: {len(new_entries)}æ¡")
    
    # 3. æ£€æŸ¥é‡å¤
    print("\n3ï¸âƒ£ æ£€æŸ¥é‡å¤...")
    unique_entries, duplicates = check_duplicates(existing_data, new_entries)
    print(f"   å”¯ä¸€æ•°æ®: {len(unique_entries)}æ¡")
    if duplicates:
        print(f"   âš ï¸ é‡å¤æ•°æ®: {len(duplicates)}æ¡")
        for dup in duplicates[:5]:
            print(f"      - {dup}")
    else:
        print(f"   âœ… æ— é‡å¤æ•°æ®")
    
    # 4. åˆå¹¶å¹¶ä¿å­˜
    print("\n4ï¸âƒ£ åˆå¹¶å¹¶ä¿å­˜...")
    output_file = 'api/data/dictionary.json'
    merged_data = merge_and_save(existing_data, unique_entries, output_file)
    print(f"   âœ… å·²ä¿å­˜åˆ°: {output_file}")
    print(f"   æ€»æ¡ç›®æ•°: {len(merged_data)}æ¡")
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    print("\n5ï¸âƒ£ ç”Ÿæˆå¯¼å…¥æŠ¥å‘Š...")
    report = generate_import_report(len(existing_data), len(unique_entries), duplicates)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = 'IMPORT_REPORT.md'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"   âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # 6. æ˜¾ç¤ºæŠ¥å‘Š
    print(report)
    
    # 7. åˆ†ç±»ç»Ÿè®¡
    print("\n6ï¸âƒ£ åˆ†ç±»ç»Ÿè®¡:")
    from collections import Counter
    category_counts = Counter(e['category'] for e in merged_data)
    for cat, count in category_counts.most_common():
        print(f"   {cat}: {count}æ¡")
    
    print("\n" + "=" * 80)
    print("âœ… å¯¼å…¥å®Œæˆï¼")
    print("=" * 80)
    
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("   1. è¿è¡Œ python check_dictionary_quality.py éªŒè¯æ•°æ®è´¨é‡")
    print("   2. è¿è¡Œ python sync_to_neo4j.py åŒæ­¥åˆ°Neo4jå›¾è°±")
    print("   3. é‡å¯åç«¯APIæœåŠ¡")

if __name__ == "__main__":
    main()
