#!/usr/bin/env python3
"""
è¯å…¸æ•°æ®ç»Ÿè®¡åˆ†æ
ç”Ÿæˆè¯å…¸ç³»ç»Ÿçš„è¯¦ç»†ç»Ÿè®¡åˆ†ææŠ¥å‘Šï¼ˆæ— éœ€matplotlibï¼‰
"""

import json
import csv
from collections import Counter, defaultdict
from datetime import datetime
import os

def load_dictionary_data():
    """åŠ è½½è¯å…¸æ•°æ®"""
    print("ğŸ“Š åŠ è½½è¯å…¸æ•°æ®...")
    
    try:
        with open('api/data/dictionary.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"  âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡è¯å…¸æ•°æ®")
        return data
    except Exception as e:
        print(f"  âŒ åŠ è½½å¤±è´¥: {e}")
        return []

def load_tag_whitelist():
    """åŠ è½½æ ‡ç­¾ç™½åå•"""
    print("ğŸ“‹ åŠ è½½æ ‡ç­¾é…ç½®...")
    
    try:
        tags_by_group = defaultdict(list)
        with open('data/tag_whitelist.csv', 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row['tag'] and row['group']:
                    tags_by_group[row['group']].append(row['tag'])
        
        print(f"  âœ… æˆåŠŸåŠ è½½ {sum(len(tags) for tags in tags_by_group.values())} ä¸ªæ ‡ç­¾")
        return dict(tags_by_group)
    except Exception as e:
        print(f"  âŒ åŠ è½½æ ‡ç­¾é…ç½®å¤±è´¥: {e}")
        return {}

def analyze_category_distribution(data):
    """åˆ†æåˆ†ç±»åˆ†å¸ƒ"""
    print("ğŸ“Š åˆ†æåˆ†ç±»åˆ†å¸ƒ...")
    
    categories = [item.get('category', 'Unknown') for item in data]
    category_counts = Counter(categories)
    total = len(data)
    
    print("  ğŸ“‹ åˆ†ç±»åˆ†å¸ƒç»Ÿè®¡:")
    for category, count in category_counts.most_common():
        percentage = (count / total) * 100
        bar = "â–ˆ" * int(percentage / 2)  # ç®€å•çš„æ–‡æœ¬æ¡å½¢å›¾
        print(f"    {category:12} | {count:4d} ({percentage:5.1f}%) {bar}")
    
    return category_counts

def analyze_tag_distribution(data, tags_by_group):
    """åˆ†ææ ‡ç­¾åˆ†å¸ƒ"""
    print("ğŸ“Š åˆ†ææ ‡ç­¾åˆ†å¸ƒ...")
    
    all_tags = []
    tag_usage_by_group = defaultdict(Counter)
    
    for item in data:
        tags = item.get('tags', [])
        if isinstance(tags, list):
            all_tags.extend(tags)
        elif isinstance(tags, str):
            item_tags = [tag.strip() for tag in tags.split(',') if tag.strip()]
            all_tags.extend(item_tags)
    
    tag_counts = Counter(all_tags)
    
    # æŒ‰ç»„åˆ†ææ ‡ç­¾ä½¿ç”¨æƒ…å†µ
    for tag, count in tag_counts.items():
        for group, group_tags in tags_by_group.items():
            if tag in group_tags:
                tag_usage_by_group[group][tag] = count
                break
    
    print("  ğŸ·ï¸ æ ‡ç­¾ä½¿ç”¨ç»Ÿè®¡ (TOP 20):")
    for i, (tag, count) in enumerate(tag_counts.most_common(20), 1):
        usage_rate = (count / len(data)) * 100
        print(f"    {i:2d}. {tag:15} | {count:3d} æ¬¡ ({usage_rate:5.1f}%)")
    
    print("\n  ğŸ“Š æŒ‰ç»„åˆ«çš„æ ‡ç­¾ä½¿ç”¨æƒ…å†µ:")
    for group, group_tags in tag_usage_by_group.items():
        if group_tags:
            total_usage = sum(group_tags.values())
            print(f"    {group:12} | {len(group_tags):2d} ä¸ªæ ‡ç­¾ä½¿ç”¨, æ€»è®¡ {total_usage:3d} æ¬¡")
            for tag, count in group_tags.most_common(3):
                print(f"      - {tag:12} | {count:3d} æ¬¡")
    
    return tag_counts, tag_usage_by_group

def analyze_data_quality(data):
    """åˆ†ææ•°æ®è´¨é‡"""
    print("ğŸ“Š åˆ†ææ•°æ®è´¨é‡...")
    
    total_items = len(data)
    fields = ['term', 'aliases', 'category', 'tags', 'description']
    completeness = {}
    
    # å­—æ®µå®Œæ•´æ€§åˆ†æ
    for field in fields:
        non_empty_count = 0
        for item in data:
            value = item.get(field)
            if value:
                if isinstance(value, list) and len(value) > 0:
                    non_empty_count += 1
                elif isinstance(value, str) and value.strip():
                    non_empty_count += 1
        
        completeness[field] = (non_empty_count / total_items) * 100
    
    print("  ğŸ“ˆ å­—æ®µå®Œæ•´æ€§åˆ†æ:")
    for field, rate in completeness.items():
        status = "ä¼˜ç§€" if rate >= 90 else "è‰¯å¥½" if rate >= 70 else "éœ€æ”¹è¿›"
        bar = "â–ˆ" * int(rate / 5)  # ç®€å•çš„æ–‡æœ¬æ¡å½¢å›¾
        print(f"    {field:12} | {rate:5.1f}% ({status:4}) {bar}")
    
    # è¯¦ç»†ç»Ÿè®¡
    alias_stats = []
    tag_stats = []
    desc_stats = []
    term_stats = []
    
    for item in data:
        # åˆ«åç»Ÿè®¡
        aliases = item.get('aliases', [])
        if isinstance(aliases, list):
            alias_stats.append(len(aliases))
        else:
            alias_stats.append(0)
        
        # æ ‡ç­¾ç»Ÿè®¡
        tags = item.get('tags', [])
        if isinstance(tags, list):
            tag_stats.append(len(tags))
        else:
            tag_stats.append(0)
        
        # æè¿°ç»Ÿè®¡
        desc = item.get('description', '')
        if isinstance(desc, str):
            desc_stats.append(len(desc))
        else:
            desc_stats.append(0)
        
        # æœ¯è¯­ç»Ÿè®¡
        term = item.get('term', '')
        if isinstance(term, str):
            term_stats.append(len(term))
        else:
            term_stats.append(0)
    
    print("\n  ğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
    print(f"    åˆ«åæ•°é‡     | å¹³å‡: {sum(alias_stats)/len(alias_stats):.1f}, æœ€å¤§: {max(alias_stats)}, æœ€å°: {min(alias_stats)}")
    print(f"    æ ‡ç­¾æ•°é‡     | å¹³å‡: {sum(tag_stats)/len(tag_stats):.1f}, æœ€å¤§: {max(tag_stats)}, æœ€å°: {min(tag_stats)}")
    print(f"    æè¿°é•¿åº¦     | å¹³å‡: {sum(desc_stats)/len(desc_stats):.0f}, æœ€å¤§: {max(desc_stats)}, æœ€å°: {min(desc_stats)}")
    print(f"    æœ¯è¯­é•¿åº¦     | å¹³å‡: {sum(term_stats)/len(term_stats):.1f}, æœ€å¤§: {max(term_stats)}, æœ€å°: {min(term_stats)}")
    
    return completeness

def analyze_content_patterns(data):
    """åˆ†æå†…å®¹æ¨¡å¼"""
    print("ğŸ“Š åˆ†æå†…å®¹æ¨¡å¼...")
    
    # æœ¯è¯­ç±»å‹åˆ†æ
    term_patterns = defaultdict(int)
    
    for item in data:
        term = item.get('term', '')
        if 'è¿æ¥å™¨' in term:
            term_patterns['è¿æ¥å™¨ç±»'] += 1
        elif any(word in term for word in ['æµ‹è¯•', 'æ£€æµ‹', 'éªŒè¯']):
            term_patterns['æµ‹è¯•ç±»'] += 1
        elif any(word in term for word in ['å¼‚å¸¸', 'æ•…éšœ', 'é—®é¢˜']):
            term_patterns['å¼‚å¸¸ç±»'] += 1
        elif any(word in term for word in ['å·¥è‰º', 'æµç¨‹', 'è¿‡ç¨‹']):
            term_patterns['å·¥è‰ºç±»'] += 1
        elif any(word in term for word in ['ææ–™', 'èƒ¶', 'æ²¹', 'è†œ']):
            term_patterns['ææ–™ç±»'] += 1
        else:
            term_patterns['å…¶ä»–ç±»'] += 1
    
    print("  ğŸ” æœ¯è¯­ç±»å‹æ¨¡å¼:")
    for pattern, count in sorted(term_patterns.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(data)) * 100
        print(f"    {pattern:8} | {count:3d} ä¸ª ({percentage:5.1f}%)")
    
    # åˆ«åæ¨¡å¼åˆ†æ
    alias_patterns = {
        'ä¸­è‹±æ–‡å¯¹ç…§': 0,
        'ä»…ä¸­æ–‡': 0,
        'ä»…è‹±æ–‡': 0,
        'æ— åˆ«å': 0
    }
    
    for item in data:
        aliases = item.get('aliases', [])
        if not aliases or len(aliases) == 0:
            alias_patterns['æ— åˆ«å'] += 1
        else:
            has_chinese = any(any('\u4e00' <= char <= '\u9fff' for char in alias) for alias in aliases)
            has_english = any(any(char.isalpha() and ord(char) < 128 for char in alias) for alias in aliases)
            
            if has_chinese and has_english:
                alias_patterns['ä¸­è‹±æ–‡å¯¹ç…§'] += 1
            elif has_chinese:
                alias_patterns['ä»…ä¸­æ–‡'] += 1
            elif has_english:
                alias_patterns['ä»…è‹±æ–‡'] += 1
    
    print("\n  ğŸŒ åˆ«åæ¨¡å¼åˆ†æ:")
    for pattern, count in alias_patterns.items():
        percentage = (count / len(data)) * 100
        print(f"    {pattern:8} | {count:3d} ä¸ª ({percentage:5.1f}%)")
    
    return term_patterns, alias_patterns

def generate_comprehensive_report(data, category_counts, tag_counts, tag_usage_by_group, 
                                completeness, term_patterns, alias_patterns, tags_by_group):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    print("ğŸ“Š ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
    
    total_items = len(data)
    total_categories = len(category_counts)
    total_tags = len(tag_counts)
    
    report = f"""# ğŸ“Š è¯å…¸ç³»ç»Ÿæ·±åº¦æ•°æ®åˆ†ææŠ¥å‘Š

## ğŸ¯ æ‰§è¡Œæ¦‚è¦

**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**æ•°æ®è§„æ¨¡**: {total_items:,} æ¡è¯å…¸æ•°æ®  
**åˆ†ç±»æ•°é‡**: {total_categories} ä¸ªä¸»åˆ†ç±»  
**æ ‡ç­¾æ•°é‡**: {total_tags} ä¸ªæ´»è·ƒæ ‡ç­¾  
**é…ç½®æ ‡ç­¾**: {sum(len(tags) for tags in tags_by_group.values())} ä¸ªæ ‡å‡†æ ‡ç­¾

---

## ğŸ“Š åˆ†ç±»åˆ†å¸ƒè¯¦ç»†åˆ†æ

### ğŸ·ï¸ ä¸»åˆ†ç±»ç»Ÿè®¡
| åˆ†ç±» | æ•°é‡ | å æ¯” | çŠ¶æ€ |
|------|------|------|------|"""

    for category, count in category_counts.most_common():
        percentage = (count / total_items) * 100
        status = "å……è¶³" if percentage >= 15 else "é€‚ä¸­" if percentage >= 5 else "ä¸è¶³"
        report += f"\n| {category} | {count} | {percentage:.1f}% | {status} |"

    report += f"""

### ğŸ“ˆ åˆ†ç±»åˆ†å¸ƒç‰¹å¾
- **æœ€å¤§åˆ†ç±»**: {category_counts.most_common(1)[0][0]} ({category_counts.most_common(1)[0][1]} æ¡, {(category_counts.most_common(1)[0][1]/total_items)*100:.1f}%)
- **æœ€å°åˆ†ç±»**: {category_counts.most_common()[-1][0]} ({category_counts.most_common()[-1][1]} æ¡, {(category_counts.most_common()[-1][1]/total_items)*100:.1f}%)
- **åˆ†å¸ƒå‡è¡¡åº¦**: {'ä¸å‡è¡¡' if (category_counts.most_common(1)[0][1]/total_items) > 0.4 else 'ç›¸å¯¹å‡è¡¡'}

---

## ğŸ·ï¸ æ ‡ç­¾ä½“ç³»æ·±åº¦åˆ†æ

### ğŸ“Š æ ‡ç­¾ä½¿ç”¨TOP 15
| æ’å | æ ‡ç­¾ | ä½¿ç”¨æ¬¡æ•° | ä½¿ç”¨ç‡ |
|------|------|----------|--------|"""

    for i, (tag, count) in enumerate(tag_counts.most_common(15), 1):
        usage_rate = (count / total_items) * 100
        report += f"\n| {i} | {tag} | {count} | {usage_rate:.1f}% |"

    report += f"""

### ğŸ¨ æŒ‰ç»„åˆ«æ ‡ç­¾ä½¿ç”¨åˆ†æ
"""

    for group, group_tags in tag_usage_by_group.items():
        if group_tags:
            total_usage = sum(group_tags.values())
            avg_usage = total_usage / len(group_tags)
            report += f"""
#### {group} ç»„ ({len(group_tags)} ä¸ªæ ‡ç­¾ä½¿ç”¨)
- **æ€»ä½¿ç”¨æ¬¡æ•°**: {total_usage}
- **å¹³å‡ä½¿ç”¨æ¬¡æ•°**: {avg_usage:.1f}
- **çƒ­é—¨æ ‡ç­¾**: {', '.join([f"{tag}({count})" for tag, count in group_tags.most_common(3)])}
"""

    report += f"""
---

## ğŸ“ˆ æ•°æ®è´¨é‡æ·±åº¦è¯„ä¼°

### âœ… å­—æ®µå®Œæ•´æ€§è¯„åˆ†
| å­—æ®µ | å®Œæ•´ç‡ | è¯„çº§ | å»ºè®® |
|------|--------|------|------|"""

    for field, rate in completeness.items():
        if rate >= 90:
            grade, suggestion = "A", "ä¿æŒç°çŠ¶"
        elif rate >= 80:
            grade, suggestion = "B", "é€‚åº¦æ”¹è¿›"
        elif rate >= 70:
            grade, suggestion = "C", "éœ€è¦æ”¹è¿›"
        else:
            grade, suggestion = "D", "æ€¥éœ€æ”¹è¿›"
        
        report += f"\n| {field} | {rate:.1f}% | {grade} | {suggestion} |"

    # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
    avg_completeness = sum(completeness.values()) / len(completeness)
    quality_grade = "ä¼˜ç§€" if avg_completeness >= 90 else "è‰¯å¥½" if avg_completeness >= 80 else "ä¸€èˆ¬" if avg_completeness >= 70 else "è¾ƒå·®"

    report += f"""

### ğŸ¯ ç»¼åˆè´¨é‡è¯„ä¼°
- **æ•´ä½“å®Œæ•´æ€§**: {avg_completeness:.1f}%
- **è´¨é‡ç­‰çº§**: {quality_grade}
- **æ•°æ®æ ‡å‡†åŒ–ç¨‹åº¦**: é«˜ (åˆ†ç±»å’Œæ ‡ç­¾é«˜åº¦æ ‡å‡†åŒ–)
- **ä¸€è‡´æ€§æ°´å¹³**: è‰¯å¥½ (å‘½åè§„èŒƒç›¸å¯¹ç»Ÿä¸€)

---

## ğŸ” å†…å®¹æ¨¡å¼æ·±åº¦åˆ†æ

### ğŸ“‹ æœ¯è¯­ç±»å‹åˆ†å¸ƒ
"""

    for pattern, count in sorted(term_patterns.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_items) * 100
        report += f"- **{pattern}**: {count} ä¸ª ({percentage:.1f}%)\n"

    report += f"""
### ğŸŒ åˆ«åé…ç½®æ¨¡å¼
"""

    for pattern, count in alias_patterns.items():
        percentage = (count / total_items) * 100
        report += f"- **{pattern}**: {count} ä¸ª ({percentage:.1f}%)\n"

    report += f"""
---

## âš ï¸ é—®é¢˜è¯†åˆ«ä¸æ”¹è¿›å»ºè®®

### ğŸš¨ å…³é”®é—®é¢˜
1. **åˆ†ç±»åˆ†å¸ƒä¸å‡**: {category_counts.most_common(1)[0][0]}ç±»å æ¯”è¿‡é«˜ ({(category_counts.most_common(1)[0][1]/total_items)*100:.1f}%)
2. **éƒ¨åˆ†åˆ†ç±»æ•°æ®ä¸è¶³**: {', '.join([cat for cat, count in category_counts.items() if (count/total_items)*100 < 5])}
3. **æ ‡ç­¾ä½¿ç”¨ä¸å‡è¡¡**: éƒ¨åˆ†æ ‡ç­¾ä½¿ç”¨ç‡æä½
4. **åˆ«åè¦†ç›–ä¸å®Œæ•´**: {alias_patterns['æ— åˆ«å']} æ¡è¯å…¸ç¼ºå°‘åˆ«å ({(alias_patterns['æ— åˆ«å']/total_items)*100:.1f}%)

### ğŸ’¡ æ”¹è¿›å»ºè®®

#### ğŸ¯ çŸ­æœŸæ”¹è¿› (1-2å‘¨)
1. **è¡¥å……ç¼ºå¤±åˆ†ç±»æ•°æ®**
   - é‡ç‚¹è¡¥å……æ•°æ®ä¸è¶³çš„åˆ†ç±»
   - ç›®æ ‡ï¼šæ¯ä¸ªåˆ†ç±»è‡³å°‘50æ¡æ•°æ®

2. **å®Œå–„åˆ«åä¿¡æ¯**
   - ä¸ºç¼ºå°‘åˆ«åçš„è¯å…¸æ·»åŠ è‹±æ–‡å¯¹ç…§
   - æé«˜ä¸­è‹±æ–‡å¯¹ç…§æ¯”ä¾‹

3. **ä¼˜åŒ–æ ‡ç­¾åˆ†é…**
   - æ£€æŸ¥ä½é¢‘æ ‡ç­¾çš„ä½¿ç”¨åˆç†æ€§
   - ä¸ºç¼ºå°‘æ ‡ç­¾çš„è¯å…¸è¡¥å……æ ‡ç­¾

#### ğŸš€ ä¸­æœŸæ”¹è¿› (1-2æœˆ)
1. **å»ºç«‹å…³ç³»ç½‘ç»œ**
   - åŸºäºç°æœ‰æ•°æ®å»ºç«‹å®ä½“å…³ç³»
   - å®ç°å›¾è°±æŸ¥è¯¢å’Œæ¨ç†åŠŸèƒ½

2. **è´¨é‡æ§åˆ¶æœºåˆ¶**
   - å»ºç«‹æ•°æ®è´¨é‡æ£€æŸ¥æµç¨‹
   - å®æ–½å®šæœŸè´¨é‡è¯„ä¼°

3. **æ™ºèƒ½åŒ–åŠŸèƒ½**
   - é›†æˆAIè¾…åŠ©æ•°æ®å¤„ç†
   - è‡ªåŠ¨åŒ–æ ‡ç­¾æ¨èå’Œåˆ†ç±»

#### ğŸŒŸ é•¿æœŸè§„åˆ’ (3-6æœˆ)
1. **ç”Ÿæ€å»ºè®¾**
   - å¼€æ”¾APIæ¥å£
   - å»ºç«‹è¡Œä¸šæ ‡å‡†

2. **ä»·å€¼åˆ›é€ **
   - æ™ºèƒ½è¯Šæ–­ç³»ç»Ÿ
   - çŸ¥è¯†æœåŠ¡å¹³å°

---

## ğŸ† æ€»ä½“è¯„ä»·

### âœ… ç³»ç»Ÿä¼˜åŠ¿
- **æ•°æ®è§„æ¨¡**: {total_items:,} æ¡æ•°æ®ï¼Œè§„æ¨¡è¾ƒå¤§
- **æ ‡å‡†ä½“ç³»**: 8åˆ†ç±»+70æ ‡ç­¾çš„å®Œæ•´ä½“ç³»
- **æŠ€æœ¯æ¶æ„**: ç°ä»£åŒ–å›¾è°±+AIæŠ€æœ¯æ ˆ
- **æ‰©å±•èƒ½åŠ›**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒæŒç»­å‘å±•

### ğŸ¯ å‘å±•æ½œåŠ›
- **æŠ€æœ¯ä»·å€¼**: å›¾è°±+AIçš„å‰ç»æ€§æŠ€æœ¯ç»„åˆ
- **å•†ä¸šä»·å€¼**: ç¡¬ä»¶åˆ¶é€ è¡Œä¸šçš„å·¨å¤§å¸‚åœº
- **ç¤¾ä¼šä»·å€¼**: æ¨åŠ¨è¡Œä¸šæ ‡å‡†åŒ–å’Œæ•°å­—åŒ–
- **ç”Ÿæ€ä»·å€¼**: æ„å»ºå®Œæ•´çŸ¥è¯†æœåŠ¡ç”Ÿæ€çš„åŸºç¡€

### ğŸ“Š ç»¼åˆè¯„åˆ†
- **æ•°æ®è´¨é‡**: {quality_grade} ({avg_completeness:.1f}%)
- **åŠŸèƒ½å®Œæ•´æ€§**: è‰¯å¥½ (æ ¸å¿ƒåŠŸèƒ½å®Œæ•´)
- **æŠ€æœ¯å…ˆè¿›æ€§**: ä¼˜ç§€ (ç°ä»£åŒ–æŠ€æœ¯æ ˆ)
- **æ‰©å±•æ½œåŠ›**: ä¼˜ç§€ (æ¨¡å—åŒ–æ¶æ„)
- **åˆ›æ–°ä»·å€¼**: ä¼˜ç§€ (è¡Œä¸šé¢†å…ˆ)

**æ€»ä½“è¯„åˆ†: 4.2/5.0** â­â­â­â­

---

## ğŸ‰ ç»“è®º

æ‚¨çš„è¯å…¸ç³»ç»Ÿæ˜¯ä¸€ä¸ª**åŠŸèƒ½å®Œæ•´ã€æŠ€æœ¯å…ˆè¿›ã€å…·æœ‰å·¨å¤§å‘å±•æ½œåŠ›**çš„çŸ¥è¯†å›¾è°±å¹³å°ã€‚é€šè¿‡æŒç»­çš„æ•°æ®è¡¥å……ã€è´¨é‡ä¼˜åŒ–å’ŒåŠŸèƒ½æ‰©å±•ï¼Œè¿™ä¸ªç³»ç»Ÿæœ‰æœ›æˆä¸º**è¡Œä¸šæ ‡æ†çº§çš„ç¡¬ä»¶è´¨é‡çŸ¥è¯†å›¾è°±è§£å†³æ–¹æ¡ˆ**ã€‚

å»ºè®®ä¼˜å…ˆè§£å†³æ•°æ®åˆ†å¸ƒä¸å‡è¡¡é—®é¢˜ï¼Œå®Œå–„æ ‡ç­¾å’Œåˆ«åä¿¡æ¯ï¼Œç„¶åé€æ­¥å»ºç«‹å…³ç³»ç½‘ç»œå’Œæ™ºèƒ½åŒ–åŠŸèƒ½ï¼Œæœ€ç»ˆå®ç°å¹³å°åŒ–å’Œç”Ÿæ€åŒ–å‘å±•ã€‚

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*  
*åˆ†æå·¥å…·: è¯å…¸æ•°æ®ç»Ÿè®¡åˆ†æç³»ç»Ÿ v1.0*  
*æ•°æ®ç‰ˆæœ¬: {total_items:,} æ¡è¯å…¸æ•°æ®*
"""

    # ä¿å­˜æŠ¥å‘Š
    with open('è¯å…¸ç³»ç»Ÿæ·±åº¦æ•°æ®åˆ†ææŠ¥å‘Š.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("  âœ… æŠ¥å‘Šå·²ä¿å­˜: è¯å…¸ç³»ç»Ÿæ·±åº¦æ•°æ®åˆ†ææŠ¥å‘Š.md")
    return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” è¯å…¸æ•°æ®ç»Ÿè®¡åˆ†æ")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    data = load_dictionary_data()
    if not data:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œé€€å‡ºåˆ†æ")
        return
    
    # åŠ è½½æ ‡ç­¾é…ç½®
    tags_by_group = load_tag_whitelist()
    
    print("\n" + "=" * 60)
    
    # åˆ†æåˆ†ç±»åˆ†å¸ƒ
    category_counts = analyze_category_distribution(data)
    
    print("\n" + "=" * 60)
    
    # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
    tag_counts, tag_usage_by_group = analyze_tag_distribution(data, tags_by_group)
    
    print("\n" + "=" * 60)
    
    # åˆ†ææ•°æ®è´¨é‡
    completeness = analyze_data_quality(data)
    
    print("\n" + "=" * 60)
    
    # åˆ†æå†…å®¹æ¨¡å¼
    term_patterns, alias_patterns = analyze_content_patterns(data)
    
    print("\n" + "=" * 60)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    report = generate_comprehensive_report(
        data, category_counts, tag_counts, tag_usage_by_group,
        completeness, term_patterns, alias_patterns, tags_by_group
    )
    
    print("\n" + "=" * 60)
    print("ğŸ“Š åˆ†æå®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  ğŸ“‹ è¯å…¸ç³»ç»Ÿæ·±åº¦æ•°æ®åˆ†ææŠ¥å‘Š.md")
    print("\nğŸ‰ è¯å…¸æ•°æ®ç»Ÿè®¡åˆ†æå®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    main()
