#!/usr/bin/env python3
"""
Excelæ•°æ®ç»“æ„åˆ†æå·¥å…·
åˆ†æç”¨æˆ·æä¾›çš„Excelæ–‡ä»¶ï¼Œè¯†åˆ«å­—æ®µç±»å‹ã€æ•°æ®è´¨é‡ã€å®ä½“å…³ç³»
ä¸ºè‡ªåŠ¨æŠ½å–å’ŒçŸ¥è¯†å›¾è°±æ„å»ºåšå‡†å¤‡
"""
import pandas as pd
import numpy as np
from pathlib import Path
import re
from collections import Counter
import json

def analyze_excel_structure(file_path):
    """åˆ†æExcelæ–‡ä»¶ç»“æ„"""
    print(f"ğŸ” åˆ†ææ–‡ä»¶: {file_path}")
    
    try:
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(file_path)
        
        print(f"ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
        print(f"  - è¡Œæ•°: {len(df)}")
        print(f"  - åˆ—æ•°: {len(df.columns)}")
        print(f"  - æ–‡ä»¶å¤§å°: {Path(file_path).stat().st_size / 1024:.1f} KB")
        
        print(f"\nğŸ“‹ åˆ—åä¿¡æ¯:")
        for i, col in enumerate(df.columns):
            print(f"  {i+1:2d}. {col}")
            
        print(f"\nğŸ” æ•°æ®ç±»å‹åˆ†æ:")
        for col in df.columns:
            dtype = df[col].dtype
            null_count = df[col].isnull().sum()
            unique_count = df[col].nunique()
            
            print(f"  {col}:")
            print(f"    - ç±»å‹: {dtype}")
            print(f"    - ç©ºå€¼: {null_count}/{len(df)} ({null_count/len(df)*100:.1f}%)")
            print(f"    - å”¯ä¸€å€¼: {unique_count}")
            
            # æ˜¾ç¤ºæ ·ä¾‹æ•°æ®
            sample_values = df[col].dropna().head(3).tolist()
            print(f"    - æ ·ä¾‹: {sample_values}")
            print()
            
        return df
        
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None

def identify_entity_types(df):
    """è¯†åˆ«å¯èƒ½çš„å®ä½“ç±»å‹"""
    print("ğŸ¯ å®ä½“ç±»å‹è¯†åˆ«:")
    
    entity_patterns = {
        'AnomalyID': [r'å¼‚å¸¸.*ç¼–å·', r'é—®é¢˜.*ç¼–å·', r'ç¼ºé™·.*ç¼–å·', r'ID', r'ç¼–å·'],
        'Product': [r'äº§å“', r'æœºå‹', r'å‹å·', r'è®¾å¤‡'],
        'Component': [r'ç»„ä»¶', r'æ¨¡å—', r'éƒ¨ä»¶', r'å™¨ä»¶'],
        'Symptom': [r'ç—‡çŠ¶', r'ç°è±¡', r'é—®é¢˜.*æè¿°', r'æ•…éšœ.*ç°è±¡'],
        'Severity': [r'ä¸¥é‡.*ç¨‹åº¦', r'çº§åˆ«', r'ä¼˜å…ˆçº§'],
        'Status': [r'çŠ¶æ€', r'è¿›åº¦'],
        'Date': [r'æ—¥æœŸ', r'æ—¶é—´', r'åˆ›å»º.*æ—¶é—´'],
        'Owner': [r'è´Ÿè´£äºº', r'å¤„ç†äºº', r'åˆ†é….*ç»™'],
        'Description': [r'æè¿°', r'è¯´æ˜', r'è¯¦æƒ…', r'å¤‡æ³¨']
    }
    
    identified_entities = {}
    
    for col in df.columns:
        col_lower = col.lower()
        for entity_type, patterns in entity_patterns.items():
            for pattern in patterns:
                if re.search(pattern, col, re.IGNORECASE):
                    identified_entities[col] = entity_type
                    print(f"  âœ… {col} -> {entity_type}")
                    break
            if col in identified_entities:
                break
    
    # æœªè¯†åˆ«çš„åˆ—
    unidentified = [col for col in df.columns if col not in identified_entities]
    if unidentified:
        print(f"  â“ æœªè¯†åˆ«åˆ—: {unidentified}")
    
    return identified_entities

def analyze_data_quality(df):
    """åˆ†ææ•°æ®è´¨é‡"""
    print("ğŸ“ˆ æ•°æ®è´¨é‡åˆ†æ:")
    
    # æ•´ä½“å®Œæ•´æ€§
    total_cells = len(df) * len(df.columns)
    null_cells = df.isnull().sum().sum()
    completeness = (total_cells - null_cells) / total_cells * 100
    
    print(f"  - æ•°æ®å®Œæ•´æ€§: {completeness:.1f}%")
    
    # é‡å¤è¡Œæ£€æŸ¥
    duplicate_rows = df.duplicated().sum()
    print(f"  - é‡å¤è¡Œ: {duplicate_rows}")
    
    # å­—æ®µå€¼åˆ†å¸ƒ
    print(f"  - å­—æ®µå€¼åˆ†å¸ƒ:")
    for col in df.columns:
        if df[col].dtype == 'object':
            value_counts = df[col].value_counts()
            if len(value_counts) <= 10:
                print(f"    {col}: {dict(value_counts.head())}")
            else:
                print(f"    {col}: {len(value_counts)} ä¸ªå”¯ä¸€å€¼")

def suggest_knowledge_graph_schema(df, identified_entities):
    """å»ºè®®çŸ¥è¯†å›¾è°±Schema"""
    print("ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±Schemaå»ºè®®:")
    
    # èŠ‚ç‚¹ç±»å‹å»ºè®®
    node_types = set(identified_entities.values())
    print(f"  ğŸ“ å»ºè®®èŠ‚ç‚¹ç±»å‹: {list(node_types)}")
    
    # å…³ç³»å»ºè®®
    relationships = []
    if 'AnomalyID' in identified_entities.values() and 'Component' in identified_entities.values():
        relationships.append("Anomaly -[:AFFECTS]-> Component")
    if 'AnomalyID' in identified_entities.values() and 'Symptom' in identified_entities.values():
        relationships.append("Anomaly -[:HAS_SYMPTOM]-> Symptom")
    if 'Product' in identified_entities.values() and 'Component' in identified_entities.values():
        relationships.append("Product -[:INCLUDES]-> Component")
    
    print(f"  ğŸ”— å»ºè®®å…³ç³»ç±»å‹:")
    for rel in relationships:
        print(f"    - {rel}")

def generate_extraction_config(df, identified_entities):
    """ç”ŸæˆæŠ½å–é…ç½®"""
    config = {
        'file_info': {
            'columns': df.columns.tolist(),
            'shape': df.shape,
            'dtypes': df.dtypes.astype(str).to_dict()
        },
        'entity_mapping': identified_entities,
        'extraction_rules': {}
    }
    
    # ä¸ºæ¯ä¸ªå®ä½“ç±»å‹ç”ŸæˆæŠ½å–è§„åˆ™
    for col, entity_type in identified_entities.items():
        config['extraction_rules'][col] = {
            'entity_type': entity_type,
            'required': df[col].isnull().sum() < len(df) * 0.5,  # å°‘äº50%ç©ºå€¼è®¤ä¸ºæ˜¯å¿…éœ€å­—æ®µ
            'unique': df[col].nunique() > len(df) * 0.8,  # è¶…è¿‡80%å”¯ä¸€å€¼è®¤ä¸ºæ˜¯æ ‡è¯†ç¬¦
            'sample_values': df[col].dropna().head(5).tolist()
        }
    
    return config

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Excelæ•°æ®ç»“æ„åˆ†æå·¥å…·")
    print("=" * 50)
    
    # æŸ¥æ‰¾Excelæ–‡ä»¶
    data_dir = Path('data')
    excel_files = []
    
    for pattern in ['*.xlsx', '*.xls']:
        excel_files.extend(data_dir.rglob(pattern))
    
    if not excel_files:
        print("âŒ æœªæ‰¾åˆ°Excelæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶:")
    for i, file in enumerate(excel_files):
        print(f"  {i+1}. {file}")
    
    # åˆ†ææ¯ä¸ªæ–‡ä»¶
    for file_path in excel_files:
        print(f"\n{'='*60}")
        df = analyze_excel_structure(file_path)
        
        if df is not None:
            identified_entities = identify_entity_types(df)
            analyze_data_quality(df)
            suggest_knowledge_graph_schema(df, identified_entities)
            
            # ç”Ÿæˆé…ç½®æ–‡ä»¶
            config = generate_extraction_config(df, identified_entities)
            config_file = f"extraction_config_{file_path.stem}.json"
            
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_file}")

if __name__ == "__main__":
    main()
