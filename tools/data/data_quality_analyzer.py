#!/usr/bin/env python3
"""
æ•°æ®è´¨é‡åˆ†æå·¥å…·
æ£€æŸ¥å„ä¸ªæ•°æ®æºçš„å®Œæ•´æ€§ï¼Œæ‰¾å‡ºç©ºç¼ºæ•°æ®
"""
import csv
import json
from pathlib import Path
from typing import Dict, List, Any, Set
import pandas as pd

def analyze_csv_data_quality(file_path: Path, name: str) -> Dict[str, Any]:
    """åˆ†æCSVæ–‡ä»¶çš„æ•°æ®è´¨é‡"""
    if not file_path.exists():
        return {"name": name, "exists": False, "error": "æ–‡ä»¶ä¸å­˜åœ¨"}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            rows = list(reader)
        
        if not rows:
            return {"name": name, "exists": True, "total": 0, "error": "æ–‡ä»¶ä¸ºç©º"}
        
        # åˆ†æå­—æ®µå®Œæ•´æ€§
        fields = list(rows[0].keys())
        field_stats = {}
        
        for field in fields:
            empty_count = sum(1 for row in rows if not row.get(field, "").strip())
            field_stats[field] = {
                "total": len(rows),
                "empty": empty_count,
                "filled": len(rows) - empty_count,
                "completeness": (len(rows) - empty_count) / len(rows) * 100
            }
        
        # æ‰¾å‡ºç©ºç¼ºæ•°æ®çš„è¡Œ
        incomplete_rows = []
        for i, row in enumerate(rows):
            missing_fields = []
            for field in fields:
                if not row.get(field, "").strip():
                    missing_fields.append(field)
            if missing_fields:
                incomplete_rows.append({
                    "row": i + 2,  # +2 å› ä¸ºæœ‰æ ‡é¢˜è¡Œï¼Œä¸”ä»1å¼€å§‹è®¡æ•°
                    "data": row,
                    "missing_fields": missing_fields
                })
        
        return {
            "name": name,
            "exists": True,
            "total": len(rows),
            "fields": fields,
            "field_stats": field_stats,
            "incomplete_rows": incomplete_rows,
            "completeness_score": sum(stats["completeness"] for stats in field_stats.values()) / len(field_stats)
        }
        
    except Exception as e:
        return {"name": name, "exists": True, "error": str(e)}

def analyze_json_data_quality(file_path: Path, name: str) -> Dict[str, Any]:
    """åˆ†æJSONæ–‡ä»¶çš„æ•°æ®è´¨é‡"""
    if not file_path.exists():
        return {"name": name, "exists": False, "error": "æ–‡ä»¶ä¸å­˜åœ¨"}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        entries = data.get('entries', [])
        if not entries:
            return {"name": name, "exists": True, "total": 0, "error": "æ— æ¡ç›®æ•°æ®"}
        
        # åˆ†æå­—æ®µå®Œæ•´æ€§
        required_fields = ['term', 'aliases', 'category', 'tags', 'definition']
        field_stats = {}
        
        for field in required_fields:
            empty_count = 0
            for entry in entries:
                value = entry.get(field)
                if not value or (isinstance(value, str) and not value.strip()) or (isinstance(value, list) and not value):
                    empty_count += 1
            
            field_stats[field] = {
                "total": len(entries),
                "empty": empty_count,
                "filled": len(entries) - empty_count,
                "completeness": (len(entries) - empty_count) / len(entries) * 100
            }
        
        # æ‰¾å‡ºç©ºç¼ºæ•°æ®çš„æ¡ç›®
        incomplete_entries = []
        for i, entry in enumerate(entries):
            missing_fields = []
            for field in required_fields:
                value = entry.get(field)
                if not value or (isinstance(value, str) and not value.strip()) or (isinstance(value, list) and not value):
                    missing_fields.append(field)
            if missing_fields:
                incomplete_entries.append({
                    "index": i,
                    "term": entry.get('term', 'æœªçŸ¥'),
                    "missing_fields": missing_fields
                })
        
        return {
            "name": name,
            "exists": True,
            "total": len(entries),
            "fields": required_fields,
            "field_stats": field_stats,
            "incomplete_entries": incomplete_entries,
            "completeness_score": sum(stats["completeness"] for stats in field_stats.values()) / len(field_stats)
        }
        
    except Exception as e:
        return {"name": name, "exists": True, "error": str(e)}

def print_quality_report(analysis: Dict[str, Any]):
    """æ‰“å°æ•°æ®è´¨é‡æŠ¥å‘Š"""
    print(f"\nğŸ“Š {analysis['name']} æ•°æ®è´¨é‡æŠ¥å‘Š")
    print("=" * 60)
    
    if not analysis.get('exists'):
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    if 'error' in analysis:
        print(f"âŒ é”™è¯¯: {analysis['error']}")
        return
    
    print(f"ğŸ“ˆ æ€»è®°å½•æ•°: {analysis['total']}")
    print(f"ğŸ¯ å®Œæ•´æ€§è¯„åˆ†: {analysis['completeness_score']:.1f}%")
    
    print(f"\nğŸ“‹ å­—æ®µå®Œæ•´æ€§:")
    for field, stats in analysis['field_stats'].items():
        status = "âœ…" if stats['completeness'] == 100 else "âš ï¸" if stats['completeness'] >= 80 else "âŒ"
        print(f"  {status} {field}: {stats['filled']}/{stats['total']} ({stats['completeness']:.1f}%)")
    
    # æ˜¾ç¤ºä¸å®Œæ•´çš„è®°å½•
    if 'incomplete_rows' in analysis:
        incomplete_items = analysis['incomplete_rows']
    else:
        incomplete_items = analysis.get('incomplete_entries', [])
    
    if incomplete_items:
        print(f"\nâš ï¸ ä¸å®Œæ•´è®°å½• ({len(incomplete_items)} æ¡):")
        for item in incomplete_items[:10]:  # åªæ˜¾ç¤ºå‰10æ¡
            if 'row' in item:
                print(f"  è¡Œ {item['row']}: {item['data'].get('term', item['data'].get('name', 'æœªçŸ¥'))} - ç¼ºå¤±: {', '.join(item['missing_fields'])}")
            else:
                print(f"  {item['term']} - ç¼ºå¤±: {', '.join(item['missing_fields'])}")
        if len(incomplete_items) > 10:
            print(f"  ... è¿˜æœ‰ {len(incomplete_items) - 10} æ¡ä¸å®Œæ•´è®°å½•")

def recommend_best_data_source(analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
    """æ¨èæœ€ä½³æ•°æ®æº"""
    print(f"\nğŸ† æ•°æ®æºè´¨é‡æ’å")
    print("=" * 60)
    
    valid_analyses = [a for a in analyses if a.get('exists') and 'error' not in a and a.get('total', 0) > 0]
    
    if not valid_analyses:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®æº")
        return None
    
    # æŒ‰å®Œæ•´æ€§è¯„åˆ†æ’åº
    sorted_analyses = sorted(valid_analyses, key=lambda x: x['completeness_score'], reverse=True)
    
    for i, analysis in enumerate(sorted_analyses):
        rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i] if i < 3 else f"{i+1}."
        print(f"{rank_emoji} {analysis['name']}")
        print(f"   ğŸ“Š å®Œæ•´æ€§: {analysis['completeness_score']:.1f}%")
        print(f"   ğŸ“ˆ è®°å½•æ•°: {analysis['total']}")
        print(f"   ğŸ“‹ å­—æ®µæ•°: {len(analysis['fields'])}")
    
    best_source = sorted_analyses[0]
    print(f"\nâœ… æ¨èä½¿ç”¨: {best_source['name']}")
    print(f"   ç†ç”±: å®Œæ•´æ€§æœ€é«˜ ({best_source['completeness_score']:.1f}%)ï¼Œæ•°æ®æœ€å®Œæ•´")
    
    return best_source

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ•°æ®æºè´¨é‡åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    
    # å®šä¹‰è¦åˆ†æçš„æ•°æ®æº
    data_sources = [
        {
            "name": "ontology/dictionaries/components.csv",
            "path": Path("ontology/dictionaries/components.csv"),
            "type": "csv"
        },
        {
            "name": "ontology/dictionaries/symptoms.csv", 
            "path": Path("ontology/dictionaries/symptoms.csv"),
            "type": "csv"
        },
        {
            "name": "ontology/dictionaries/causes.csv",
            "path": Path("ontology/dictionaries/causes.csv"),
            "type": "csv"
        },
        {
            "name": "ontology/dictionaries/countermeasures.csv",
            "path": Path("ontology/dictionaries/countermeasures.csv"),
            "type": "csv"
        },
        {
            "name": "data/vocab/components.csv",
            "path": Path("data/vocab/components.csv"),
            "type": "csv"
        },
        {
            "name": "data/vocab/dictionary.json",
            "path": Path("data/vocab/dictionary.json"),
            "type": "json"
        }
    ]
    
    analyses = []
    
    # åˆ†ææ¯ä¸ªæ•°æ®æº
    for source in data_sources:
        if source["type"] == "csv":
            analysis = analyze_csv_data_quality(source["path"], source["name"])
        else:
            analysis = analyze_json_data_quality(source["path"], source["name"])
        
        analyses.append(analysis)
        print_quality_report(analysis)
    
    # æ¨èæœ€ä½³æ•°æ®æº
    best_source = recommend_best_data_source(analyses)
    
    # ç”Ÿæˆæ¸…ç†å»ºè®®
    print(f"\nğŸ§¹ æ•°æ®æ¸…ç†å»ºè®®")
    print("=" * 60)
    print("1. ä½¿ç”¨ ontology/dictionaries/ ä½œä¸ºä¸»è¦æ•°æ®æº")
    print("   - æ•°æ®æœ€å®Œæ•´ï¼Œå­—æ®µæœ€ä¸°å¯Œ")
    print("   - åŒ…å« term, canonical_name, aliases, category, tags, description")
    print("   - å»ºè®®ä½œä¸ºç»Ÿä¸€æ ‡å‡†")
    
    print("\n2. æ¸…ç†å’Œåˆå¹¶ç­–ç•¥:")
    print("   - ä¿ç•™ ontology/dictionaries/ ç›®å½•")
    print("   - åˆ é™¤æˆ–å½’æ¡£ data/vocab/ ä¸­çš„é‡å¤æ•°æ®")
    print("   - ç»Ÿä¸€ä½¿ç”¨æ ‡å‡†å­—æ®µæ ¼å¼")
    
    print("\n3. æ•°æ®å®Œæ•´æ€§è¦æ±‚:")
    print("   - term (å¿…å¡«): æœ¯è¯­åç§°")
    print("   - canonical_name (å¿…å¡«): æ ‡å‡†åç§°") 
    print("   - category (å¿…å¡«): åˆ†ç±»")
    print("   - description (æ¨è): æè¿°ä¿¡æ¯")
    print("   - aliases (å¯é€‰): åˆ«ååˆ—è¡¨")
    print("   - tags (å¯é€‰): æ ‡ç­¾åˆ—è¡¨")

if __name__ == "__main__":
    main()
