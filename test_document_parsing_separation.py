#!/usr/bin/env python3
"""
æµ‹è¯•æ–‡æ¡£è§£æä¸å›¾è°±æ„å»ºåˆ†ç¦»åŠŸèƒ½
"""

import requests
import time
import json
import pandas as pd
from pathlib import Path

API_BASE = "http://127.0.0.1:8000"

def create_document_parsing_test_file():
    """åˆ›å»ºä¸“é—¨ç”¨äºæ–‡æ¡£è§£ææµ‹è¯•çš„æ–‡ä»¶"""
    data = {
        "åºå·": [1, 2, 3, 4, 5],
        "é—®é¢˜ç¼–å·": ["ISSUE-001", "ISSUE-002", "ISSUE-003", "ISSUE-004", "ISSUE-005"],
        "é—®é¢˜æè¿°": [
            "æ‘„åƒå¤´å¯¹ç„¦å¤±è´¥ï¼Œæ— æ³•æ­£å¸¸æ‹ç…§",
            "æ˜¾ç¤ºå±å‡ºç°é—ªçƒç°è±¡ï¼Œå½±å“ä½¿ç”¨",
            "å……ç”µé€Ÿåº¦æ˜æ˜¾å˜æ…¢ï¼Œå……ç”µæ—¶é—´å»¶é•¿",
            "è®¾å¤‡å‘çƒ­ä¸¥é‡ï¼Œæ¸©åº¦è¿‡é«˜",
            "ç³»ç»Ÿç»å¸¸æ­»æœºé‡å¯ï¼Œç¨³å®šæ€§å·®"
        ],
        "å‘ç°æ—¥æœŸ": ["2024-01-15", "2024-01-16", "2024-01-17", "2024-01-18", "2024-01-19"],
        "ä¸¥é‡ç­‰çº§": ["é«˜", "ä¸­", "ä½", "é«˜", "ä¸­"],
        "è´£ä»»éƒ¨é—¨": ["ç¡¬ä»¶éƒ¨", "æ˜¾ç¤ºéƒ¨", "ç”µæºéƒ¨", "æ•£çƒ­éƒ¨", "è½¯ä»¶éƒ¨"],
        "å¤„ç†çŠ¶æ€": ["å¤„ç†ä¸­", "å·²è§£å†³", "å¾…å¤„ç†", "å¤„ç†ä¸­", "å·²è§£å†³"],
        "å¤‡æ³¨": [
            "éœ€è¦æ›´æ¢é•œå¤´æ¨¡ç»„",
            "å·²æ›´æ–°é©±åŠ¨ç¨‹åº",
            "å»ºè®®å‡çº§å……ç”µå™¨",
            "å¢åŠ æ•£çƒ­ç‰‡",
            "å·²å‘å¸ƒç³»ç»Ÿè¡¥ä¸"
        ]
    }
    
    df = pd.DataFrame(data)
    test_file = Path("document_parsing_test.xlsx")
    df.to_excel(test_file, index=False, sheet_name="é—®é¢˜æ¸…å•")
    
    print(f"âœ… åˆ›å»ºæ–‡æ¡£è§£ææµ‹è¯•æ–‡ä»¶: {test_file}")
    print(f"ğŸ“Š æ•°æ®è¡Œæ•°: {len(df)}")
    print(f"ğŸ“‹ å­—æ®µæ•°: {len(df.columns)}")
    print(f"ğŸ“„ å­—æ®µåˆ—è¡¨: {list(df.columns)}")
    
    return test_file

def test_document_parsing_only():
    """æµ‹è¯•çº¯æ–‡æ¡£è§£æåŠŸèƒ½ï¼ˆä¸æ¶‰åŠå›¾è°±æ„å»ºï¼‰"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•çº¯æ–‡æ¡£è§£æåŠŸèƒ½...")
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_file = create_document_parsing_test_file()
    
    try:
        # æ­¥éª¤1: ä¸Šä¼ æ–‡æ¡£
        print(f"\nğŸ“¤ æ­¥éª¤1: ä¸Šä¼ æ–‡æ¡£...")
        with open(test_file, "rb") as f:
            files = {"file": (test_file.name, f, "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")}
            upload_response = requests.post(f"{API_BASE}/kg/upload", files=files)
        
        upload_result = upload_response.json()
        if not upload_result.get("success"):
            print(f"   âŒ æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {upload_result.get('message')}")
            return False
        
        upload_id = upload_result.get("upload_id")
        print(f"   âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {upload_id}")
        
        # æ­¥éª¤2: ç­‰å¾…æ–‡æ¡£è§£æå®Œæˆ
        print(f"\nâ³ æ­¥éª¤2: ç­‰å¾…æ–‡æ¡£è§£æ...")
        max_attempts = 15
        for attempt in range(max_attempts):
            status_response = requests.get(f"{API_BASE}/kg/files/{upload_id}/status")
            status_result = status_response.json()
            
            if status_result.get("success"):
                file_status = status_result["data"]["status"]
                print(f"   å°è¯• {attempt + 1}: {file_status}")
                
                if file_status == "parsed":
                    print("   âœ… æ–‡æ¡£è§£æå®Œæˆ!")
                    break
                elif file_status == "failed":
                    error = status_result["data"].get("error", "æœªçŸ¥é”™è¯¯")
                    print(f"   âŒ æ–‡æ¡£è§£æå¤±è´¥: {error}")
                    return False
                
                time.sleep(2)
            else:
                print(f"   âŒ çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {status_result.get('message')}")
                return False
        else:
            print("   â° æ–‡æ¡£è§£æè¶…æ—¶")
            return False
        
        # æ­¥éª¤3: è·å–è§£æç»“æœ
        print(f"\nğŸ“‹ æ­¥éª¤3: è·å–æ–‡æ¡£è§£æç»“æœ...")
        preview_response = requests.get(f"{API_BASE}/kg/files/{upload_id}/preview")
        preview_result = preview_response.json()
        
        if not preview_result.get("success"):
            print(f"   âŒ è·å–è§£æç»“æœå¤±è´¥: {preview_result.get('message')}")
            return False
        
        preview_data = preview_result["data"]
        raw_data = preview_data.get("raw_data", [])
        entities = preview_data.get("entities", [])
        metadata = preview_data.get("metadata", {})
        
        print(f"   âœ… æ–‡æ¡£è§£æç»“æœè·å–æˆåŠŸ!")
        
        # æ­¥éª¤4: åˆ†æåŸå§‹æ•°æ®æå–
        print(f"\nğŸ“Š æ­¥éª¤4: åŸå§‹æ•°æ®åˆ†æ...")
        if raw_data:
            print(f"   ğŸ“‹ æå–è®°å½•æ•°: {len(raw_data)}")
            if raw_data:
                first_record = raw_data[0]
                print(f"   ğŸ“„ å­—æ®µæ•°é‡: {len(first_record.keys())}")
                print(f"   ğŸ·ï¸ å­—æ®µåˆ—è¡¨: {list(first_record.keys())}")
                
                # æ˜¾ç¤ºå‰3æ¡è®°å½•
                print(f"   ğŸ“ æ•°æ®ç¤ºä¾‹:")
                for i, record in enumerate(raw_data[:3]):
                    print(f"      è®°å½•{i+1}: {record.get('é—®é¢˜ç¼–å·', 'N/A')} - {record.get('é—®é¢˜æè¿°', 'N/A')[:20]}...")
        else:
            print(f"   âš ï¸ æœªæå–åˆ°åŸå§‹æ•°æ®")
        
        # æ­¥éª¤5: åˆ†æè¯†åˆ«çš„å®ä½“ï¼ˆå¦‚æœæœ‰ï¼‰
        print(f"\nğŸ·ï¸ æ­¥éª¤5: å®ä½“è¯†åˆ«åˆ†æ...")
        if entities:
            entity_types = {}
            for entity in entities:
                etype = entity.get("type", "Unknown")
                if etype not in entity_types:
                    entity_types[etype] = []
                entity_types[etype].append(entity.get("name"))
            
            print(f"   ğŸ“Š è¯†åˆ«å®ä½“æ•°: {len(entities)}")
            for etype, names in entity_types.items():
                print(f"   {etype}: {len(names)}ä¸ª - {', '.join(names[:3])}{'...' if len(names) > 3 else ''}")
        else:
            print(f"   â„¹ï¸ æœªè¿›è¡Œå®ä½“è¯†åˆ«ï¼ˆçº¯æ–‡æ¡£è§£ææ¨¡å¼ï¼‰")
        
        # æ­¥éª¤6: å…ƒæ•°æ®åˆ†æ
        print(f"\nğŸ“ˆ æ­¥éª¤6: è§£æå…ƒæ•°æ®...")
        for key, value in metadata.items():
            print(f"   {key}: {value}")
        
        # æ­¥éª¤7: æ•°æ®è´¨é‡è¯„ä¼°
        print(f"\nğŸ¯ æ­¥éª¤7: æ•°æ®è´¨é‡è¯„ä¼°...")
        
        total_records = len(raw_data) if raw_data else metadata.get("total_records", 0)
        field_count = len(raw_data[0].keys()) if raw_data else metadata.get("field_count", 0)
        
        # è®¡ç®—æ•°æ®å®Œæ•´æ€§
        if raw_data:
            complete_records = 0
            for record in raw_data:
                if all(value and str(value).strip() for value in record.values()):
                    complete_records += 1
            
            completeness = (complete_records / total_records) * 100 if total_records > 0 else 0
        else:
            completeness = 0
        
        print(f"   ğŸ“Š æ€»è®°å½•æ•°: {total_records}")
        print(f"   ğŸ“„ å­—æ®µæ•°é‡: {field_count}")
        print(f"   âœ… æ•°æ®å®Œæ•´æ€§: {completeness:.1f}%")
        print(f"   ğŸ¯ è§£æè´¨é‡: {'ä¼˜ç§€' if completeness > 90 else 'è‰¯å¥½' if completeness > 70 else 'ä¸€èˆ¬'}")
        
        # æ­¥éª¤8: æ¨¡æ‹Ÿå¯¼å‡ºåŠŸèƒ½
        print(f"\nğŸ’¾ æ­¥éª¤8: æ¨¡æ‹Ÿæ•°æ®å¯¼å‡º...")
        
        export_data = {
            "file_info": {
                "filename": test_file.name,
                "upload_id": upload_id,
                "parsing_time": metadata.get("parsing_time", "unknown")
            },
            "raw_data": raw_data,
            "statistics": {
                "total_records": total_records,
                "field_count": field_count,
                "completeness": f"{completeness:.1f}%"
            },
            "metadata": metadata,
            "export_time": "2024-01-20T10:30:00Z"
        }
        
        export_file = Path("parsed_data_export.json")
        export_file.write_text(json.dumps(export_data, ensure_ascii=False, indent=2), encoding="utf-8")
        
        print(f"   âœ… è§£ææ•°æ®å·²å¯¼å‡ºåˆ°: {export_file}")
        print(f"   ğŸ“Š å¯¼å‡ºæ–‡ä»¶å¤§å°: {export_file.stat().st_size} å­—èŠ‚")
        
        # æ¸…ç†å¯¼å‡ºæ–‡ä»¶
        export_file.unlink(missing_ok=True)
        
        return True
    
    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        test_file.unlink(missing_ok=True)

def test_parsing_vs_graph_separation():
    """æµ‹è¯•è§£æä¸å›¾è°±æ„å»ºçš„åˆ†ç¦»"""
    print("\nğŸ”„ æµ‹è¯•è§£æä¸å›¾è°±æ„å»ºåˆ†ç¦»...")
    
    print("ğŸ“„ æ–‡æ¡£è§£æé˜¶æ®µ:")
    print("   âœ… ä¸“æ³¨äºä»æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–æ•°æ®")
    print("   âœ… ä¸æ¶‰åŠè¯­ä¹‰ç†è§£å’ŒçŸ¥è¯†å»ºæ¨¡")
    print("   âœ… è¾“å‡ºåŸå§‹æ•°æ®å’ŒåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯")
    print("   âœ… æ”¯æŒæ•°æ®å¯¼å‡ºå’Œè´¨é‡è¯„ä¼°")
    
    print("\nğŸ•¸ï¸ å›¾è°±æ„å»ºé˜¶æ®µï¼ˆæœªå®ç°ï¼‰:")
    print("   ğŸ”„ ä»è§£ææ•°æ®ä¸­è¯†åˆ«å®ä½“å’Œå…³ç³»")
    print("   ğŸ”„ åº”ç”¨ä¸šåŠ¡è§„åˆ™å’Œæœ¬ä½“æ¨¡å‹")
    print("   ğŸ”„ æ„å»ºçŸ¥è¯†å›¾è°±å¹¶å­˜å‚¨åˆ°Neo4j")
    print("   ğŸ”„ æä¾›å›¾è°±æŸ¥è¯¢å’Œåˆ†æåŠŸèƒ½")
    
    print("\nğŸ¯ åˆ†ç¦»çš„ä¼˜åŠ¿:")
    print("   âœ… èŒè´£æ¸…æ™°ï¼šè§£æä¸“æ³¨æ•°æ®æå–ï¼Œæ„å»ºä¸“æ³¨çŸ¥è¯†å»ºæ¨¡")
    print("   âœ… çµæ´»ä½¿ç”¨ï¼šå¯ä»¥åªä½¿ç”¨è§£æåŠŸèƒ½ï¼Œä¸å¿…æ„å»ºå›¾è°±")
    print("   âœ… æ˜“äºç»´æŠ¤ï¼šä¸¤ä¸ªæ¨¡å—å¯ä»¥ç‹¬ç«‹å¼€å‘å’Œä¼˜åŒ–")
    print("   âœ… ç”¨æˆ·å‹å¥½ï¼šç”¨æˆ·æ˜ç¡®çŸ¥é“æ¯ä¸ªæ­¥éª¤åœ¨åšä»€ä¹ˆ")

if __name__ == "__main__":
    print("ğŸ§ª æ–‡æ¡£è§£æä¸å›¾è°±æ„å»ºåˆ†ç¦»æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•çº¯æ–‡æ¡£è§£æåŠŸèƒ½
    if test_document_parsing_only():
        print("\nğŸ‰ æ–‡æ¡£è§£æåŠŸèƒ½æµ‹è¯•æˆåŠŸ!")
        print("âœ… æˆåŠŸæå–æ–‡æ¡£ä¸­çš„ç»“æ„åŒ–æ•°æ®")
        print("âœ… æ•°æ®è´¨é‡è¯„ä¼°åŠŸèƒ½æ­£å¸¸")
        print("âœ… è§£æç»“æœå±•ç¤ºå®Œæ•´")
    else:
        print("\nâŒ æ–‡æ¡£è§£æåŠŸèƒ½æµ‹è¯•å¤±è´¥!")
        print("âš ï¸ è¯·æ£€æŸ¥APIæœåŠ¡å’Œè§£æé€»è¾‘")
    
    # æµ‹è¯•åˆ†ç¦»è®¾è®¡
    test_parsing_vs_graph_separation()
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ")
