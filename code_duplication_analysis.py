#!/usr/bin/env python3
"""
ä»£ç é‡å¤åº¦åˆ†æå·¥å…·
"""
import os
import hashlib
from pathlib import Path
from collections import defaultdict
import re

def get_file_hash(file_path):
    """è®¡ç®—æ–‡ä»¶å†…å®¹çš„å“ˆå¸Œå€¼"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # å»é™¤ç©ºç™½è¡Œå’Œæ³¨é‡Šè¿›è¡Œæ¯”è¾ƒ
            lines = [line.strip() for line in content.split('\n') 
                    if line.strip() and not line.strip().startswith('#')]
            normalized_content = '\n'.join(lines)
            return hashlib.md5(normalized_content.encode()).hexdigest()
    except:
        return None

def analyze_function_similarity(file_path):
    """åˆ†ææ–‡ä»¶ä¸­çš„å‡½æ•°ç›¸ä¼¼åº¦"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–å‡½æ•°å®šä¹‰
        functions = re.findall(r'def\s+(\w+)\s*\([^)]*\):[^}]*?(?=\ndef|\nclass|\n@|\Z)', 
                              content, re.MULTILINE | re.DOTALL)
        return functions
    except:
        return []

def find_duplicate_files():
    """æŸ¥æ‰¾é‡å¤æ–‡ä»¶"""
    print("ğŸ” åˆ†æä»£ç é‡å¤åº¦...")
    print("=" * 60)
    
    file_hashes = defaultdict(list)
    api_files = []
    test_files = []
    config_files = []
    
    # æ‰«æé¡¹ç›®æ–‡ä»¶
    for root, dirs, files in os.walk('.'):
        # è·³è¿‡ç‰¹å®šç›®å½•
        dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__', 'node_modules', '.venv']]
        
        for file in files:
            if file.endswith(('.py', '.js', '.ts', '.vue', '.yaml', '.yml', '.json')):
                file_path = Path(root) / file
                
                # åˆ†ç±»æ–‡ä»¶
                if 'api' in str(file_path) or 'main' in file:
                    api_files.append(file_path)
                elif 'test' in file:
                    test_files.append(file_path)
                elif file.endswith(('.yaml', '.yml', '.json', '.env')):
                    config_files.append(file_path)
                
                # è®¡ç®—å“ˆå¸Œ
                file_hash = get_file_hash(file_path)
                if file_hash:
                    file_hashes[file_hash].append(file_path)
    
    # åˆ†æç»“æœ
    print("ğŸ“Š æ–‡ä»¶ç»Ÿè®¡:")
    print(f"   APIæ–‡ä»¶: {len(api_files)}")
    print(f"   æµ‹è¯•æ–‡ä»¶: {len(test_files)}")
    print(f"   é…ç½®æ–‡ä»¶: {len(config_files)}")
    
    # æŸ¥æ‰¾å®Œå…¨é‡å¤çš„æ–‡ä»¶
    print("\nğŸš¨ å®Œå…¨é‡å¤çš„æ–‡ä»¶:")
    duplicate_count = 0
    for file_hash, files in file_hashes.items():
        if len(files) > 1:
            duplicate_count += len(files) - 1
            print(f"   é‡å¤ç»„ {len(files)} ä¸ªæ–‡ä»¶:")
            for file_path in files:
                print(f"     - {file_path}")
    
    if duplicate_count == 0:
        print("   âœ… æœªå‘ç°å®Œå…¨é‡å¤çš„æ–‡ä»¶")
    else:
        print(f"   âŒ å‘ç° {duplicate_count} ä¸ªé‡å¤æ–‡ä»¶")
    
    return api_files, test_files, config_files

def analyze_api_duplication(api_files):
    """åˆ†æAPIæ–‡ä»¶çš„é‡å¤åº¦"""
    print("\nğŸ” APIæ–‡ä»¶é‡å¤åº¦åˆ†æ:")
    print("-" * 40)
    
    # åˆ†æAPIè·¯ç”±é‡å¤
    route_patterns = defaultdict(list)
    function_patterns = defaultdict(list)
    
    for file_path in api_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–è·¯ç”±å®šä¹‰
            routes = re.findall(r'@app\.(get|post|put|delete)\("([^"]+)"', content)
            for method, path in routes:
                route_key = f"{method.upper()} {path}"
                route_patterns[route_key].append(file_path)
            
            # æå–å‡½æ•°å
            functions = re.findall(r'def\s+(\w+)\s*\(', content)
            for func in functions:
                function_patterns[func].append(file_path)
                
        except Exception as e:
            print(f"   âŒ åˆ†æ {file_path} å¤±è´¥: {e}")
    
    # æŠ¥å‘Šè·¯ç”±é‡å¤
    print("ğŸ“ é‡å¤çš„APIè·¯ç”±:")
    route_duplicates = 0
    for route, files in route_patterns.items():
        if len(files) > 1:
            route_duplicates += 1
            print(f"   {route}:")
            for file_path in files:
                print(f"     - {file_path}")
    
    if route_duplicates == 0:
        print("   âœ… æœªå‘ç°é‡å¤çš„APIè·¯ç”±")
    else:
        print(f"   âŒ å‘ç° {route_duplicates} ä¸ªé‡å¤è·¯ç”±")
    
    # æŠ¥å‘Šå‡½æ•°é‡å¤
    print("\nğŸ”§ é‡å¤çš„å‡½æ•°å:")
    function_duplicates = 0
    common_functions = ['health_check', 'root', 'get_products', 'upload_document']
    
    for func, files in function_patterns.items():
        if len(files) > 1 and func in common_functions:
            function_duplicates += 1
            print(f"   {func}():")
            for file_path in files:
                print(f"     - {file_path}")
    
    if function_duplicates == 0:
        print("   âœ… æœªå‘ç°é‡å¤çš„æ ¸å¿ƒå‡½æ•°")
    else:
        print(f"   âŒ å‘ç° {function_duplicates} ä¸ªé‡å¤å‡½æ•°")

def analyze_config_duplication(config_files):
    """åˆ†æé…ç½®æ–‡ä»¶é‡å¤åº¦"""
    print("\nâš™ï¸ é…ç½®æ–‡ä»¶é‡å¤åº¦åˆ†æ:")
    print("-" * 40)
    
    requirements_files = [f for f in config_files if 'requirements' in str(f)]
    yaml_files = [f for f in config_files if f.suffix in ['.yaml', '.yml']]
    
    print(f"ğŸ“‹ Requirementsæ–‡ä»¶: {len(requirements_files)}")
    for req_file in requirements_files:
        print(f"   - {req_file}")
    
    print(f"ğŸ“‹ YAMLé…ç½®æ–‡ä»¶: {len(yaml_files)}")
    for yaml_file in yaml_files:
        print(f"   - {yaml_file}")
    
    # åˆ†ærequirementsé‡å¤
    if len(requirements_files) > 1:
        print("\nâŒ å‘ç°å¤šä¸ªrequirementsæ–‡ä»¶ï¼Œå¯èƒ½å­˜åœ¨ä¾èµ–å†²çª")
    else:
        print("\nâœ… Requirementsæ–‡ä»¶é…ç½®æ­£å¸¸")

def generate_cleanup_recommendations():
    """ç”Ÿæˆæ¸…ç†å»ºè®®"""
    print("\n" + "=" * 60)
    print("ğŸ¯ ä»£ç æ¸…ç†å»ºè®®")
    print("=" * 60)
    
    recommendations = [
        {
            "priority": "ğŸ”¥ é«˜ä¼˜å…ˆçº§",
            "items": [
                "åˆ é™¤é‡å¤çš„APIæœåŠ¡æ–‡ä»¶ (ä¿ç•™ api/main_v01.py)",
                "åˆå¹¶é‡å¤çš„requirements.txtæ–‡ä»¶",
                "åˆ é™¤æœªä½¿ç”¨çš„å¯åŠ¨è„šæœ¬",
                "æ¸…ç†é‡å¤çš„æµ‹è¯•æ–‡ä»¶"
            ]
        },
        {
            "priority": "âš¡ ä¸­ä¼˜å…ˆçº§", 
            "items": [
                "ç»Ÿä¸€æ•°æ®æ¨¡å‹å®šä¹‰ (ä½¿ç”¨Pydantic v2)",
                "é‡æ„é‡å¤çš„å‡½æ•°é€»è¾‘",
                "æ•´ç†é…ç½®æ–‡ä»¶ç»“æ„",
                "ä¼˜åŒ–ç›®å½•ç»„ç»‡"
            ]
        },
        {
            "priority": "ğŸ’¡ ä½ä¼˜å…ˆçº§",
            "items": [
                "æ·»åŠ ä»£ç å¤ç”¨æ£€æŸ¥å·¥å…·",
                "å»ºç«‹ä»£ç è§„èŒƒæ–‡æ¡£",
                "å®ç°è‡ªåŠ¨åŒ–é‡å¤æ£€æµ‹",
                "ä¼˜åŒ–å¯¼å…¥ç»“æ„"
            ]
        }
    ]
    
    for rec in recommendations:
        print(f"\n{rec['priority']}:")
        for item in rec['items']:
            print(f"   â€¢ {item}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä»£ç é‡å¤åº¦åˆ†ææŠ¥å‘Š")
    print("=" * 60)
    
    # åˆ†ææ–‡ä»¶é‡å¤
    api_files, test_files, config_files = find_duplicate_files()
    
    # åˆ†æAPIé‡å¤
    analyze_api_duplication(api_files)
    
    # åˆ†æé…ç½®é‡å¤
    analyze_config_duplication(config_files)
    
    # ç”Ÿæˆå»ºè®®
    generate_cleanup_recommendations()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š åˆ†æå®Œæˆ")
    print("=" * 60)

if __name__ == "__main__":
    main()
