#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from api.parsers.ir_unified_parser import IRUnifiedParser
from api.parsers.ir_core import IRConverter
from pathlib import Path
import json

def test_full_docx_flow():
    """æµ‹è¯•å®Œæ•´çš„DOCXè§£ææµç¨‹"""
    
    # æ‰¾åˆ°ä¸€ä¸ªçœŸæ­£çš„DOCXæ–‡ä»¶
    docx_file = Path("api/uploads/357d434f-3011-4732-aec6-6217392bfe3f")
    
    if not docx_file.exists():
        print(f"âŒ DOCXæ–‡ä»¶ä¸å­˜åœ¨: {docx_file}")
        return
    
    print(f"ğŸ“„ æµ‹è¯•DOCXæ–‡ä»¶: {docx_file}")
    print(f"   æ–‡ä»¶å¤§å°: {docx_file.stat().st_size} bytes")
    
    try:
        # æ­¥éª¤1: IRè§£æ
        print("\nğŸ”„ æ­¥éª¤1: IRè§£æ...")
        parser = IRUnifiedParser()
        ir_result = parser.parse_document(docx_file, ".docx")
        
        if not ir_result['success']:
            print(f"âŒ IRè§£æå¤±è´¥: {ir_result['error']}")
            return
        
        document_ir = ir_result['ir']
        print(f"âœ… IRè§£ææˆåŠŸ: {len(document_ir.blocks)} ä¸ªå†…å®¹å—")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªå—çš„ä¿¡æ¯
        print("   å‰5ä¸ªå†…å®¹å—:")
        for i, block in enumerate(document_ir.blocks[:5]):
            print(f"     å—{i+1}: {block.type.value} - {(block.text or '')[:50]}...")
        
        # æ­¥éª¤2: è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
        print("\nğŸ”„ æ­¥éª¤2: è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼...")
        preview_data = IRConverter.to_legacy_format(document_ir)
        
        print(f"âœ… æ ¼å¼è½¬æ¢æˆåŠŸ:")
        print(f"   raw_data: {len(preview_data.get('raw_data', []))} æ¡è®°å½•")
        print(f"   entities: {len(preview_data.get('entities', []))} ä¸ªå®ä½“")
        print(f"   relations: {len(preview_data.get('relations', []))} ä¸ªå…³ç³»")
        print(f"   metadata: {len(preview_data.get('metadata', {}))} ä¸ªå…ƒæ•°æ®å­—æ®µ")
        
        # æ˜¾ç¤ºå‰å‡ æ¡è®°å½•
        print("\n   å‰3æ¡è®°å½•:")
        for i, record in enumerate(preview_data.get('raw_data', [])[:3]):
            content_type = record.get('content_type', 'unknown')
            content = record.get('content', '')
            print(f"     è®°å½•{i+1}: {content_type} - {content[:80]}...")
        
        # æ­¥éª¤3: éªŒè¯æ•°æ®ç»“æ„
        print("\nğŸ”„ æ­¥éª¤3: éªŒè¯æ•°æ®ç»“æ„...")
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ['raw_data', 'entities', 'relations', 'metadata']
        missing_fields = [field for field in required_fields if field not in preview_data]
        
        if missing_fields:
            print(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
        else:
            print("âœ… æ•°æ®ç»“æ„å®Œæ•´")
        
        # æ£€æŸ¥raw_dataç»“æ„
        if preview_data.get('raw_data'):
            first_record = preview_data['raw_data'][0]
            print(f"   ç¬¬ä¸€æ¡è®°å½•å­—æ®µ: {list(first_record.keys())}")
        
        # æ­¥éª¤4: æ¨¡æ‹Ÿä¿å­˜å’ŒåŠ è½½
        print("\nğŸ”„ æ­¥éª¤4: æ¨¡æ‹Ÿä¿å­˜å’ŒåŠ è½½...")
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        test_file = "test_preview_data.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(preview_data, f, ensure_ascii=False, indent=2)
        
        # é‡æ–°åŠ è½½
        with open(test_file, 'r', encoding='utf-8') as f:
            loaded_data = json.load(f)
        
        print(f"âœ… æ•°æ®ä¿å­˜å’ŒåŠ è½½æˆåŠŸ")
        print(f"   ä¿å­˜çš„è®°å½•æ•°: {len(loaded_data.get('raw_data', []))}")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        os.remove(test_file)
        
        print("\nğŸ‰ å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸï¼")
        print("   DOCXæ–‡ä»¶å¯ä»¥æ­£å¸¸è§£æå¹¶è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_full_docx_flow()
