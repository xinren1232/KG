#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¡¥å……è¯å…¸æ•°æ®å¯¼å…¥è„šæœ¬
å°†æ–°çš„è¯å…¸æ•°æ®å¯¼å…¥åˆ°Neo4jæ•°æ®åº“ä¸­
"""

import pandas as pd
import numpy as np
from neo4j import GraphDatabase
import json
from datetime import datetime
import os
import argparse
from pathlib import Path
import shutil
import subprocess
import hashlib


class DictionaryDataImporter:
    def __init__(self, uri="bolt://localhost:7687", user="neo4j", password="password123"):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))

    def close(self):
        self.driver.close()

    def load_csv_data(self, csv_file):
        """åŠ è½½CSVæ•°æ®"""
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
            print(f"âœ… æˆåŠŸåŠ è½½ {csv_file}: {len(df)} æ¡è®°å½•")
            return df
        except Exception as e:
            print(f"âŒ åŠ è½½ {csv_file} å¤±è´¥: {e}")
            return None

    def validate_data(self, df):
        """éªŒè¯æ•°æ®è´¨é‡"""
        issues = []

        # æ£€æŸ¥å¿…å¡«å­—æ®µ
        required_fields = ['term', 'category']
        for field in required_fields:
            if field not in df.columns:
                issues.append(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")
            elif df[field].isna().any():
                null_count = df[field].isna().sum()
                issues.append(f"å­—æ®µ {field} æœ‰ {null_count} ä¸ªç©ºå€¼")

        # æ£€æŸ¥categoryæ˜¯å¦åœ¨å…è®¸çš„Labelä¸­
        valid_labels = ['Symptom', 'Component', 'Tool', 'Process', 'TestCase', 'Metric', 'Material', 'Role']
        if 'category' in df.columns:
            invalid_categories = df[~df['category'].isin(valid_labels)]['category'].unique()
            if len(invalid_categories) > 0:
                issues.append(f"æ— æ•ˆçš„categoryå€¼: {invalid_categories}")

        # æ£€æŸ¥é‡å¤æœ¯è¯­
        if 'term' in df.columns:
            duplicates = df[df['term'].duplicated()]['term'].unique()
            if len(duplicates) > 0:
                issues.append(f"é‡å¤çš„æœ¯è¯­: {duplicates}")

        return issues

    def check_existing_terms(self, df):
        """æ£€æŸ¥æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æœ¯è¯­"""
        with self.driver.session() as session:
            existing_terms = []
            for _, row in df.iterrows():
                term = row['term']
                category = row['category']

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                query = f"""
                MATCH (n:{category} {{name: $term}})
                RETURN n.name as name
                """
                result = session.run(query, term=term)
                if result.single():
                    existing_terms.append(term)

            return existing_terms

    def import_data(self, df, skip_existing=True):
        """å¯¼å…¥æ•°æ®åˆ°Neo4j"""
        success_count = 0
        error_count = 0
        skipped_count = 0

        # æ£€æŸ¥å·²å­˜åœ¨çš„æœ¯è¯­
        existing_terms = self.check_existing_terms(df) if skip_existing else []

        with self.driver.session() as session:
            for index, row in df.iterrows():
                try:
                    term = row['term']
                    category = row['category']

                    # è·³è¿‡å·²å­˜åœ¨çš„æœ¯è¯­
                    if skip_existing and term in existing_terms:
                        print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„æœ¯è¯­: {term}")
                        skipped_count += 1
                        continue

                    # å¤„ç†åˆ«å
                    aliases = []
                    if pd.notna(row.get('aliases', '')):
                        aliases = [alias.strip() for alias in str(row['aliases']).split(';') if alias.strip()]

                    # å¤„ç†æ ‡ç­¾
                    tags = []
                    if pd.notna(row.get('tags', '')):
                        tags = [tag.strip() for tag in str(row['tags']).split(';') if tag.strip()]

                    # æ„å»ºèŠ‚ç‚¹å±æ€§
                    properties = {
                        'name': term,
                        'aliases': aliases,
                        'tags': tags,
                        'definition': str(row.get('definition', '')).strip() if pd.notna(row.get('definition')) else '',
                        'example': str(row.get('example', '')).strip() if pd.notna(row.get('example')) else '',
                        'source': str(row.get('source', 'æ ‡å‡†åŒ–è¯å…¸')).strip(),
                        'status': str(row.get('status', 'active')).strip(),
                        'updated_at': str(row.get('updated_at', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))).strip()
                    }

                    # æ·»åŠ å­ç±»åˆ«
                    if pd.notna(row.get('sub_category')):
                        properties['sub_category'] = str(row['sub_category']).strip()

                    # åˆ›å»º/æ›´æ–°èŠ‚ç‚¹ï¼ˆå¹‚ç­‰ï¼‰ + ç»Ÿä¸€ä¸»é”® canonical_id å›å¡«
                    _prefix = {
                        'Component': 'CMP','Symptom': 'SYM','Tool': 'TOL','Process': 'PRC',
                        'TestCase': 'TST','Material': 'MAT','Role': 'ROL','Metric': 'MET'
                    }.get(category, 'UNK')
                    _hash = hashlib.sha1(f"{category}|{term.lower()}".encode('utf-8')).hexdigest()[:8].upper()
                    cid = f"{_prefix}_{_hash}"

                    query = f"""
                    MERGE (n:{category} {{name: $name}})
                    ON CREATE SET n += $properties,
                                   n.canonical_id = coalesce(n.canonical_id, $cid),
                                   n.created_at = coalesce(n.created_at, datetime())
                    ON MATCH SET n += $properties,
                                  n.canonical_id = coalesce(n.canonical_id, $cid),
                                  n.updated_at = datetime()
                    RETURN n.name as name
                    """

                    result = session.run(query, name=term, properties=properties, cid=cid)
                    if result.single():
                        print(f"âœ… æˆåŠŸå¯¼å…¥: {term} ({category})")
                        success_count += 1
                    else:
                        print(f"âŒ å¯¼å…¥å¤±è´¥: {term}")
                        error_count += 1

                except Exception as e:
                    print(f"âŒ å¯¼å…¥ {row.get('term', 'Unknown')} æ—¶å‡ºé”™: {e}")
                    error_count += 1

        return {
            'success': success_count,
            'error': error_count,
            'skipped': skipped_count,
            'total': len(df)
        }

    def merge_into_dictionary_file(self, dfs, dict_path="api/data/dictionary.json", dry_run=True, allow_new=False):
        """å°†CSVæ•°æ®åˆå¹¶åˆ°å­—å…¸JSONæ–‡ä»¶ï¼ˆé»˜è®¤åªæ›´æ–°å·²å­˜åœ¨æœ¯è¯­ï¼Œé¿å…ç›´æ¥å†™åº“ï¼‰
        - dfs: DataFrame åˆ—è¡¨
        - dict_path: è¯å…¸JSONè·¯å¾„
        - dry_run: åªé¢„è§ˆä¸è½ç›˜
        - allow_new: å…è®¸æ–°å¢æœ¯è¯­ï¼ˆé»˜è®¤Falseï¼Œé¿å…çªç ´1124ï¼‰
        è¿”å›ï¼šæ±‡æ€»ç»“æœå­—å…¸
        """
        dict_path = Path(dict_path)
        if not dict_path.exists():
            raise FileNotFoundError(f"è¯å…¸æ–‡ä»¶ä¸å­˜åœ¨: {dict_path}")

        # åŠ è½½ç°æœ‰è¯å…¸
        with open(dict_path, 'r', encoding='utf-8') as f:
            try:
                dictionary = json.load(f)
                if isinstance(dictionary, dict) and 'entries' in dictionary:
                    dictionary = dictionary['entries']
                if not isinstance(dictionary, list):
                    raise ValueError('è¯å…¸JSONæ ¼å¼åº”ä¸ºæ•°ç»„')
            except Exception as e:
                raise RuntimeError(f"è¯»å–è¯å…¸å¤±è´¥: {e}")

        # å»ºç«‹ç´¢å¼• (term, category) -> idx
        index = {}
        for i, item in enumerate(dictionary):
            term = (item.get('term') or item.get('name') or '').strip()
            category = (item.get('category') or '').strip()
            if term and category:
                index[(term, category)] = i

        def normalize_list(v):
            if v is None or (isinstance(v, float) and pd.isna(v)):
                return []
            if isinstance(v, list):
                return [str(x).strip() for x in v if str(x).strip()]
            # ä»¥åˆ†å·æ‹†åˆ†
            return [p.strip() for p in str(v).split(';') if p and str(p).strip()]

        # åˆå¹¶å¤šä¸ªæ•°æ®æº
        combined = []
        for df in dfs:
            if df is not None and len(df) > 0:
                combined.append(df)
        if not combined:
            return {
                'added': 0, 'updated': 0, 'skipped_new': 0, 'total_after': len(dictionary)
            }
        df_all = pd.concat(combined, ignore_index=True)

        added = 0
        updated = 0
        skipped_new = 0

        valid_labels = ['Symptom', 'Component', 'Tool', 'Process', 'TestCase', 'Metric', 'Material', 'Role']

        for _, row in df_all.iterrows():
            term = str(row.get('term', '')).strip()
            category = str(row.get('category', '')).strip()
            if not term or not category:
                continue
            if category not in valid_labels:
                continue

            definition = str(row.get('definition', '')).strip() if pd.notna(row.get('definition')) else ''
            aliases = normalize_list(row.get('aliases'))
            tags = normalize_list(row.get('tags'))
            sub_category = str(row.get('sub_category', '')).strip() if pd.notna(row.get('sub_category')) else ''

            key = (term, category)
            if key in index:
                # æ›´æ–°
                i = index[key]
                item = dictionary[i]
                # åˆå¹¶åˆ«å/æ ‡ç­¾
                old_aliases = normalize_list(item.get('aliases', []))
                old_tags = normalize_list(item.get('tags', []))
                new_aliases = sorted(list({*old_aliases, *aliases}))
                new_tags = sorted(list({*old_tags, *tags}))

                # ä¿ç•™ç°æœ‰è¯å…¸è®¾è®¡ï¼šä¸è¦†ç›–å·²æœ‰ definition/descriptionï¼Œä»…åœ¨ç¼ºå¤±æ—¶è¡¥å……
                if definition and not (item.get('definition') or item.get('description')):
                    item['definition'] = definition
                    item['description'] = definition
                item['aliases'] = new_aliases
                item['tags'] = new_tags
                item['category'] = category
                if sub_category:
                    item['sub_category'] = sub_category
                item['term'] = term
                item['name'] = term
                item['source'] = str(row.get('source', 'dictionary')).strip()
                item['status'] = str(row.get('status', 'active')).strip()
                item['updated_at'] = datetime.now().isoformat(timespec='seconds')
                updated += 1
            else:
                # æ–°å¢ï¼ˆé»˜è®¤ä¸å…è®¸ï¼Œé¿å…çªç ´1124ï¼‰
                if not allow_new:
                    skipped_new += 1
                    continue
                new_item = {
                    'term': term,
                    'name': term,
                    'category': category,
                    'definition': definition,
                    'description': definition,
                    'aliases': aliases,
                    'tags': tags,
                    'sub_category': sub_category if sub_category else None,
                    'source': str(row.get('source', 'dictionary')).strip(),
                    'status': str(row.get('status', 'active')).strip(),
                    'created_at': datetime.now().isoformat(timespec='seconds'),
                    'updated_at': datetime.now().isoformat(timespec='seconds')
                }
                # å»æ‰Noneå­—æ®µ
                new_item = {k: v for k, v in new_item.items() if v is not None}
                dictionary.append(new_item)
                index[key] = len(dictionary) - 1
                added += 1

        # è½ç›˜
        if not dry_run:
            backup_dir = Path('data/dictionary_backup')
            backup_dir.mkdir(parents=True, exist_ok=True)
            backup_file = backup_dir / f"dictionary_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            try:
                shutil.copyfile(dict_path, backup_file)
                print(f"ğŸ’¾ å·²å¤‡ä»½è¯å…¸åˆ°: {backup_file}")
            except Exception as e:
                print(f"âš ï¸ å¤‡ä»½å¤±è´¥: {e}")

            with open(dict_path, 'w', encoding='utf-8') as f:
                json.dump(dictionary, f, ensure_ascii=False, indent=2)
            print(f"âœ… å·²æ›´æ–°è¯å…¸æ–‡ä»¶: {dict_path}")

        return {
            'added': added,
            'updated': updated,
            'skipped_new': skipped_new,
            'total_after': len(dictionary)
        }


    def get_statistics(self):
        """è·å–å¯¼å…¥åçš„ç»Ÿè®¡ä¿¡æ¯"""
        with self.driver.session() as session:
            query = """
            MATCH (n)
            RETURN labels(n)[0] as label, count(n) as count
            ORDER BY count DESC
            """
            result = session.run(query)
            stats = []
            total = 0
            for record in result:
                label = record['label']
                count = record['count']
                stats.append({'label': label, 'count': count})
                total += count

            return {'labels': stats, 'total': total}

def main():
    print("ğŸš€ è¡¥å……è¯å…¸æ•°æ® åˆå¹¶/å¯¼å…¥ å·¥å…·")

    ap = argparse.ArgumentParser()
    ap.add_argument('--mode', choices=['update-json', 'neo4j'], default='update-json', help='é»˜è®¤æ›´æ–°è¯å…¸JSONï¼Œä¸ç›´æ¥å†™åº“')
    ap.add_argument('--dry-run', action='store_true', help='ä»…é¢„è§ˆä¸è½ç›˜ï¼ˆä»…å¯¹ update-json æ¨¡å¼æœ‰æ•ˆï¼‰')
    ap.add_argument('--allow-new', action='store_true', help='å…è®¸æ–°å¢æœ¯è¯­ï¼ˆé»˜è®¤ä»…æ›´æ–°ï¼Œä¸æ–°å¢ï¼‰')
    ap.add_argument('--rebuild', action='store_true', help='æ›´æ–°JSONåè‡ªåŠ¨é‡å»ºå›¾è°±ï¼ˆéœ€édry-runï¼‰')
    ap.add_argument('--dict', dest='dict_path', default='api/data/dictionary.json', help='è¯å…¸JSONè·¯å¾„')
    ap.add_argument('--csv', nargs='*', default=['è¡¥å……è¯å…¸æ•°æ®_æ‰¹æ¬¡1.csv', 'è¡¥å……è¯å…¸æ•°æ®_æ‰¹æ¬¡2.csv'], help='ä¸€ä¸ªæˆ–å¤šä¸ªCSVæ–‡ä»¶è·¯å¾„')
    args = ap.parse_args()

    importer = DictionaryDataImporter()

    try:
        # 1) åŠ è½½CSV
        dfs = []
        issues_all = []
        for csv_path in args.csv:
            if os.path.exists(csv_path):
                print(f"\nğŸ“‹ åŠ è½½CSV: {csv_path}")
                df = importer.load_csv_data(csv_path)
                if df is not None:
                    issues = importer.validate_data(df)
                    if issues:
                        print("âš ï¸  æ•°æ®éªŒè¯å‘ç°é—®é¢˜ï¼ˆå‰è‹¥å¹²æ¡ï¼‰:")
                        for issue in issues[:10]:
                            print(f"   - {issue}")
                        issues_all.extend(issues)
                    dfs.append(df)
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°CSV: {csv_path}")

        if args.mode == 'update-json':
            # 2) åˆå¹¶è¿›è¯å…¸JSON
            summary = importer.merge_into_dictionary_file(
                dfs, dict_path=args.dict_path, dry_run=args.dry_run, allow_new=args.allow_new
            )
            print(f"\nğŸ“Š åˆå¹¶ç»“æœ: æ›´æ–° {summary['updated']} æ¡, æ–°å¢ {summary['added']} æ¡, è·³è¿‡æ–°å¢ {summary['skipped_new']} æ¡")
            print(f"ï¿½ åˆå¹¶åè¯å…¸æ€»æ•°: {summary['total_after']} æ¡")

            # 3) å¯é€‰é‡å»ºå›¾è°±
            if args.rebuild and not args.dry_run:
                print("\nğŸ” è§¦å‘å›¾è°±é‡å»º...")
                try:
                    subprocess.run(['python', 'è‡ªåŠ¨é‡å»ºå›¾è°±æ•°æ®.py'], check=False)
                except Exception as e:
                    print(f"âš ï¸ è‡ªåŠ¨é‡å»ºå¤±è´¥: {e}")

        else:
            # 2) ç›´æ¥å†™åº“ï¼ˆMERGE å¹‚ç­‰ï¼‰
            for df in dfs:
                if df is not None:
                    result = importer.import_data(df, skip_existing=False)
                    print(f"ğŸ“Š å¯¼å…¥ç»“æœ: æˆåŠŸ{result['success']}æ¡, å¤±è´¥{result['error']}æ¡, è·³è¿‡{result['skipped']}æ¡")

            # 3) ç»Ÿè®¡
            stats = importer.get_statistics()
            print(f"\nğŸ“ˆ å¯¼å…¥åæ•°æ®åº“ç»Ÿè®¡: æ€»èŠ‚ç‚¹æ•° {stats['total']}")
            for label_stat in stats['labels']:
                print(f"  {label_stat['label']}: {label_stat['count']}ä¸ª")

    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        importer.close()

if __name__ == "__main__":
    main()
