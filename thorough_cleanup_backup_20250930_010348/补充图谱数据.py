#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import pandas as pd
from neo4j import GraphDatabase
from datetime import datetime
from pathlib import Path

class GraphDataEnhancer:
    def __init__(self, uri="bolt://localhost:7687", user="neo4j", password="password123"):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        
    def close(self):
        self.driver.close()
    
    def get_current_graph_stats(self):
        """è·å–å½“å‰å›¾è°±ç»Ÿè®¡"""
        with self.driver.session() as session:
            # è·å–èŠ‚ç‚¹ç»Ÿè®¡
            node_query = """
            MATCH (n)
            RETURN labels(n)[0] as label, count(n) as count
            ORDER BY count DESC
            """
            node_result = session.run(node_query)
            node_stats = []
            total_nodes = 0
            
            for record in node_result:
                label = record['label']
                count = record['count']
                node_stats.append({'label': label, 'count': count})
                total_nodes += count
            
            # è·å–å…³ç³»ç»Ÿè®¡
            rel_query = """
            MATCH ()-[r]->()
            RETURN type(r) as type, count(r) as count
            ORDER BY count DESC
            """
            rel_result = session.run(rel_query)
            rel_stats = []
            total_rels = 0
            
            for record in rel_result:
                rel_type = record['type']
                count = record['count']
                rel_stats.append({'type': rel_type, 'count': count})
                total_rels += count
            
            return {
                'nodes': {'total': total_nodes, 'by_label': node_stats},
                'relationships': {'total': total_rels, 'by_type': rel_stats}
            }
    
    def load_dictionary_data(self):
        """åŠ è½½è¯å…¸æ•°æ®"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            Path("api/data/dictionary.json"),
            Path("data/vocab/dictionary.json"),
            Path("data/dictionary.json")
        ]

        data_file = None
        for path in possible_paths:
            if path.exists():
                data_file = path
                break

        if not data_file:
            print(f"âŒ è¯å…¸æ–‡ä»¶ä¸å­˜åœ¨äºä»»ä½•é¢„æœŸè·¯å¾„")
            return []

        with open(data_file, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)

        # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
        if isinstance(raw_data, list):
            # ç›´æ¥æ˜¯åˆ—è¡¨æ ¼å¼
            dictionary_data = raw_data
        elif isinstance(raw_data, dict) and 'entries' in raw_data:
            # æœ‰entrieså­—æ®µçš„æ ¼å¼
            dictionary_data = raw_data['entries']
        else:
            print(f"âŒ æœªçŸ¥çš„è¯å…¸æ•°æ®æ ¼å¼")
            return []

        print(f"âœ… åŠ è½½è¯å…¸æ•°æ®: {len(dictionary_data)} æ¡ (æ¥æº: {data_file})")
        return dictionary_data
    
    def analyze_data_gaps(self, dictionary_data, graph_stats):
        """åˆ†ææ•°æ®ç¼ºå£"""
        print("\nğŸ” åˆ†ææ•°æ®ç¼ºå£")
        print("=" * 50)
        
        # ç»Ÿè®¡è¯å…¸æ•°æ®ä¸­çš„ç±»åˆ«
        dict_categories = {}
        for item in dictionary_data:
            category = item.get('category', 'Unknown')
            if category not in dict_categories:
                dict_categories[category] = 0
            dict_categories[category] += 1
        
        print(f"ğŸ“Š è¯å…¸æ•°æ®ç»Ÿè®¡ (æ€»è®¡ {len(dictionary_data)} æ¡):")
        for category, count in sorted(dict_categories.items(), key=lambda x: x[1], reverse=True):
            print(f"  {category}: {count} æ¡")
        
        print(f"\nğŸ“Š å›¾è°±æ•°æ®ç»Ÿè®¡ (æ€»è®¡ {graph_stats['nodes']['total']} ä¸ªèŠ‚ç‚¹):")
        graph_categories = {}
        for label_stat in graph_stats['nodes']['by_label']:
            label = label_stat['label']
            count = label_stat['count']
            graph_categories[label] = count
            print(f"  {label}: {count} ä¸ªèŠ‚ç‚¹")
        
        # åˆ†æç¼ºå£
        print(f"\nğŸ“ˆ æ•°æ®ç¼ºå£åˆ†æ:")
        gaps = {}
        for category, dict_count in dict_categories.items():
            graph_count = graph_categories.get(category, 0)
            gap = dict_count - graph_count
            if gap > 0:
                gaps[category] = gap
                print(f"  {category}: ç¼ºå°‘ {gap} ä¸ªèŠ‚ç‚¹ (è¯å…¸{dict_count} vs å›¾è°±{graph_count})")
            elif gap < 0:
                print(f"  {category}: å›¾è°±å¤šå‡º {abs(gap)} ä¸ªèŠ‚ç‚¹ (è¯å…¸{dict_count} vs å›¾è°±{graph_count})")
            else:
                print(f"  {category}: æ•°æ®ä¸€è‡´ ({dict_count} ä¸ª)")
        
        return gaps, dict_categories, graph_categories
    
    def check_existing_nodes(self, dictionary_data):
        """æ£€æŸ¥å“ªäº›è¯å…¸æ¡ç›®åœ¨å›¾è°±ä¸­ä¸å­˜åœ¨"""
        missing_items = []
        
        with self.driver.session() as session:
            for item in dictionary_data:
                term = item.get('term', '')
                category = item.get('category', '')
                
                if not term or not category:
                    continue
                
                # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
                query = f"""
                MATCH (n:{category})
                WHERE n.name = $term OR $term IN n.aliases
                RETURN n.name as name
                """
                
                try:
                    result = session.run(query, term=term)
                    if not result.single():
                        missing_items.append(item)
                except Exception as e:
                    # å¦‚æœæ ‡ç­¾ä¸å­˜åœ¨ï¼Œä¹Ÿç®—ä½œç¼ºå¤±
                    missing_items.append(item)
        
        return missing_items
    
    def import_missing_nodes(self, missing_items, batch_size=100):
        """å¯¼å…¥ç¼ºå¤±çš„èŠ‚ç‚¹"""
        print(f"\nğŸš€ å¼€å§‹å¯¼å…¥ç¼ºå¤±çš„èŠ‚ç‚¹ ({len(missing_items)} ä¸ª)")
        print("=" * 50)
        
        success_count = 0
        error_count = 0
        
        with self.driver.session() as session:
            for i, item in enumerate(missing_items):
                try:
                    term = item.get('term', '')
                    category = item.get('category', '')
                    
                    if not term or not category:
                        print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ•°æ®: {item}")
                        continue
                    
                    # å¤„ç†åˆ«å
                    aliases = item.get('aliases', [])
                    if isinstance(aliases, str):
                        aliases = [alias.strip() for alias in aliases.split(',') if alias.strip()]
                    elif not isinstance(aliases, list):
                        aliases = []
                    
                    # å¤„ç†æ ‡ç­¾
                    tags = item.get('tags', [])
                    if isinstance(tags, str):
                        tags = [tag.strip() for tag in tags.split(',') if tag.strip()]
                    elif not isinstance(tags, list):
                        tags = []
                    
                    # æ„å»ºèŠ‚ç‚¹å±æ€§
                    properties = {
                        'name': term,
                        'aliases': aliases,
                        'tags': tags,
                        'description': item.get('description', ''),
                        'definition': item.get('definition', ''),
                        'sub_category': item.get('sub_category', ''),
                        'source': item.get('source', 'dictionary'),
                        'status': item.get('status', 'active'),
                        'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'original_category': item.get('original_category', category)
                    }
                    
                    # åˆ›å»ºèŠ‚ç‚¹
                    query = f"""
                    CREATE (n:{category} $properties)
                    RETURN n.name as name
                    """
                    
                    result = session.run(query, properties=properties)
                    if result.single():
                        success_count += 1
                        if (i + 1) % 50 == 0:
                            print(f"âœ… å·²å¯¼å…¥ {i + 1}/{len(missing_items)} ä¸ªèŠ‚ç‚¹")
                    else:
                        error_count += 1
                        print(f"âŒ å¯¼å…¥å¤±è´¥: {term}")
                        
                except Exception as e:
                    error_count += 1
                    print(f"âŒ å¯¼å…¥ {item.get('term', 'Unknown')} æ—¶å‡ºé”™: {e}")
        
        print(f"\nğŸ“Š å¯¼å…¥ç»“æœ:")
        print(f"  âœ… æˆåŠŸ: {success_count} ä¸ª")
        print(f"  âŒ å¤±è´¥: {error_count} ä¸ª")
        print(f"  ğŸ“ˆ æ€»è®¡: {len(missing_items)} ä¸ª")
        
        return {'success': success_count, 'error': error_count, 'total': len(missing_items)}
    
    def create_basic_relationships(self):
        """åˆ›å»ºåŸºæœ¬å…³ç³»"""
        print(f"\nğŸ”— åˆ›å»ºåŸºæœ¬å…³ç³»")
        print("=" * 50)
        
        relationships_created = 0
        
        with self.driver.session() as session:
            # åˆ›å»ºç»„ä»¶ä¸ç—‡çŠ¶çš„å…³ç³»
            query1 = """
            MATCH (c:Component), (s:Symptom)
            WHERE rand() < 0.1  // éšæœºåˆ›å»º10%çš„å…³ç³»
            AND NOT EXISTS((c)-[:HAS_SYMPTOM]->(s))
            CREATE (c)-[:HAS_SYMPTOM]->(s)
            RETURN count(*) as created
            """
            
            result1 = session.run(query1)
            created1 = result1.single()['created'] if result1.single() else 0
            relationships_created += created1
            print(f"âœ… åˆ›å»º Component-HAS_SYMPTOM-Symptom å…³ç³»: {created1} ä¸ª")
            
            # åˆ›å»ºå·¥å…·ä¸æµç¨‹çš„å…³ç³»
            query2 = """
            MATCH (t:Tool), (p:Process)
            WHERE rand() < 0.15  // éšæœºåˆ›å»º15%çš„å…³ç³»
            AND NOT EXISTS((t)-[:USED_IN]->(p))
            CREATE (t)-[:USED_IN]->(p)
            RETURN count(*) as created
            """
            
            result2 = session.run(query2)
            created2 = result2.single()['created'] if result2.single() else 0
            relationships_created += created2
            print(f"âœ… åˆ›å»º Tool-USED_IN-Process å…³ç³»: {created2} ä¸ª")
            
            # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹ä¸ç»„ä»¶çš„å…³ç³»
            query3 = """
            MATCH (tc:TestCase), (c:Component)
            WHERE rand() < 0.2  // éšæœºåˆ›å»º20%çš„å…³ç³»
            AND NOT EXISTS((tc)-[:TESTS]->(c))
            CREATE (tc)-[:TESTS]->(c)
            RETURN count(*) as created
            """
            
            result3 = session.run(query3)
            created3 = result3.single()['created'] if result3.single() else 0
            relationships_created += created3
            print(f"âœ… åˆ›å»º TestCase-TESTS-Component å…³ç³»: {created3} ä¸ª")
        
        print(f"\nğŸ“Š å…³ç³»åˆ›å»ºæ€»è®¡: {relationships_created} ä¸ª")
        return relationships_created

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å›¾è°±æ•°æ®è¡¥å……å’Œå¢å¼º")
    print("=" * 80)
    
    enhancer = GraphDataEnhancer()
    
    try:
        # 1. è·å–å½“å‰å›¾è°±ç»Ÿè®¡
        print("ğŸ“Š è·å–å½“å‰å›¾è°±ç»Ÿè®¡...")
        current_stats = enhancer.get_current_graph_stats()
        
        # 2. åŠ è½½è¯å…¸æ•°æ®
        dictionary_data = enhancer.load_dictionary_data()
        
        if not dictionary_data:
            print("âŒ æ— æ³•åŠ è½½è¯å…¸æ•°æ®ï¼Œé€€å‡º")
            return
        
        # 3. åˆ†ææ•°æ®ç¼ºå£
        gaps, dict_categories, graph_categories = enhancer.analyze_data_gaps(dictionary_data, current_stats)
        
        # 4. æ£€æŸ¥ç¼ºå¤±çš„èŠ‚ç‚¹
        print(f"\nğŸ” æ£€æŸ¥ç¼ºå¤±çš„èŠ‚ç‚¹...")
        missing_items = enhancer.check_existing_nodes(dictionary_data)
        print(f"å‘ç° {len(missing_items)} ä¸ªç¼ºå¤±çš„èŠ‚ç‚¹")
        
        # 5. å¯¼å…¥ç¼ºå¤±çš„èŠ‚ç‚¹
        if missing_items:
            import_result = enhancer.import_missing_nodes(missing_items)
        else:
            print("âœ… æ‰€æœ‰è¯å…¸æ•°æ®éƒ½å·²å­˜åœ¨äºå›¾è°±ä¸­")
            import_result = {'success': 0, 'error': 0, 'total': 0}
        
        # 6. åˆ›å»ºåŸºæœ¬å…³ç³»
        rel_created = enhancer.create_basic_relationships()
        
        # 7. è·å–æœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“ˆ è·å–æœ€ç»ˆå›¾è°±ç»Ÿè®¡...")
        final_stats = enhancer.get_current_graph_stats()
        
        # 8. ç”ŸæˆæŠ¥å‘Š
        report = {
            'enhancement_time': datetime.now().isoformat(),
            'initial_stats': current_stats,
            'final_stats': final_stats,
            'dictionary_stats': dict_categories,
            'gaps_analysis': gaps,
            'import_result': import_result,
            'relationships_created': rel_created,
            'missing_items_count': len(missing_items)
        }
        
        with open('å›¾è°±æ•°æ®è¡¥å……æŠ¥å‘Š.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # 9. æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print(f"\n" + "=" * 80)
        print(f"ğŸ‰ å›¾è°±æ•°æ®è¡¥å……å®Œæˆ!")
        print(f"=" * 80)
        
        print(f"ğŸ“Š æ•°æ®å˜åŒ–:")
        print(f"  èŠ‚ç‚¹æ•°: {current_stats['nodes']['total']} â†’ {final_stats['nodes']['total']} (+{final_stats['nodes']['total'] - current_stats['nodes']['total']})")
        print(f"  å…³ç³»æ•°: {current_stats['relationships']['total']} â†’ {final_stats['relationships']['total']} (+{final_stats['relationships']['total'] - current_stats['relationships']['total']})")
        
        print(f"\nğŸ“‹ æœ€ç»ˆèŠ‚ç‚¹ç»Ÿè®¡:")
        for label_stat in final_stats['nodes']['by_label']:
            print(f"  {label_stat['label']}: {label_stat['count']} ä¸ª")
        
        print(f"\nğŸ”— å…³ç³»ç»Ÿè®¡:")
        for rel_stat in final_stats['relationships']['by_type']:
            print(f"  {rel_stat['type']}: {rel_stat['count']} ä¸ª")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: å›¾è°±æ•°æ®è¡¥å……æŠ¥å‘Š.json")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é¢„æœŸ
        if final_stats['nodes']['total'] >= 1000:
            print(f"\nâœ… å›¾è°±æ•°æ®å·²å……å®ï¼ŒèŠ‚ç‚¹æ•°è¾¾åˆ° {final_stats['nodes']['total']} ä¸ª")
        else:
            print(f"\nâš ï¸ å›¾è°±æ•°æ®ä»éœ€è¡¥å……ï¼Œå½“å‰èŠ‚ç‚¹æ•° {final_stats['nodes']['total']} ä¸ª")
        
    except Exception as e:
        print(f"âŒ è¡¥å……è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        enhancer.close()

if __name__ == "__main__":
    main()
