#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import pandas as pd
from neo4j import GraphDatabase
from pathlib import Path
from datetime import datetime

class GraphDataRebuilder:
    def __init__(self, uri="bolt://localhost:7687", user="neo4j", password="password123"):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        
    def close(self):
        self.driver.close()
    
    def backup_current_graph(self):
        """å¤‡ä»½å½“å‰å›¾è°±æ•°æ®"""
        print("ğŸ’¾ å¤‡ä»½å½“å‰å›¾è°±æ•°æ®")
        print("=" * 50)
        
        backup_data = {
            'backup_time': datetime.now().isoformat(),
            'nodes': [],
            'relationships': []
        }
        
        with self.driver.session() as session:
            # å¤‡ä»½æ‰€æœ‰èŠ‚ç‚¹
            nodes_result = session.run("""
                MATCH (n)
                WHERE n:Component OR n:Symptom OR n:Tool OR n:Process OR n:TestCase OR n:Material OR n:Role OR n:Metric
                RETURN id(n) as node_id, labels(n) as labels, properties(n) as props
            """)
            
            for record in nodes_result:
                backup_data['nodes'].append({
                    'id': record['node_id'],
                    'labels': record['labels'],
                    'properties': dict(record['props'])
                })
            
            # å¤‡ä»½æ‰€æœ‰å…³ç³»
            rels_result = session.run("""
                MATCH (a)-[r]->(b)
                WHERE (a:Component OR a:Symptom OR a:Tool OR a:Process OR a:TestCase OR a:Material OR a:Role OR a:Metric)
                AND (b:Component OR b:Symptom OR b:Tool OR b:Process OR b:TestCase OR b:Material OR b:Role OR b:Metric)
                RETURN id(a) as source_id, type(r) as rel_type, id(b) as target_id, properties(r) as props
            """)
            
            for record in rels_result:
                backup_data['relationships'].append({
                    'source_id': record['source_id'],
                    'type': record['rel_type'],
                    'target_id': record['target_id'],
                    'properties': dict(record['props'])
                })
        
        # ä¿å­˜å¤‡ä»½æ–‡ä»¶
        backup_file = f"graph_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å¤‡ä»½å®Œæˆ: {backup_file}")
        print(f"  èŠ‚ç‚¹æ•°: {len(backup_data['nodes'])}")
        print(f"  å…³ç³»æ•°: {len(backup_data['relationships'])}")
        
        return backup_file
    
    def analyze_current_data(self):
        """åˆ†æå½“å‰æ•°æ®ï¼Œè¯†åˆ«è¯å…¸æ•°æ®å’ŒåŸæœ‰æ•°æ®"""
        print("\nğŸ” åˆ†æå½“å‰å›¾è°±æ•°æ®")
        print("=" * 50)
        
        # åŠ è½½è¯å…¸æ•°æ®ä½œä¸ºå‚è€ƒ
        dict_file = Path("api/data/dictionary.json")
        if not dict_file.exists():
            print("âŒ è¯å…¸æ–‡ä»¶ä¸å­˜åœ¨")
            return None, None
        
        with open(dict_file, 'r', encoding='utf-8') as f:
            dictionary_data = json.load(f)
        
        dict_terms = {item.get('term', '') for item in dictionary_data}
        print(f"ğŸ“š è¯å…¸æ•°æ®: {len(dict_terms)} ä¸ªæœ¯è¯­")
        
        # åˆ†æå›¾è°±ä¸­çš„èŠ‚ç‚¹
        with self.driver.session() as session:
            result = session.run("""
                MATCH (n)
                WHERE n:Component OR n:Symptom OR n:Tool OR n:Process OR n:TestCase OR n:Material OR n:Role OR n:Metric
                RETURN id(n) as node_id, labels(n)[0] as label, 
                       coalesce(n.name, n.term, '') as name,
                       properties(n) as props
            """)
            
            graph_nodes = list(result)
            
        # åˆ†ç±»èŠ‚ç‚¹
        dict_nodes = []  # æ¥è‡ªè¯å…¸çš„èŠ‚ç‚¹
        legacy_nodes = []  # åŸæœ‰çš„èŠ‚ç‚¹
        
        for node in graph_nodes:
            name = node['name']
            props = dict(node['props'])
            
            # åˆ¤æ–­æ˜¯å¦æ¥è‡ªè¯å…¸æ•°æ®
            if name in dict_terms:
                dict_nodes.append(node)
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰è¯å…¸æ•°æ®çš„ç‰¹å¾ï¼ˆå¦‚termå­—æ®µï¼‰
                if 'term' in props and props['term'] in dict_terms:
                    dict_nodes.append(node)
                else:
                    legacy_nodes.append(node)
        
        print(f"\nğŸ“Š æ•°æ®åˆ†æç»“æœ:")
        print(f"  æ€»èŠ‚ç‚¹æ•°: {len(graph_nodes)}")
        print(f"  è¯å…¸èŠ‚ç‚¹: {len(dict_nodes)} ä¸ª")
        print(f"  åŸæœ‰èŠ‚ç‚¹: {len(legacy_nodes)} ä¸ª")
        
        if legacy_nodes:
            print(f"\nğŸ” åŸæœ‰èŠ‚ç‚¹ç¤ºä¾‹ (å‰10ä¸ª):")
            for node in legacy_nodes[:10]:
                print(f"  - {node['name']} ({node['label']}) [ID: {node['node_id']}]")
        
        return dict_nodes, legacy_nodes
    
    def clear_all_graph_data(self):
        """æ¸…ç©ºæ‰€æœ‰å›¾è°±æ•°æ®"""
        print("\nğŸ§¹ æ¸…ç©ºæ‰€æœ‰å›¾è°±æ•°æ®")
        print("=" * 50)
        
        with self.driver.session() as session:
            # åˆ é™¤æ‰€æœ‰å…³ç³»
            rel_result = session.run("""
                MATCH ()-[r]->()
                DELETE r
                RETURN count(r) as deleted_rels
            """)
            deleted_rels = rel_result.single()['deleted_rels']
            
            # åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹
            node_result = session.run("""
                MATCH (n)
                WHERE n:Component OR n:Symptom OR n:Tool OR n:Process OR n:TestCase OR n:Material OR n:Role OR n:Metric
                DELETE n
                RETURN count(n) as deleted_nodes
            """)
            deleted_nodes = node_result.single()['deleted_nodes']
            
            print(f"âœ… å·²åˆ é™¤ {deleted_rels} ä¸ªå…³ç³»")
            print(f"âœ… å·²åˆ é™¤ {deleted_nodes} ä¸ªèŠ‚ç‚¹")
            
            return deleted_nodes, deleted_rels
    
    def import_clean_dictionary_data(self):
        """å¯¼å…¥çº¯å‡€çš„è¯å…¸æ•°æ®"""
        print("\nğŸ“¥ å¯¼å…¥çº¯å‡€çš„è¯å…¸æ•°æ®")
        print("=" * 50)
        
        # åŠ è½½è¯å…¸æ•°æ®
        dict_file = Path("api/data/dictionary.json")
        with open(dict_file, 'r', encoding='utf-8') as f:
            dictionary_data = json.load(f)
        
        print(f"ğŸ“š å‡†å¤‡å¯¼å…¥ {len(dictionary_data)} æ¡è¯å…¸æ•°æ®")
        
        imported_count = 0
        category_counts = {}
        
        with self.driver.session() as session:
            for item in dictionary_data:
                try:
                    term = item.get('term', '')
                    category = item.get('category', 'Unknown')
                    definition = item.get('definition', '')
                    aliases = item.get('aliases', [])
                    tags = item.get('tags', [])
                    
                    # ç¡®ä¿categoryæ˜¯æœ‰æ•ˆçš„æ ‡ç­¾
                    valid_categories = ['Component', 'Symptom', 'Tool', 'Process', 'TestCase', 'Material', 'Role', 'Metric']
                    if category not in valid_categories:
                        category = 'Component'  # é»˜è®¤åˆ†ç±»
                    
                    # åˆ›å»ºèŠ‚ç‚¹
                    session.run(f"""
                        CREATE (n:{category})
                        SET n.name = $term,
                            n.term = $term,
                            n.definition = $definition,
                            n.description = $definition,
                            n.aliases = $aliases,
                            n.tags = $tags,
                            n.category = $category,
                            n.source = 'dictionary',
                            n.created_at = datetime(),
                            n.updated_at = datetime()
                    """, term=term, definition=definition, aliases=aliases, tags=tags, category=category)
                    
                    imported_count += 1
                    category_counts[category] = category_counts.get(category, 0) + 1
                    
                    if imported_count % 100 == 0:
                        print(f"  å·²å¯¼å…¥ {imported_count}/{len(dictionary_data)} ä¸ªèŠ‚ç‚¹")
                        
                except Exception as e:
                    print(f"âŒ å¯¼å…¥èŠ‚ç‚¹å¤±è´¥: {item.get('term', 'Unknown')} - {e}")
        
        print(f"\nâœ… è¯å…¸æ•°æ®å¯¼å…¥å®Œæˆ:")
        print(f"  æ€»è®¡: {imported_count} ä¸ªèŠ‚ç‚¹")
        for category, count in sorted(category_counts.items()):
            print(f"  {category}: {count} ä¸ª")
        
        return imported_count, category_counts
    
    def create_basic_relationships(self):
        """åˆ›å»ºåŸºæœ¬å…³ç³»"""
        print("\nğŸ”— åˆ›å»ºåŸºæœ¬å…³ç³»")
        print("=" * 50)
        
        relationships_created = 0
        
        with self.driver.session() as session:
            # 1. åŸºäºæ ‡ç­¾åˆ›å»ºå…³ç³»
            print("  åˆ›å»ºåŸºäºæ ‡ç­¾çš„å…³ç³»...")
            
            # Component -> Symptom (åŸºäºå…±åŒæ ‡ç­¾)
            result1 = session.run("""
                MATCH (c:Component), (s:Symptom)
                WHERE any(tag IN c.tags WHERE tag IN s.tags)
                AND size(c.tags) > 0 AND size(s.tags) > 0
                WITH c, s, [tag IN c.tags WHERE tag IN s.tags] as common_tags
                WHERE size(common_tags) >= 1
                MERGE (c)-[r:HAS_SYMPTOM]->(s)
                SET r.confidence = toFloat(size(common_tags)) / 10.0,
                    r.source = 'tag_similarity',
                    r.common_tags = common_tags,
                    r.created_at = datetime()
                RETURN count(r) as created
            """)
            created1 = result1.single()['created'] if result1.single() else 0
            relationships_created += created1
            print(f"    Component->Symptom: {created1} ä¸ª")
            
            # TestCase -> Tool (æµ‹è¯•ç”¨ä¾‹ä½¿ç”¨å·¥å…·)
            result2 = session.run("""
                MATCH (tc:TestCase), (t:Tool)
                WHERE any(tag IN tc.tags WHERE tag IN t.tags)
                AND size(tc.tags) > 0 AND size(t.tags) > 0
                WITH tc, t, [tag IN tc.tags WHERE tag IN t.tags] as common_tags
                WHERE size(common_tags) >= 1
                MERGE (tc)-[r:USES_TOOL]->(t)
                SET r.confidence = toFloat(size(common_tags)) / 10.0,
                    r.source = 'tag_similarity',
                    r.common_tags = common_tags,
                    r.created_at = datetime()
                RETURN count(r) as created
            """)
            created2 = result2.single()['created'] if result2.single() else 0
            relationships_created += created2
            print(f"    TestCase->Tool: {created2} ä¸ª")
            
            # TestCase -> Metric (æµ‹è¯•ç”¨ä¾‹æµ‹é‡æŒ‡æ ‡)
            result3 = session.run("""
                MATCH (tc:TestCase), (m:Metric)
                WHERE any(tag IN tc.tags WHERE tag IN m.tags)
                AND size(tc.tags) > 0 AND size(m.tags) > 0
                WITH tc, m, [tag IN tc.tags WHERE tag IN m.tags] as common_tags
                WHERE size(common_tags) >= 1
                MERGE (tc)-[r:MEASURES]->(m)
                SET r.confidence = toFloat(size(common_tags)) / 10.0,
                    r.source = 'tag_similarity',
                    r.common_tags = common_tags,
                    r.created_at = datetime()
                RETURN count(r) as created
            """)
            created3 = result3.single()['created'] if result3.single() else 0
            relationships_created += created3
            print(f"    TestCase->Metric: {created3} ä¸ª")
            
            # Process -> Material (æµç¨‹ä½¿ç”¨ææ–™)
            result4 = session.run("""
                MATCH (p:Process), (m:Material)
                WHERE any(tag IN p.tags WHERE tag IN m.tags)
                AND size(p.tags) > 0 AND size(m.tags) > 0
                WITH p, m, [tag IN p.tags WHERE tag IN m.tags] as common_tags
                WHERE size(common_tags) >= 1
                MERGE (p)-[r:USES_MATERIAL]->(m)
                SET r.confidence = toFloat(size(common_tags)) / 10.0,
                    r.source = 'tag_similarity',
                    r.common_tags = common_tags,
                    r.created_at = datetime()
                RETURN count(r) as created
            """)
            created4 = result4.single()['created'] if result4.single() else 0
            relationships_created += created4
            print(f"    Process->Material: {created4} ä¸ª")
            
            # Process -> Tool (æµç¨‹ä½¿ç”¨å·¥å…·)
            result5 = session.run("""
                MATCH (p:Process), (t:Tool)
                WHERE any(tag IN p.tags WHERE tag IN t.tags)
                AND size(p.tags) > 0 AND size(t.tags) > 0
                WITH p, t, [tag IN p.tags WHERE tag IN t.tags] as common_tags
                WHERE size(common_tags) >= 1
                MERGE (p)-[r:USES_TOOL]->(t)
                SET r.confidence = toFloat(size(common_tags)) / 10.0,
                    r.source = 'tag_similarity',
                    r.common_tags = common_tags,
                    r.created_at = datetime()
                RETURN count(r) as created
            """)
            created5 = result5.single()['created'] if result5.single() else 0
            relationships_created += created5
            print(f"    Process->Tool: {created5} ä¸ª")
        
        print(f"\nâœ… å…³ç³»åˆ›å»ºå®Œæˆï¼Œæ€»è®¡: {relationships_created} ä¸ª")
        return relationships_created
    
    def verify_rebuilt_data(self):
        """éªŒè¯é‡å»ºåçš„æ•°æ®"""
        print("\nğŸ” éªŒè¯é‡å»ºåçš„æ•°æ®")
        print("=" * 50)
        
        with self.driver.session() as session:
            # èŠ‚ç‚¹ç»Ÿè®¡
            node_stats = session.run("""
                MATCH (n)
                WHERE n:Component OR n:Symptom OR n:Tool OR n:Process OR n:TestCase OR n:Material OR n:Role OR n:Metric
                RETURN labels(n)[0] as label, count(n) as count
                ORDER BY count DESC
            """)
            
            total_nodes = 0
            print("ğŸ“Š èŠ‚ç‚¹åˆ†å¸ƒ:")
            for record in node_stats:
                count = record['count']
                total_nodes += count
                print(f"  {record['label']}: {count} ä¸ª")
            
            # å…³ç³»ç»Ÿè®¡
            rel_stats = session.run("""
                MATCH ()-[r]->()
                RETURN type(r) as rel_type, count(r) as count
                ORDER BY count DESC
            """)
            
            total_rels = 0
            print(f"\nğŸ”— å…³ç³»åˆ†å¸ƒ:")
            for record in rel_stats:
                count = record['count']
                total_rels += count
                print(f"  {record['rel_type']}: {count} ä¸ª")
            
            print(f"\nğŸ“ˆ æ€»è®¡:")
            print(f"  èŠ‚ç‚¹æ•°: {total_nodes}")
            print(f"  å…³ç³»æ•°: {total_rels}")
            
            # éªŒè¯æ˜¯å¦ç¬¦åˆé¢„æœŸ
            if total_nodes == 1124:
                print(f"âœ… èŠ‚ç‚¹æ•°æ­£ç¡®ï¼æ­£å¥½æ˜¯1124æ¡è¯å…¸æ•°æ®")
            else:
                print(f"âš ï¸ èŠ‚ç‚¹æ•°å¼‚å¸¸ï¼Œé¢„æœŸ1124ï¼Œå®é™…{total_nodes}")
            
            if total_rels > 1000:
                print(f"âœ… å…³ç³»æ•°å……è¶³ï¼")
            else:
                print(f"âš ï¸ å…³ç³»æ•°è¾ƒå°‘ï¼Œå¯èƒ½éœ€è¦æ›´å¤šå…³ç³»è§„åˆ™")
            
            return total_nodes, total_rels

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ¸…ç†å¹¶é‡å»ºå›¾è°±æ•°æ®")
    print("=" * 80)
    
    rebuilder = GraphDataRebuilder()
    
    try:
        # 1. å¤‡ä»½å½“å‰æ•°æ®
        backup_file = rebuilder.backup_current_graph()
        
        # 2. åˆ†æå½“å‰æ•°æ®
        dict_nodes, legacy_nodes = rebuilder.analyze_current_data()
        
        if legacy_nodes:
            print(f"\nâš ï¸ å‘ç° {len(legacy_nodes)} ä¸ªåŸæœ‰èŠ‚ç‚¹éœ€è¦æ¸…ç†")
            response = input("æ˜¯å¦ç»§ç»­æ¸…ç†å¹¶é‡å»ºï¼Ÿ(y/n): ").strip().lower()
            
            if response != 'y':
                print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return
        
        # 3. æ¸…ç©ºæ‰€æœ‰æ•°æ®
        deleted_nodes, deleted_rels = rebuilder.clear_all_graph_data()
        
        # 4. å¯¼å…¥çº¯å‡€çš„è¯å…¸æ•°æ®
        imported_count, category_counts = rebuilder.import_clean_dictionary_data()
        
        # 5. åˆ›å»ºåŸºæœ¬å…³ç³»
        relationships_created = rebuilder.create_basic_relationships()
        
        # 6. éªŒè¯é‡å»ºç»“æœ
        final_nodes, final_rels = rebuilder.verify_rebuilt_data()
        
        # 7. æ€»ç»“
        print(f"\n" + "=" * 80)
        print(f"ğŸ‰ å›¾è°±æ•°æ®é‡å»ºå®Œæˆï¼")
        print(f"=" * 80)
        
        print(f"ğŸ“Š é‡å»ºç»Ÿè®¡:")
        print(f"  åˆ é™¤åŸæœ‰èŠ‚ç‚¹: {deleted_nodes} ä¸ª")
        print(f"  åˆ é™¤åŸæœ‰å…³ç³»: {deleted_rels} ä¸ª")
        print(f"  å¯¼å…¥è¯å…¸èŠ‚ç‚¹: {imported_count} ä¸ª")
        print(f"  åˆ›å»ºæ–°å…³ç³»: {relationships_created} ä¸ª")
        
        print(f"\nğŸ“ˆ æœ€ç»ˆç»“æœ:")
        print(f"  æ€»èŠ‚ç‚¹æ•°: {final_nodes} (é¢„æœŸ: 1124)")
        print(f"  æ€»å…³ç³»æ•°: {final_rels}")
        
        print(f"\nğŸ’¾ å¤‡ä»½æ–‡ä»¶: {backup_file}")
        print(f"ğŸŒ ç°åœ¨å¯ä»¥è®¿é—®å‰ç«¯æŸ¥çœ‹é‡å»ºåçš„å›¾è°±æ•°æ®")
        
    except Exception as e:
        print(f"âŒ é‡å»ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        rebuilder.close()

if __name__ == "__main__":
    main()
