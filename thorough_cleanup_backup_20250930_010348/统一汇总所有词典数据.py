#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ±‡æ€»æ‰€æœ‰è¯å…¸æ•°æ® - ç¡®ä¿æ•°æ®å®Œæ•´æ€§å’Œè·¯å¾„å”¯ä¸€æ€§
"""

import pandas as pd
import json
import csv
from pathlib import Path
from datetime import datetime
import shutil

def find_all_dictionary_sources():
    """æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„è¯å…¸æ•°æ®æº"""
    print("ğŸ” æŸ¥æ‰¾æ‰€æœ‰è¯å…¸æ•°æ®æº...")
    
    sources = []
    
    # 1. ç»Ÿä¸€è¯å…¸ç›®å½•
    unified_dir = Path("data/unified_dictionary")
    if unified_dir.exists():
        for file in ["components.csv", "symptoms.csv", "causes.csv", "countermeasures.csv"]:
            file_path = unified_dir / file
            if file_path.exists():
                sources.append({
                    "path": str(file_path),
                    "type": "unified_dictionary",
                    "category": file.replace(".csv", ""),
                    "size": file_path.stat().st_size
                })
    
    # 2. APIæ•°æ®ç›®å½•
    api_data_dir = Path("api/data")
    if api_data_dir.exists():
        for file in ["dictionary.json", "dictionary.csv"]:
            file_path = api_data_dir / file
            if file_path.exists():
                sources.append({
                    "path": str(file_path),
                    "type": "api_data",
                    "category": "all",
                    "size": file_path.stat().st_size
                })
    
    # 3. ç¡¬ä»¶æ¨¡å—CSVæ–‡ä»¶
    for file in Path(".").glob("ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_*.csv"):
        sources.append({
            "path": str(file),
            "type": "hardware_module",
            "category": file.stem.replace("ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_", ""),
            "size": file.stat().st_size
        })
    
    # 4. è¡¥å……æ•°æ®æ–‡ä»¶
    for file in Path(".").glob("è¡¥å……è¯å…¸æ•°æ®_*.csv"):
        sources.append({
            "path": str(file),
            "type": "supplement",
            "category": file.stem.replace("è¡¥å……è¯å…¸æ•°æ®_", ""),
            "size": file.stat().st_size
        })
    
    # 5. å…¶ä»–å¯èƒ½çš„è¯å…¸æ–‡ä»¶
    other_files = [
        "data/vocab/dictionary.json",
        "data/dicts/quality_terms.csv",
        "new_dictionary_data.csv"
    ]
    
    for file_path in other_files:
        path = Path(file_path)
        if path.exists():
            sources.append({
                "path": str(path),
                "type": "other",
                "category": path.stem,
                "size": path.stat().st_size
            })
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(sources)} ä¸ªæ•°æ®æº:")
    for source in sources:
        print(f"  {source['type']}: {source['path']} ({source['size']:,} bytes)")
    
    return sources

def load_and_merge_all_data(sources):
    """åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰æ•°æ®"""
    print("ğŸ“– åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰æ•°æ®...")
    
    all_data = []
    source_stats = {}
    
    for source in sources:
        try:
            path = Path(source["path"])
            
            if path.suffix == ".csv":
                df = pd.read_csv(path, encoding='utf-8')
                data = df.to_dict('records')
            elif path.suffix == ".json":
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if not isinstance(data, list):
                        continue  # è·³è¿‡éåˆ—è¡¨æ ¼å¼çš„JSON
            else:
                continue
            
            # æ ‡è®°æ•°æ®æº
            for item in data:
                if isinstance(item, dict):
                    item['_source'] = source["type"]
                    item['_source_file'] = source["path"]
            
            all_data.extend(data)
            source_stats[source["path"]] = len(data)
            print(f"  âœ… {source['path']}: {len(data)} æ¡")
            
        except Exception as e:
            print(f"  âŒ {source['path']}: åŠ è½½å¤±è´¥ - {e}")
            source_stats[source["path"]] = 0
    
    print(f"ğŸ“Š æ€»è®¡åŠ è½½: {len(all_data)} æ¡åŸå§‹æ•°æ®")
    return all_data, source_stats

def standardize_data_format(all_data):
    """æ ‡å‡†åŒ–æ•°æ®æ ¼å¼"""
    print("ğŸ”„ æ ‡å‡†åŒ–æ•°æ®æ ¼å¼...")
    
    standardized_data = []
    field_mappings = {
        # è¯æ¡åç§°
        'term': ['term', 'name', 'æœ¯è¯­', 'è¯æ¡'],
        # åˆ«å
        'aliases': ['aliases', 'alias', 'åˆ«å', 'åŒä¹‰è¯'],
        # ç±»åˆ«
        'category': ['category', 'label', 'type', 'ç±»åˆ«', 'æ ‡ç­¾'],
        # æ ‡ç­¾
        'tags': ['tags', 'tag', 'æ ‡ç­¾', 'å¤šæ ‡ç­¾'],
        # æè¿°
        'description': ['description', 'definition', 'desc', 'å®šä¹‰', 'æè¿°', 'å¤‡æ³¨'],
        # å­ç±»åˆ«
        'sub_category': ['sub_category', 'subcategory', 'å­ç±»åˆ«'],
        # æ¥æº
        'source': ['source', 'source_file', 'æ¥æº'],
        # çŠ¶æ€
        'status': ['status', 'çŠ¶æ€']
    }
    
    for item in all_data:
        if not isinstance(item, dict):
            continue
        
        standardized_item = {}
        
        # æ˜ å°„å­—æ®µ
        for std_field, possible_fields in field_mappings.items():
            value = None
            for field in possible_fields:
                if field in item and item[field] is not None:
                    value = item[field]
                    break
            
            if value is not None:
                # å¤„ç†ç‰¹æ®Šå­—æ®µ
                if std_field in ['aliases', 'tags']:
                    if isinstance(value, str):
                        # åˆ†å‰²å­—ç¬¦ä¸²
                        if 'ï¼›' in value:
                            value = [v.strip() for v in value.split('ï¼›') if v.strip()]
                        elif ';' in value:
                            value = [v.strip() for v in value.split(';') if v.strip()]
                        elif ',' in value:
                            value = [v.strip() for v in value.split(',') if v.strip()]
                        else:
                            value = [value.strip()] if value.strip() else []
                    elif not isinstance(value, list):
                        value = []
                elif std_field == 'description':
                    value = str(value).strip()
                else:
                    value = str(value).strip()
            else:
                # è®¾ç½®é»˜è®¤å€¼
                if std_field in ['aliases', 'tags']:
                    value = []
                else:
                    value = ''
            
            standardized_item[std_field] = value
        
        # æ·»åŠ å…ƒæ•°æ®
        standardized_item['_source'] = item.get('_source', 'unknown')
        standardized_item['_source_file'] = item.get('_source_file', 'unknown')
        standardized_item['_processed_at'] = datetime.now().isoformat()
        
        # åªä¿ç•™æœ‰æ•ˆçš„è¯æ¡ï¼ˆå¿…é¡»æœ‰termï¼‰
        if standardized_item['term']:
            standardized_data.append(standardized_item)
    
    print(f"âœ… æ ‡å‡†åŒ–å®Œæˆ: {len(standardized_data)} æ¡æœ‰æ•ˆæ•°æ®")
    return standardized_data

def deduplicate_data(standardized_data):
    """å»é‡æ•°æ®"""
    print("ğŸ”„ å»é‡æ•°æ®...")
    
    # æŒ‰termå»é‡ï¼Œä¿ç•™æœ€å®Œæ•´çš„è®°å½•
    term_groups = {}
    
    for item in standardized_data:
        term = item['term'].lower().strip()
        
        if term not in term_groups:
            term_groups[term] = []
        
        term_groups[term].append(item)
    
    deduplicated_data = []
    duplicate_count = 0
    
    for term, items in term_groups.items():
        if len(items) == 1:
            deduplicated_data.append(items[0])
        else:
            # é€‰æ‹©æœ€å®Œæ•´çš„è®°å½•
            best_item = max(items, key=lambda x: (
                len(x['description']),
                len(x['aliases']),
                len(x['tags']),
                1 if x['category'] else 0
            ))
            
            # åˆå¹¶åˆ«åå’Œæ ‡ç­¾
            all_aliases = set()
            all_tags = set()
            
            for item in items:
                all_aliases.update(item['aliases'])
                all_tags.update(item['tags'])
            
            best_item['aliases'] = list(all_aliases)
            best_item['tags'] = list(all_tags)
            
            deduplicated_data.append(best_item)
            duplicate_count += len(items) - 1
    
    print(f"âœ… å»é‡å®Œæˆ: ç§»é™¤ {duplicate_count} æ¡é‡å¤æ•°æ®ï¼Œä¿ç•™ {len(deduplicated_data)} æ¡")
    return deduplicated_data

def save_unified_data(deduplicated_data):
    """ä¿å­˜ç»Ÿä¸€æ•°æ®"""
    print("ğŸ’¾ ä¿å­˜ç»Ÿä¸€æ•°æ®...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("unified_final_dictionary")
    output_dir.mkdir(exist_ok=True)
    
    # å¤‡ä»½ç°æœ‰æ•°æ®
    backup_dir = Path("data/dictionary_backup") / f"before_final_unification_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    backup_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆAPIä½¿ç”¨ï¼‰
    json_file = output_dir / "dictionary.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(deduplicated_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
    
    # ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆå¤‡ç”¨ï¼‰
    csv_file = output_dir / "dictionary.csv"
    df = pd.DataFrame(deduplicated_data)
    df.to_csv(csv_file, index=False, encoding='utf-8')
    
    print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    stats = {
        'total_terms': len(deduplicated_data),
        'categories': {},
        'sources': {},
        'unification_date': datetime.now().isoformat()
    }
    
    for item in deduplicated_data:
        category = item.get('category', 'unknown')
        source = item.get('_source', 'unknown')
        
        stats['categories'][category] = stats['categories'].get(category, 0) + 1
        stats['sources'][source] = stats['sources'].get(source, 0) + 1
    
    stats_file = output_dir / "statistics.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç»Ÿè®¡æ–‡ä»¶å·²ä¿å­˜: {stats_file}")
    
    return output_dir, stats

def update_api_data_source(output_dir):
    """æ›´æ–°APIæ•°æ®æº"""
    print("ğŸ”„ æ›´æ–°APIæ•°æ®æº...")
    
    # ç¡®ä¿APIæ•°æ®ç›®å½•å­˜åœ¨
    api_data_dir = Path("api/data")
    api_data_dir.mkdir(parents=True, exist_ok=True)
    
    # å¤åˆ¶ç»Ÿä¸€æ•°æ®åˆ°APIç›®å½•
    source_json = output_dir / "dictionary.json"
    target_json = api_data_dir / "dictionary.json"
    
    if source_json.exists():
        shutil.copy2(source_json, target_json)
        print(f"âœ… å·²æ›´æ–°APIæ•°æ®æº: {target_json}")
    
    # å¤åˆ¶CSVæ–‡ä»¶
    source_csv = output_dir / "dictionary.csv"
    target_csv = api_data_dir / "dictionary.csv"
    
    if source_csv.exists():
        shutil.copy2(source_csv, target_csv)
        print(f"âœ… å·²æ›´æ–°API CSVæ–‡ä»¶: {target_csv}")
    
    # å¤åˆ¶ç»Ÿè®¡æ–‡ä»¶
    source_stats = output_dir / "statistics.json"
    target_stats = api_data_dir / "dictionary_stats.json"
    
    if source_stats.exists():
        shutil.copy2(source_stats, target_stats)
        print(f"âœ… å·²æ›´æ–°APIç»Ÿè®¡æ–‡ä»¶: {target_stats}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç»Ÿä¸€æ±‡æ€»æ‰€æœ‰è¯å…¸æ•°æ®")
    print("=" * 60)
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æº
    sources = find_all_dictionary_sources()
    
    if not sources:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•è¯å…¸æ•°æ®æº")
        return
    
    # 2. åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰æ•°æ®
    all_data, source_stats = load_and_merge_all_data(sources)
    
    if not all_data:
        print("âŒ æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®")
        return
    
    # 3. æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
    standardized_data = standardize_data_format(all_data)
    
    # 4. å»é‡æ•°æ®
    deduplicated_data = deduplicate_data(standardized_data)
    
    # 5. ä¿å­˜ç»Ÿä¸€æ•°æ®
    output_dir, stats = save_unified_data(deduplicated_data)
    
    # 6. æ›´æ–°APIæ•°æ®æº
    update_api_data_source(output_dir)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ è¯å…¸æ•°æ®ç»Ÿä¸€æ±‡æ€»å®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆæ•°æ®: {stats['total_terms']} æ¡")
    print(f"ğŸ“Š æ•°æ®åˆ†å¸ƒ:")
    for category, count in sorted(stats['categories'].items(), key=lambda x: x[1], reverse=True):
        print(f"  {category}: {count} æ¡")
    
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  ç»Ÿä¸€æ•°æ®: {output_dir}/dictionary.json")
    print(f"  APIæ•°æ®: api/data/dictionary.json")
    print(f"  ç»Ÿè®¡æŠ¥å‘Š: {output_dir}/statistics.json")
    
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"1. é‡å¯APIæœåŠ¡åŠ è½½æ–°æ•°æ®")
    print(f"2. éªŒè¯å‰ç«¯æ˜¾ç¤º {stats['total_terms']} æ¡æ•°æ®")
    print(f"3. æµ‹è¯•æœç´¢å’Œç­›é€‰åŠŸèƒ½")
    print(f"4. ç¡®è®¤è·¯å¾„å”¯ä¸€æ€§")

if __name__ == "__main__":
    main()
