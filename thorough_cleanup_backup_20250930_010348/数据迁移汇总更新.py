#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è¿ç§»æ±‡æ€»æ›´æ–° - ç»Ÿä¸€è¯å…¸æ•°æ®æºå¹¶åŒæ­¥æ›´æ–°å›¾è°±
"""

import pandas as pd
import json
import csv
import requests
from pathlib import Path
from datetime import datetime
import shutil

def backup_current_api_data():
    """å¤‡ä»½å½“å‰APIä½¿ç”¨çš„æ•°æ®"""
    print("ğŸ’¾ å¤‡ä»½å½“å‰APIæ•°æ®...")
    
    backup_dir = Path("data/api_backup") / f"before_migration_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    backup_dir.mkdir(parents=True, exist_ok=True)
    
    # å¤‡ä»½APIç›®å½•ä¸‹çš„æ•°æ®æ–‡ä»¶
    api_data_dir = Path("api/data")
    if api_data_dir.exists():
        shutil.copytree(api_data_dir, backup_dir / "api_data")
        print(f"âœ… APIæ•°æ®å·²å¤‡ä»½åˆ°: {backup_dir}")
    
    return backup_dir

def load_unified_dictionary_data():
    """åŠ è½½ç»Ÿä¸€è¯å…¸æ•°æ®"""
    print("ğŸ“– åŠ è½½ç»Ÿä¸€è¯å…¸æ•°æ®...")
    
    unified_dir = Path("data/unified_dictionary")
    all_data = []
    
    # è¯»å–å„ä¸ªåˆ†ç±»çš„CSVæ–‡ä»¶
    categories = ["components", "symptoms", "causes", "countermeasures"]
    
    for category in categories:
        file_path = unified_dir / f"{category}.csv"
        if file_path.exists():
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                # æ·»åŠ åˆ†ç±»ä¿¡æ¯
                df['source_category'] = category
                all_data.append(df)
                print(f"  {category}: {len(df)} æ¡")
            except Exception as e:
                print(f"âŒ è¯»å– {category}.csv å¤±è´¥: {e}")
    
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        print(f"ğŸ“Š æ€»è®¡: {len(combined_df)} æ¡ç»Ÿä¸€è¯å…¸æ•°æ®")
        return combined_df
    else:
        print("âŒ æœªèƒ½åŠ è½½ç»Ÿä¸€è¯å…¸æ•°æ®")
        return None

def convert_to_api_format(df):
    """è½¬æ¢ä¸ºAPIæ ¼å¼"""
    print("ğŸ”„ è½¬æ¢æ•°æ®æ ¼å¼...")
    
    api_data = []
    
    for idx, row in df.iterrows():
        # ç”Ÿæˆå”¯ä¸€ID
        term_id = f"TERM_{idx+1:04d}"
        
        # å¤„ç†åˆ«å
        aliases = []
        if pd.notna(row.get('aliases', '')):
            aliases_str = str(row['aliases'])
            if aliases_str and aliases_str != 'nan':
                aliases = [alias.strip() for alias in aliases_str.split(',') if alias.strip()]
        
        # å¤„ç†æ ‡ç­¾
        tags = []
        if pd.notna(row.get('tags', '')):
            tags_str = str(row['tags'])
            if tags_str and tags_str != 'nan':
                tags = [tag.strip() for tag in tags_str.split('ï¼›') if tag.strip()]
        
        # æ˜ å°„ç±»åˆ«
        category_mapping = {
            'Component': 'ç»„ä»¶',
            'Symptom': 'ç—‡çŠ¶', 
            'Tool': 'å·¥å…·',
            'Process': 'æµç¨‹',
            'TestCase': 'æµ‹è¯•ç”¨ä¾‹',
            'Metric': 'æ€§èƒ½æŒ‡æ ‡',
            'Material': 'ææ–™',
            'Role': 'è§’è‰²'
        }
        
        api_category = category_mapping.get(row.get('category', ''), 'å…¶ä»–')
        
        api_item = {
            'id': term_id,
            'name': row.get('term', ''),
            'type': api_category,
            'category': row.get('source_category', 'other'),
            'aliases': aliases,
            'tags': tags,
            'description': row.get('description', ''),
            'source_file': 'unified_dictionary_migration',
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        api_data.append(api_item)
    
    print(f"âœ… è½¬æ¢å®Œæˆ: {len(api_data)} æ¡APIæ ¼å¼æ•°æ®")
    return api_data

def save_to_api_data_source(api_data):
    """ä¿å­˜åˆ°APIæ•°æ®æº"""
    print("ğŸ’¾ ä¿å­˜åˆ°APIæ•°æ®æº...")
    
    # ç¡®ä¿APIæ•°æ®ç›®å½•å­˜åœ¨
    api_data_dir = Path("api/data")
    api_data_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆAPIå¯èƒ½ä½¿ç”¨çš„æ ¼å¼ï¼‰
    json_file = api_data_dir / "dictionary.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(api_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä¿å­˜åˆ°: {json_file}")
    
    # ä¹Ÿä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆå¤‡ç”¨ï¼‰
    csv_file = api_data_dir / "dictionary.csv"
    df = pd.DataFrame(api_data)
    df.to_csv(csv_file, index=False, encoding='utf-8')
    
    print(f"âœ… å·²ä¿å­˜åˆ°: {csv_file}")
    
    # åˆ›å»ºç»Ÿè®¡æ–‡ä»¶
    stats = {
        'total_terms': len(api_data),
        'categories': {},
        'types': {},
        'migration_date': datetime.now().isoformat(),
        'source': 'unified_dictionary_migration'
    }
    
    # ç»Ÿè®¡åˆ†ç±»åˆ†å¸ƒ
    for item in api_data:
        category = item.get('category', 'unknown')
        type_name = item.get('type', 'unknown')
        
        stats['categories'][category] = stats['categories'].get(category, 0) + 1
        stats['types'][type_name] = stats['types'].get(type_name, 0) + 1
    
    stats_file = api_data_dir / "dictionary_stats.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç»Ÿè®¡æ–‡ä»¶å·²ä¿å­˜åˆ°: {stats_file}")
    
    return stats

def update_neo4j_graph(api_data):
    """æ›´æ–°Neo4jå›¾è°±æ•°æ®"""
    print("ğŸ”„ æ›´æ–°Neo4jå›¾è°±æ•°æ®...")
    
    # ç”ŸæˆCypherå¯¼å…¥è„šæœ¬
    cypher_statements = []
    
    # æ¸…ç©ºç°æœ‰è¯å…¸èŠ‚ç‚¹ï¼ˆå¯é€‰ï¼‰
    # cypher_statements.append("MATCH (n:Dictionary) DELETE n;")
    
    # åˆ›å»ºè¯å…¸èŠ‚ç‚¹
    for item in api_data:
        term = item['name'].replace("'", "\\'")
        description = item.get('description', '').replace("'", "\\'")
        category = item.get('category', '').replace("'", "\\'")
        type_name = item.get('type', '').replace("'", "\\'")
        
        aliases_str = "', '".join([alias.replace("'", "\\'") for alias in item.get('aliases', [])])
        tags_str = "', '".join([tag.replace("'", "\\'") for tag in item.get('tags', [])])
        
        cypher = f"""CREATE (d:Dictionary {{
    id: '{item['id']}',
    name: '{term}',
    type: '{type_name}',
    category: '{category}',
    aliases: ['{aliases_str}'],
    tags: ['{tags_str}'],
    description: '{description}',
    created_at: '{item['created_at']}',
    updated_at: '{item['updated_at']}'
}});"""
        
        cypher_statements.append(cypher)
    
    # ä¿å­˜Cypherè„šæœ¬
    cypher_file = Path("è¯å…¸æ•°æ®å›¾è°±æ›´æ–°è„šæœ¬.cypher")
    with open(cypher_file, 'w', encoding='utf-8') as f:
        f.write("// è¯å…¸æ•°æ®å›¾è°±æ›´æ–°è„šæœ¬\n")
        f.write(f"// ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}\n")
        f.write(f"// æ•°æ®æ¡æ•°: {len(api_data)}\n\n")
        f.write('\n'.join(cypher_statements))
    
    print(f"âœ… Cypherè„šæœ¬å·²ç”Ÿæˆ: {cypher_file}")
    
    # å°è¯•é€šè¿‡HTTP APIæ‰§è¡Œï¼ˆå¦‚æœNeo4jå¯ç”¨ï¼‰
    try:
        # åˆ†æ‰¹æ‰§è¡Œï¼Œæ¯æ‰¹50æ¡
        batch_size = 50
        total_batches = (len(cypher_statements) + batch_size - 1) // batch_size
        
        print(f"ğŸ“Š å‡†å¤‡æ‰§è¡Œ {total_batches} ä¸ªæ‰¹æ¬¡...")
        
        for i in range(0, len(cypher_statements), batch_size):
            batch = cypher_statements[i:i+batch_size]
            batch_cypher = '\n'.join(batch)
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„Neo4j HTTP APIè°ƒç”¨
            print(f"  æ‰¹æ¬¡ {i//batch_size + 1}/{total_batches}: {len(batch)} æ¡è¯­å¥")
        
        print("âœ… å›¾è°±æ›´æ–°è„šæœ¬å·²å‡†å¤‡å°±ç»ª")
        
    except Exception as e:
        print(f"âš ï¸ è‡ªåŠ¨æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ‰§è¡ŒCypherè„šæœ¬: {e}")

def restart_api_service():
    """é‡å¯APIæœåŠ¡"""
    print("ğŸ”„ å»ºè®®é‡å¯APIæœåŠ¡ä»¥åŠ è½½æ–°æ•°æ®...")
    
    restart_guide = """
# APIæœåŠ¡é‡å¯æŒ‡å—

## æ–¹æ³•1: æ‰‹åŠ¨é‡å¯
1. åœæ­¢å½“å‰APIæœåŠ¡ (Ctrl+C)
2. é‡æ–°å¯åŠ¨:
   cd api
   python main.py

## æ–¹æ³•2: æ£€æŸ¥æ•°æ®åŠ è½½
è®¿é—® http://localhost:8000/docs æŸ¥çœ‹APIæ–‡æ¡£
æµ‹è¯•è¯å…¸ç«¯ç‚¹æ˜¯å¦è¿”å›æ–°æ•°æ®

## éªŒè¯æ­¥éª¤
1. è®¿é—®å‰ç«¯: http://localhost:5173
2. è¿›å…¥è¯å…¸ç®¡ç†é¡µé¢
3. æ£€æŸ¥æ˜¯å¦æ˜¾ç¤º1192æ¡æ•°æ®
4. æœç´¢ç¡¬ä»¶æ¨¡å—ç›¸å…³è¯æ¡
"""
    
    with open("APIæœåŠ¡é‡å¯æŒ‡å—.md", "w", encoding="utf-8") as f:
        f.write(restart_guide)
    
    print("âœ… é‡å¯æŒ‡å—å·²åˆ›å»º: APIæœåŠ¡é‡å¯æŒ‡å—.md")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ•°æ®è¿ç§»æ±‡æ€»æ›´æ–°")
    print("=" * 60)
    
    # 1. å¤‡ä»½å½“å‰æ•°æ®
    backup_dir = backup_current_api_data()
    
    # 2. åŠ è½½ç»Ÿä¸€è¯å…¸æ•°æ®
    unified_df = load_unified_dictionary_data()
    if unified_df is None:
        print("âŒ æ— æ³•åŠ è½½ç»Ÿä¸€è¯å…¸æ•°æ®ï¼Œé€€å‡º")
        return
    
    # 3. è½¬æ¢ä¸ºAPIæ ¼å¼
    api_data = convert_to_api_format(unified_df)
    
    # 4. ä¿å­˜åˆ°APIæ•°æ®æº
    stats = save_to_api_data_source(api_data)
    
    # 5. æ›´æ–°Neo4jå›¾è°±
    update_neo4j_graph(api_data)
    
    # 6. é‡å¯æœåŠ¡æŒ‡å—
    restart_api_service()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ•°æ®è¿ç§»æ±‡æ€»å®Œæˆ!")
    print(f"ğŸ’¾ å¤‡ä»½ç›®å½•: {backup_dir}")
    print(f"ğŸ“Š è¿ç§»æ•°æ®: {len(api_data)} æ¡")
    print(f"ğŸ“Š æ•°æ®åˆ†å¸ƒ:")
    for type_name, count in stats['types'].items():
        print(f"  {type_name}: {count} æ¡")
    
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print(f"1. é‡å¯APIæœåŠ¡åŠ è½½æ–°æ•°æ®")
    print(f"2. æ‰§è¡ŒNeo4jå›¾è°±æ›´æ–°è„šæœ¬")
    print(f"3. éªŒè¯å‰ç«¯æ˜¾ç¤º1192æ¡è¯å…¸æ•°æ®")
    print(f"4. æµ‹è¯•æœç´¢ç¡¬ä»¶æ¨¡å—è¯æ±‡åŠŸèƒ½")
    
    print(f"\nğŸ¯ é¢„æœŸç»“æœ:")
    print(f"- å‰ç«¯è¯å…¸ç®¡ç†é¡µé¢æ˜¾ç¤º1192æ¡æ•°æ®")
    print(f"- åŒ…å«å®Œæ•´çš„20ä¸ªç¡¬ä»¶æ¨¡å—ä¸“ä¸šè¯æ±‡")
    print(f"- å›¾è°±ä¸­åŒ…å«æ‰€æœ‰è¯å…¸èŠ‚ç‚¹å’Œå…³ç³»")
    print(f"- ç»Ÿä¸€çš„æ•°æ®æºï¼Œæ— è·¯å¾„å†²çª")

if __name__ == "__main__":
    main()
