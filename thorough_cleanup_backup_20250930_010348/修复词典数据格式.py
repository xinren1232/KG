#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤è¯å…¸æ•°æ®æ ¼å¼ - æ¸…ç†aliaseså’Œtagså­—æ®µçš„æ ¼å¼é”™è¯¯
"""

import json
import re
from pathlib import Path
from datetime import datetime

def clean_list_field(field_value):
    """æ¸…ç†åˆ—è¡¨å­—æ®µï¼Œå»é™¤åµŒå¥—å­—ç¬¦ä¸²å’Œæ ¼å¼é”™è¯¯"""
    if not field_value:
        return []
    
    if isinstance(field_value, str):
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
        field_value = [field_value]
    
    if not isinstance(field_value, list):
        return []
    
    cleaned_items = set()  # ä½¿ç”¨setå»é‡
    
    for item in field_value:
        if not item or not isinstance(item, str):
            continue
        
        # æ¸…ç†å•ä¸ªé¡¹ç›®
        cleaned_item = clean_single_item(item)
        if cleaned_item:
            cleaned_items.update(cleaned_item)
    
    return list(cleaned_items)

def clean_single_item(item):
    """æ¸…ç†å•ä¸ªé¡¹ç›®"""
    if not item or not isinstance(item, str):
        return []
    
    # ç§»é™¤å¤šä½™çš„å¼•å·å’Œæ‹¬å·
    item = item.strip()
    
    # å¤„ç†å„ç§åˆ†éš”ç¬¦
    separators = [';', ',', 'ã€', 'ï¼Œ', 'ï¼›']
    items = [item]
    
    for sep in separators:
        new_items = []
        for it in items:
            new_items.extend([x.strip() for x in it.split(sep) if x.strip()])
        items = new_items
    
    # æ¸…ç†æ¯ä¸ªé¡¹ç›®
    cleaned = []
    for it in items:
        # ç§»é™¤å„ç§æ‹¬å·å’Œå¼•å·
        it = re.sub(r'^[\[\(\'"]+', '', it)
        it = re.sub(r'[\]\)\'"]+$', '', it)
        it = re.sub(r'^[\[\(\'"]+', '', it)  # å†æ¬¡æ¸…ç†
        it = re.sub(r'[\]\)\'"]+$', '', it)
        
        # ç§»é™¤åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ— æ•ˆé¡¹
        if re.search(r'[\[\]{}()"\']', it):
            continue
        
        # ç§»é™¤ç©ºé¡¹å’Œè¿‡çŸ­é¡¹
        it = it.strip()
        if len(it) > 1 and not re.match(r'^[^\w\u4e00-\u9fff]+$', it):
            cleaned.append(it)
    
    return cleaned

def fix_dictionary_data():
    """ä¿®å¤è¯å…¸æ•°æ®"""
    print("ğŸ”§ ä¿®å¤è¯å…¸æ•°æ®æ ¼å¼...")
    
    # è¯»å–åŸå§‹æ•°æ®
    input_file = Path("api/data/dictionary.json")
    
    if not input_file.exists():
        print(f"âŒ è¯å…¸æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return False
    
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(data)} æ¡")
        
        # å¤‡ä»½åŸå§‹æ–‡ä»¶
        backup_file = Path(f"api/data/dictionary_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å¤‡ä»½æ–‡ä»¶: {backup_file}")
        
        # ä¿®å¤æ•°æ®
        fixed_data = []
        error_count = 0
        
        for i, item in enumerate(data):
            try:
                fixed_item = fix_single_entry(item)
                if fixed_item:
                    fixed_data.append(fixed_item)
                else:
                    error_count += 1
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ¡ç›® {i}: {item.get('term', 'Unknown')}")
            except Exception as e:
                error_count += 1
                print(f"âŒ å¤„ç†æ¡ç›® {i} æ—¶å‡ºé”™: {e}")
        
        print(f"âœ… ä¿®å¤å®Œæˆ: {len(fixed_data)} æ¡æœ‰æ•ˆæ•°æ®")
        print(f"âš ï¸ é”™è¯¯æ¡ç›®: {error_count} æ¡")
        
        # ä¿å­˜ä¿®å¤åçš„æ•°æ®
        with open(input_file, 'w', encoding='utf-8') as f:
            json.dump(fixed_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ä¿®å¤åæ•°æ®å·²ä¿å­˜: {input_file}")
        
        # ç”Ÿæˆä¿®å¤æŠ¥å‘Š
        generate_fix_report(data, fixed_data, error_count)
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤è¿‡ç¨‹å‡ºé”™: {e}")
        return False

def fix_single_entry(item):
    """ä¿®å¤å•ä¸ªè¯å…¸æ¡ç›®"""
    if not isinstance(item, dict):
        return None
    
    # å¿…é¡»æœ‰termå­—æ®µ
    term = item.get('term', '').strip()
    if not term:
        return None
    
    fixed_item = {
        'term': term,
        'aliases': clean_list_field(item.get('aliases', [])),
        'category': item.get('category', '').strip(),
        'tags': clean_list_field(item.get('tags', [])),
        'description': item.get('description', '').strip(),
        'sub_category': item.get('sub_category', '').strip(),
        'source': item.get('source', '').strip(),
        'status': item.get('status', '').strip()
    }
    
    # ç§»é™¤ç©ºå­—æ®µ
    cleaned_item = {}
    for key, value in fixed_item.items():
        if key in ['term']:  # å¿…éœ€å­—æ®µ
            cleaned_item[key] = value
        elif key in ['aliases', 'tags']:  # åˆ—è¡¨å­—æ®µ
            if value:  # åªä¿ç•™éç©ºåˆ—è¡¨
                cleaned_item[key] = value
            else:
                cleaned_item[key] = []
        else:  # å­—ç¬¦ä¸²å­—æ®µ
            if value:  # åªä¿ç•™éç©ºå­—ç¬¦ä¸²
                cleaned_item[key] = value
            else:
                cleaned_item[key] = ''
    
    return cleaned_item

def generate_fix_report(original_data, fixed_data, error_count):
    """ç”Ÿæˆä¿®å¤æŠ¥å‘Š"""
    print("ğŸ“ ç”Ÿæˆä¿®å¤æŠ¥å‘Š...")
    
    # ç»Ÿè®¡ä¿®å¤å‰åçš„æ•°æ®
    original_stats = analyze_data(original_data)
    fixed_stats = analyze_data(fixed_data)
    
    report = {
        'fix_time': datetime.now().isoformat(),
        'original_count': len(original_data),
        'fixed_count': len(fixed_data),
        'error_count': error_count,
        'original_stats': original_stats,
        'fixed_stats': fixed_stats,
        'improvements': {
            'aliases_cleaned': original_stats['avg_aliases'] - fixed_stats['avg_aliases'],
            'tags_cleaned': original_stats['avg_tags'] - fixed_stats['avg_tags'],
            'empty_terms_removed': original_stats['empty_terms'] - fixed_stats['empty_terms']
        }
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = Path("è¯å…¸æ•°æ®ä¿®å¤æŠ¥å‘Š.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ ä¿®å¤æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # æ‰“å°å…³é”®ç»Ÿè®¡
    print(f"\nğŸ“Š ä¿®å¤ç»Ÿè®¡:")
    print(f"  åŸå§‹æ•°æ®: {len(original_data)} æ¡")
    print(f"  ä¿®å¤åæ•°æ®: {len(fixed_data)} æ¡")
    print(f"  é”™è¯¯æ¡ç›®: {error_count} æ¡")
    print(f"  å¹³å‡åˆ«åæ•°: {original_stats['avg_aliases']:.1f} â†’ {fixed_stats['avg_aliases']:.1f}")
    print(f"  å¹³å‡æ ‡ç­¾æ•°: {original_stats['avg_tags']:.1f} â†’ {fixed_stats['avg_tags']:.1f}")
    print(f"  ç©ºæœ¯è¯­æ¡ç›®: {original_stats['empty_terms']} â†’ {fixed_stats['empty_terms']}")

def analyze_data(data):
    """åˆ†ææ•°æ®ç»Ÿè®¡"""
    if not data:
        return {'avg_aliases': 0, 'avg_tags': 0, 'empty_terms': 0}
    
    total_aliases = 0
    total_tags = 0
    empty_terms = 0
    
    for item in data:
        if isinstance(item, dict):
            if not item.get('term', '').strip():
                empty_terms += 1
            
            aliases = item.get('aliases', [])
            if isinstance(aliases, list):
                total_aliases += len(aliases)
            
            tags = item.get('tags', [])
            if isinstance(tags, list):
                total_tags += len(tags)
    
    return {
        'avg_aliases': total_aliases / len(data) if data else 0,
        'avg_tags': total_tags / len(data) if data else 0,
        'empty_terms': empty_terms
    }

def test_fixed_data():
    """æµ‹è¯•ä¿®å¤åçš„æ•°æ®"""
    print("ğŸ” æµ‹è¯•ä¿®å¤åçš„æ•°æ®...")
    
    input_file = Path("api/data/dictionary.json")
    
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ“Š æ•°æ®æ€»æ•°: {len(data)}")
        
        # æ£€æŸ¥å‰å‡ æ¡æ•°æ®
        for i, item in enumerate(data[:3]):
            print(f"\nğŸ“‹ ç¤ºä¾‹ {i+1}:")
            print(f"  æœ¯è¯­: {item.get('term', 'N/A')}")
            print(f"  ç±»åˆ«: {item.get('category', 'N/A')}")
            print(f"  åˆ«å: {item.get('aliases', [])} ({len(item.get('aliases', []))} ä¸ª)")
            print(f"  æ ‡ç­¾: {item.get('tags', [])} ({len(item.get('tags', []))} ä¸ª)")
            print(f"  æè¿°: {item.get('description', 'N/A')[:50]}...")
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        valid_terms = sum(1 for item in data if item.get('term', '').strip())
        has_aliases = sum(1 for item in data if item.get('aliases'))
        has_tags = sum(1 for item in data if item.get('tags'))
        has_description = sum(1 for item in data if item.get('description', '').strip())
        
        print(f"\nğŸ“Š æ•°æ®è´¨é‡:")
        print(f"  æœ‰æ•ˆæœ¯è¯­: {valid_terms}/{len(data)} ({valid_terms/len(data)*100:.1f}%)")
        print(f"  æœ‰åˆ«å: {has_aliases}/{len(data)} ({has_aliases/len(data)*100:.1f}%)")
        print(f"  æœ‰æ ‡ç­¾: {has_tags}/{len(data)} ({has_tags/len(data)*100:.1f}%)")
        print(f"  æœ‰æè¿°: {has_description}/{len(data)} ({has_description/len(data)*100:.1f}%)")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¿®å¤è¯å…¸æ•°æ®æ ¼å¼")
    print("=" * 50)
    
    # 1. ä¿®å¤æ•°æ®
    success = fix_dictionary_data()
    
    if success:
        # 2. æµ‹è¯•ä¿®å¤ç»“æœ
        test_fixed_data()
        
        print("\n" + "=" * 50)
        print("âœ… è¯å…¸æ•°æ®ä¿®å¤å®Œæˆ!")
        print("ğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("  1. é‡å¯APIæœåŠ¡")
        print("  2. åˆ·æ–°å‰ç«¯é¡µé¢")
        print("  3. éªŒè¯æ•°æ®æ˜¾ç¤ºæ­£å¸¸")
    else:
        print("\nâŒ è¯å…¸æ•°æ®ä¿®å¤å¤±è´¥")

if __name__ == "__main__":
    main()
