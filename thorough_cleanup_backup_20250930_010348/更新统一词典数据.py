#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›´æ–°ç»Ÿä¸€è¯å…¸æ•°æ® - æ·»åŠ 654æ¡æ–°æ•°æ®
"""

import pandas as pd
import json
import csv
from datetime import datetime
from pathlib import Path

def backup_current_data():
    """å¤‡ä»½å½“å‰æ•°æ®"""
    print("ğŸ’¾ å¤‡ä»½å½“å‰ç»Ÿä¸€è¯å…¸æ•°æ®...")
    
    backup_dir = Path("data/dictionary_backup") / f"before_update_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    backup_dir.mkdir(parents=True, exist_ok=True)
    
    unified_dir = Path("data/unified_dictionary")
    if unified_dir.exists():
        import shutil
        shutil.copytree(unified_dir, backup_dir / "unified_dictionary")
        print(f"âœ… å¤‡ä»½å®Œæˆ: {backup_dir}")
    
    return backup_dir

def load_new_supplement_data():
    """åŠ è½½æ–°çš„è¡¥å……æ•°æ®"""
    print("ğŸ“– åŠ è½½æ–°çš„è¡¥å……æ•°æ®...")
    
    # è¯»å–åŸºç¡€è¡¥å……æ•°æ®
    basic_files = [
        ('è¡¥å……è¯å…¸æ•°æ®_æ‰¹æ¬¡1.csv', 'åŸºç¡€è¡¥å……æ‰¹æ¬¡1'),
        ('è¡¥å……è¯å…¸æ•°æ®_æ‰¹æ¬¡2.csv', 'åŸºç¡€è¡¥å……æ‰¹æ¬¡2')
    ]
    
    # è¯»å–20ä¸ªç¡¬ä»¶æ¨¡å—æ•°æ®
    hardware_files = [
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_æ˜¾ç¤ºå±.csv', 'æ˜¾ç¤ºå±æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_æ‘„åƒå¤´.csv', 'æ‘„åƒå¤´æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_ç”µæ± .csv', 'ç”µæ± æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_ä¸»æ¿PCBA.csv', 'ä¸»æ¿PCBAæ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_å°„é¢‘å¤©çº¿.csv', 'å°„é¢‘ä¸å¤©çº¿æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_å£°å­¦.csv', 'å£°å­¦æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_ç»“æ„è¿æ¥å™¨.csv', 'ç»“æ„ä»¶ä¸è¿æ¥å™¨æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_æ•£çƒ­ç³»ç»Ÿ.csv', 'æ•£çƒ­ç³»ç»Ÿæ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_ä¼ æ„Ÿå™¨.csv', 'ä¼ æ„Ÿå™¨æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_å……ç”µç”µæº.csv', 'å……ç”µä¸ç”µæºç®¡ç†æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_é©¬è¾¾è§¦è§‰.csv', 'é©¬è¾¾ä¸è§¦è§‰åé¦ˆæ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_å¤–å£³æ¶‚å±‚.csv', 'å¤–å£³æ¶‚å±‚ä¸å¤–è§‚æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_è¿æ¥ç½‘ç»œ.csv', 'è¿æ¥ä¸ç½‘ç»œæ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_æ¥å£è¿æ¥å™¨.csv', 'æ¥å£ä¸è¿æ¥å™¨æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_è¢«åŠ¨å…ƒä»¶.csv', 'è¢«åŠ¨å…ƒä»¶ä¸ç”µè·¯ä¿æŠ¤æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_ç”Ÿäº§æµ‹è¯•æ²»å…·.csv', 'ç”Ÿäº§ä¸æµ‹è¯•æ²»å…·æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_ææ–™ç§‘å­¦åŸºç¡€.csv', 'ææ–™ç§‘å­¦åŸºç¡€æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_å…ˆè¿›åˆ¶é€ å·¥è‰º.csv', 'å…ˆè¿›åˆ¶é€ å·¥è‰ºæ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_å¤±æ•ˆåˆ†æå¯é æ€§.csv', 'å¤±æ•ˆåˆ†æä¸å¯é æ€§å·¥ç¨‹æ¨¡å—'),
        ('ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_æ ‡å‡†æ³•è§„.csv', 'æ ‡å‡†ä¸æ³•è§„æ¨¡å—')
    ]
    
    all_files = basic_files + hardware_files
    all_data = []
    
    for file_name, desc in all_files:
        try:
            df = pd.read_csv(file_name, encoding='utf-8')
            all_data.append(df)
            print(f"âœ… {desc}: {len(df)} æ¡è®°å½•")
        except Exception as e:
            print(f"âŒ è¯»å– {desc} å¤±è´¥: {e}")
    
    if all_data:
        df_all = pd.concat(all_data, ignore_index=True)
        print(f"ğŸ“Š æ€»è®¡åŠ è½½: {len(df_all)} æ¡æ–°æ•°æ®")
        return df_all
    else:
        print("âŒ æœªèƒ½åŠ è½½ä»»ä½•æ–°æ•°æ®")
        return None

def categorize_data(df):
    """å°†æ•°æ®æŒ‰ç…§ç»Ÿä¸€è¯å…¸çš„åˆ†ç±»è¿›è¡Œå½’ç±»"""
    print("ğŸ“‹ æŒ‰ç»Ÿä¸€è¯å…¸åˆ†ç±»å½’ç±»æ•°æ®...")
    
    categories = {
        "components": [],
        "symptoms": [],
        "causes": [],
        "countermeasures": []
    }
    
    for _, row in df.iterrows():
        term = row['term']
        category = row['category']
        
        # è½¬æ¢ä¸ºç»Ÿä¸€è¯å…¸æ ¼å¼
        item = {
            'term': term,
            'canonical_name': term,
            'aliases': row.get('aliases', ''),
            'category': category,
            'tags': row.get('tags', ''),
            'description': row.get('definition', ''),
            'source_file': 'hardware_module_expansion',
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        # æ ¹æ®Labelåˆ†ç±»
        if category in ['Component', 'Material']:
            categories["components"].append(item)
        elif category in ['Symptom']:
            categories["symptoms"].append(item)
        elif category in ['Process', 'TestCase', 'Tool', 'Role']:
            categories["countermeasures"].append(item)
        elif category in ['Metric']:
            categories["components"].append(item)  # æ€§èƒ½æŒ‡æ ‡å½’ç±»åˆ°ç»„ä»¶
        else:
            categories["countermeasures"].append(item)  # é»˜è®¤å½’ç±»åˆ°å¯¹ç­–
    
    print(f"ğŸ“Š åˆ†ç±»ç»“æœ:")
    for cat, items in categories.items():
        print(f"  {cat}: {len(items)} æ¡")
    
    return categories

def load_existing_data():
    """åŠ è½½ç°æœ‰çš„ç»Ÿä¸€è¯å…¸æ•°æ®"""
    print("ğŸ“– åŠ è½½ç°æœ‰ç»Ÿä¸€è¯å…¸æ•°æ®...")
    
    unified_dir = Path("data/unified_dictionary")
    existing_data = {
        "components": [],
        "symptoms": [],
        "causes": [],
        "countermeasures": []
    }
    
    for category in existing_data.keys():
        file_path = unified_dir / f"{category}.csv"
        if file_path.exists():
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                existing_data[category] = df.to_dict('records')
                print(f"  {category}: {len(existing_data[category])} æ¡ç°æœ‰æ•°æ®")
            except Exception as e:
                print(f"âŒ è¯»å– {category}.csv å¤±è´¥: {e}")
    
    return existing_data

def merge_and_save_data(existing_data, new_categories):
    """åˆå¹¶å¹¶ä¿å­˜æ•°æ®"""
    print("ğŸ’¾ åˆå¹¶å¹¶ä¿å­˜æ•°æ®...")
    
    unified_dir = Path("data/unified_dictionary")
    fieldnames = ['term', 'canonical_name', 'aliases', 'category', 'tags', 'description', 'source_file', 'created_at', 'updated_at']
    
    total_added = 0
    final_counts = {}
    
    for category_name in existing_data.keys():
        existing_items = existing_data[category_name]
        new_items = new_categories[category_name]
        
        # å»é‡ - åŸºäºtermå­—æ®µ
        existing_terms = {item['term'].lower() for item in existing_items}
        unique_new_items = []
        
        for item in new_items:
            if item['term'].lower() not in existing_terms:
                unique_new_items.append(item)
            else:
                print(f"âš ï¸ è·³è¿‡é‡å¤è¯æ¡: {item['term']}")
        
        # åˆå¹¶æ•°æ®
        all_items = existing_items + unique_new_items
        final_counts[category_name] = len(all_items)
        total_added += len(unique_new_items)
        
        # ä¿å­˜æ–‡ä»¶
        file_path = unified_dir / f"{category_name}.csv"
        with open(file_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_items)
        
        print(f"âœ… æ›´æ–° {category_name}.csv: æ–°å¢{len(unique_new_items)}æ¡ï¼Œæ€»è®¡{len(all_items)}æ¡")
    
    # æ›´æ–°ç»Ÿè®¡æ–‡ä»¶
    stats = {
        'total_terms': sum(final_counts.values()),
        'categories': final_counts,
        'last_updated': datetime.now().isoformat(),
        'last_expansion': {
            'date': datetime.now().isoformat(),
            'added_count': total_added,
            'source': 'hardware_module_expansion_654_terms',
            'modules_covered': 20,
            'tech_domains': 18
        },
        'unified_directory': str(unified_dir)
    }
    
    with open(unified_dir / "statistics.json", 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç»Ÿè®¡æ–‡ä»¶å·²æ›´æ–°")
    return total_added, final_counts

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ›´æ–°ç»Ÿä¸€è¯å…¸æ•°æ® - æ·»åŠ 654æ¡ç¡¬ä»¶æ¨¡å—æ•°æ®")
    print("=" * 60)
    
    # 1. å¤‡ä»½å½“å‰æ•°æ®
    backup_dir = backup_current_data()
    
    # 2. åŠ è½½æ–°çš„è¡¥å……æ•°æ®
    new_df = load_new_supplement_data()
    if new_df is None:
        print("âŒ æ— æ³•åŠ è½½æ–°æ•°æ®ï¼Œé€€å‡º")
        return
    
    # 3. åˆ†ç±»æ–°æ•°æ®
    new_categories = categorize_data(new_df)
    
    # 4. åŠ è½½ç°æœ‰æ•°æ®
    existing_data = load_existing_data()
    
    # 5. åˆå¹¶å¹¶ä¿å­˜
    total_added, final_counts = merge_and_save_data(existing_data, new_categories)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ç»Ÿä¸€è¯å…¸æ•°æ®æ›´æ–°å®Œæˆ!")
    print(f"ğŸ’¾ å¤‡ä»½ç›®å½•: {backup_dir}")
    print(f"ğŸ“Š æœ¬æ¬¡æ–°å¢: {total_added} æ¡")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    for cat, count in final_counts.items():
        print(f"  {cat}: {count} æ¡")
    print(f"ğŸ“Š æ€»è®¡: {sum(final_counts.values())} æ¡")
    
    growth_rate = (total_added / 110) * 100  # åŸæœ‰110æ¡
    print(f"ğŸ“ˆ å¢é•¿ç‡: +{growth_rate:.1f}%")
    
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"1. é‡å¯APIæœåŠ¡ä»¥åŠ è½½æ–°æ•°æ®")
    print(f"2. é‡å¯å‰ç«¯æœåŠ¡")
    print(f"3. éªŒè¯å‰ç«¯è¯å…¸é¡µé¢æ˜¾ç¤º")
    print(f"4. æµ‹è¯•æ–°å¢ç¡¬ä»¶æ¨¡å—æ•°æ®æŸ¥è¯¢")

if __name__ == "__main__":
    main()
