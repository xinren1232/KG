#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›®å½•æ¸…ç†åˆ†ææŠ¥å‘Š
åˆ†æå½“å‰ç›®å½•ç»“æ„ï¼Œè¯†åˆ«é‡å¤ã€å¤šä½™å’Œå†²çªçš„æ–‡ä»¶
"""

import os
import re
from pathlib import Path
from collections import defaultdict
import json

def analyze_directory_structure():
    """åˆ†æç›®å½•ç»“æ„"""
    print("ğŸ“ ç›®å½•ç»“æ„åˆ†æ")
    print("=" * 60)
    
    # åˆ†ç±»æ–‡ä»¶
    file_categories = {
        "æŠ¥å‘Šæ–‡ä»¶": [],
        "è„šæœ¬æ–‡ä»¶": [],
        "é…ç½®æ–‡ä»¶": [],
        "æ•°æ®æ–‡ä»¶": [],
        "æµ‹è¯•æ–‡ä»¶": [],
        "æ–‡æ¡£æ–‡ä»¶": [],
        "ä¸´æ—¶æ–‡ä»¶": [],
        "é‡å¤æ–‡ä»¶": [],
        "å†²çªæ–‡ä»¶": []
    }
    
    # æ‰«ææ ¹ç›®å½•æ–‡ä»¶
    root_files = [f for f in os.listdir('.') if os.path.isfile(f)]
    
    # æŒ‰ç±»å‹åˆ†ç±»
    for file in root_files:
        file_lower = file.lower()
        
        # æŠ¥å‘Šæ–‡ä»¶
        if any(keyword in file for keyword in ['REPORT', 'æŠ¥å‘Š', 'æ€»ç»“', 'æŒ‡å—', 'SUMMARY']):
            file_categories["æŠ¥å‘Šæ–‡ä»¶"].append(file)
        
        # è„šæœ¬æ–‡ä»¶
        elif file.endswith(('.py', '.bat', '.sh')):
            file_categories["è„šæœ¬æ–‡ä»¶"].append(file)
        
        # é…ç½®æ–‡ä»¶
        elif file.endswith(('.yml', '.yaml', '.json', '.conf', '.cfg')):
            file_categories["é…ç½®æ–‡ä»¶"].append(file)
        
        # æ•°æ®æ–‡ä»¶
        elif file.endswith(('.csv', '.xlsx', '.cypher')):
            file_categories["æ•°æ®æ–‡ä»¶"].append(file)
        
        # æµ‹è¯•æ–‡ä»¶
        elif 'test' in file_lower or 'æµ‹è¯•' in file:
            file_categories["æµ‹è¯•æ–‡ä»¶"].append(file)
        
        # æ–‡æ¡£æ–‡ä»¶
        elif file.endswith(('.md', '.txt', '.doc', '.docx', '.pdf')):
            file_categories["æ–‡æ¡£æ–‡ä»¶"].append(file)
        
        # ä¸´æ—¶æ–‡ä»¶
        elif any(temp in file_lower for temp in ['temp', 'tmp', 'backup', '_old', '_bak']):
            file_categories["ä¸´æ—¶æ–‡ä»¶"].append(file)
    
    # æ‰“å°åˆ†ç±»ç»“æœ
    for category, files in file_categories.items():
        if files:
            print(f"\n{category} ({len(files)}ä¸ª):")
            for file in sorted(files)[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"   - {file}")
            if len(files) > 10:
                print(f"   ... è¿˜æœ‰{len(files) - 10}ä¸ªæ–‡ä»¶")
    
    return file_categories

def identify_duplicate_files():
    """è¯†åˆ«é‡å¤æ–‡ä»¶"""
    print("\nğŸ” é‡å¤æ–‡ä»¶è¯†åˆ«")
    print("=" * 60)
    
    # æŒ‰åŠŸèƒ½åˆ†ç»„çš„é‡å¤æ–‡ä»¶
    duplicate_groups = {
        "APIç›¸å…³": [],
        "Neo4jç›¸å…³": [],
        "å‰ç«¯ç›¸å…³": [],
        "æ•°æ®å¯¼å…¥": [],
        "ç³»ç»Ÿæ£€æŸ¥": [],
        "è¯å…¸å¤„ç†": [],
        "å›¾è°±æ›´æ–°": [],
        "æœåŠ¡å¯åŠ¨": []
    }
    
    root_files = [f for f in os.listdir('.') if os.path.isfile(f)]
    
    for file in root_files:
        file_lower = file.lower()
        
        # APIç›¸å…³é‡å¤
        if any(keyword in file for keyword in ['api', 'API']):
            duplicate_groups["APIç›¸å…³"].append(file)
        
        # Neo4jç›¸å…³é‡å¤
        elif any(keyword in file for keyword in ['neo4j', 'Neo4j']):
            duplicate_groups["Neo4jç›¸å…³"].append(file)
        
        # å‰ç«¯ç›¸å…³é‡å¤
        elif any(keyword in file for keyword in ['å‰ç«¯', 'frontend', 'vue', 'VUE']):
            duplicate_groups["å‰ç«¯ç›¸å…³"].append(file)
        
        # æ•°æ®å¯¼å…¥é‡å¤
        elif any(keyword in file for keyword in ['å¯¼å…¥', 'åˆå¹¶', 'å¤„ç†', 'import']):
            duplicate_groups["æ•°æ®å¯¼å…¥"].append(file)
        
        # ç³»ç»Ÿæ£€æŸ¥é‡å¤
        elif any(keyword in file for keyword in ['æ£€æŸ¥', 'éªŒè¯', 'æµ‹è¯•', 'check', 'test', 'verify']):
            duplicate_groups["ç³»ç»Ÿæ£€æŸ¥"].append(file)
        
        # è¯å…¸å¤„ç†é‡å¤
        elif any(keyword in file for keyword in ['è¯å…¸', 'dictionary', 'dict']):
            duplicate_groups["è¯å…¸å¤„ç†"].append(file)
        
        # å›¾è°±æ›´æ–°é‡å¤
        elif any(keyword in file for keyword in ['å›¾è°±', 'æ›´æ–°', 'graph', 'update']):
            duplicate_groups["å›¾è°±æ›´æ–°"].append(file)
        
        # æœåŠ¡å¯åŠ¨é‡å¤
        elif any(keyword in file for keyword in ['å¯åŠ¨', 'é‡å¯', 'start', 'restart']):
            duplicate_groups["æœåŠ¡å¯åŠ¨"].append(file)
    
    # æ˜¾ç¤ºé‡å¤æ–‡ä»¶ç»„
    for group, files in duplicate_groups.items():
        if len(files) > 1:
            print(f"\n{group} (é‡å¤{len(files)}ä¸ª):")
            for file in sorted(files):
                print(f"   - {file}")
    
    return duplicate_groups

def identify_obsolete_files():
    """è¯†åˆ«è¿‡æ—¶æ–‡ä»¶"""
    print("\nğŸ—‘ï¸ è¿‡æ—¶æ–‡ä»¶è¯†åˆ«")
    print("=" * 60)
    
    obsolete_patterns = [
        # ç‰ˆæœ¬æ ‡è®°çš„æ—§æ–‡ä»¶
        r'.*_v\d+\..*',
        r'.*_old\..*',
        r'.*_backup\..*',
        r'.*_bak\..*',
        r'.*_temp\..*',
        r'.*_tmp\..*',
        
        # æ‰¹æ¬¡æ–‡ä»¶
        r'.*æ‰¹æ¬¡.*',
        r'.*batch.*',
        
        # æµ‹è¯•æ–‡ä»¶
        r'test_.*\.py',
        r'.*_test\..*',
        r'debug_.*\.py',
        
        # ä¸´æ—¶æŠ¥å‘Š
        r'.*æŠ¥å‘Š.*\.json',
        r'.*REPORT.*\.md',
        r'.*æ€»ç»“.*\.md'
    ]
    
    root_files = [f for f in os.listdir('.') if os.path.isfile(f)]
    obsolete_files = []
    
    for file in root_files:
        for pattern in obsolete_patterns:
            if re.match(pattern, file, re.IGNORECASE):
                obsolete_files.append(file)
                break
    
    print(f"å‘ç° {len(obsolete_files)} ä¸ªå¯èƒ½è¿‡æ—¶çš„æ–‡ä»¶:")
    for file in sorted(obsolete_files)[:20]:  # æ˜¾ç¤ºå‰20ä¸ª
        print(f"   - {file}")
    
    if len(obsolete_files) > 20:
        print(f"   ... è¿˜æœ‰{len(obsolete_files) - 20}ä¸ªæ–‡ä»¶")
    
    return obsolete_files

def analyze_directory_conflicts():
    """åˆ†æç›®å½•å†²çª"""
    print("\nâš ï¸ ç›®å½•å†²çªåˆ†æ")
    print("=" * 60)
    
    conflicts = []
    
    # æ£€æŸ¥é‡å¤åŠŸèƒ½çš„ç›®å½•
    directories = [d for d in os.listdir('.') if os.path.isdir(d)]
    
    # åŠŸèƒ½é‡å¤æ£€æŸ¥
    function_dirs = defaultdict(list)
    
    for dir_name in directories:
        dir_lower = dir_name.lower()
        
        if 'api' in dir_lower:
            function_dirs['APIæœåŠ¡'].append(dir_name)
        elif 'app' in dir_lower:
            function_dirs['åº”ç”¨'].append(dir_name)
        elif 'service' in dir_lower:
            function_dirs['æœåŠ¡'].append(dir_name)
        elif 'data' in dir_lower:
            function_dirs['æ•°æ®'].append(dir_name)
        elif 'config' in dir_lower:
            function_dirs['é…ç½®'].append(dir_name)
        elif 'test' in dir_lower:
            function_dirs['æµ‹è¯•'].append(dir_name)
        elif 'tool' in dir_lower:
            function_dirs['å·¥å…·'].append(dir_name)
    
    # æ˜¾ç¤ºå¯èƒ½çš„å†²çª
    for function, dirs in function_dirs.items():
        if len(dirs) > 1:
            print(f"{function}ç›®å½•é‡å¤: {', '.join(dirs)}")
            conflicts.append((function, dirs))
    
    return conflicts

def generate_cleanup_recommendations():
    """ç”Ÿæˆæ¸…ç†å»ºè®®"""
    print("\nğŸ’¡ æ¸…ç†å»ºè®®")
    print("=" * 60)
    
    recommendations = {
        "ç«‹å³åˆ é™¤": [
            # æ˜æ˜¾çš„ä¸´æ—¶æ–‡ä»¶
            "backup_data.sh",  # å·²æœ‰æ›´å¥½çš„éƒ¨ç½²è„šæœ¬
            "graph_backup_20250928_110602.json",  # æ—§å¤‡ä»½æ–‡ä»¶
            
            # é‡å¤çš„æµ‹è¯•æ–‡ä»¶
            "simple_test.doc", "simple_test.docx", "simple_test.txt", "simple_test.xlsx",
            "test.docx", "simple_test_debug.txt",
            
            # é‡å¤çš„å¯åŠ¨è„šæœ¬
            "å¯åŠ¨APIæœåŠ¡.bat",  # åŠŸèƒ½é‡å¤
            "å¯åŠ¨æ‰€æœ‰æœåŠ¡.bat",  # åŠŸèƒ½é‡å¤
            "å¿«é€Ÿä¿®å¤å¹¶å¯åŠ¨.bat",  # åŠŸèƒ½é‡å¤
            
            # è¿‡æ—¶çš„æ£€æŸ¥è„šæœ¬
            "check_api_status.py", "check_neo4j_data.py", "check_node_structure.py",
            "comprehensive_system_check.py",
            
            # é‡å¤çš„ä¿®å¤è„šæœ¬
            "ä¿®å¤APIæŸ¥è¯¢é€»è¾‘.py", "ä¿®å¤Labelåˆ†ç±».py", "ä¿®å¤neo4jæ˜¾ç¤º.py",
            "ä¿®å¤neo4jè¿æ¥.py", "ä¿®å¤å…³ç³»åˆ›å»º.py", "ä¿®å¤å‰ç«¯é”™è¯¯.py",
        ],
        
        "åˆå¹¶æ•´ç†": [
            # æ•°æ®å¯¼å…¥è„šæœ¬
            "åˆå¹¶å…¨éƒ¨12ä¸ªç¡¬ä»¶æ¨¡å—æ•°æ®.py", "åˆå¹¶å…¨éƒ¨16ä¸ªç¡¬ä»¶æ¨¡å—æ•°æ®.py", 
            "åˆå¹¶å…¨éƒ¨20ä¸ªç¡¬ä»¶æ¨¡å—æ•°æ®.py", "åˆå¹¶å…¨éƒ¨ç¡¬ä»¶æ¨¡å—æ•°æ®.py",
            
            # æ‰¹æ¬¡å¯¼å…¥æ–‡ä»¶
            "å¯¼å…¥æ‰¹æ¬¡_01.cypher", "å¯¼å…¥æ‰¹æ¬¡_02.cypher", "å¯¼å…¥æ‰¹æ¬¡_03.cypher",
            # ... å…¶ä»–æ‰¹æ¬¡æ–‡ä»¶
            
            # è¯å…¸æ•°æ®æ–‡ä»¶
            "ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_ä¸»æ¿PCBA.csv", "ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_ä¼ æ„Ÿå™¨.csv",
            # ... å…¶ä»–è¯å…¸æ–‡ä»¶
        ],
        
        "ç§»åŠ¨åˆ°å­ç›®å½•": [
            # æŠ¥å‘Šæ–‡ä»¶ç§»åŠ¨åˆ° reports/
            "ALL_SERVICES_STATUS_REPORT.md", "API_422_ERROR_FIX_REPORT.md",
            "BACKEND_SERVICE_STATUS.md", "COMPREHENSIVE_SYSTEM_REPORT.md",
            
            # è„šæœ¬æ–‡ä»¶ç§»åŠ¨åˆ° scripts/
            "append_dictionary_data.py", "append_new_dictionary_data.py",
            "clean_and_reimport.py", "cleanup_duplicate_dictionary_files.py",
            
            # é…ç½®æ–‡ä»¶ç§»åŠ¨åˆ° config/
            "Labelåˆ†ç±»ä¿®å¤æŠ¥å‘Š.json", "prompt_integration_report.json",
            "system_test_report.json",
        ],
        
        "ä¿ç•™æ ¸å¿ƒæ–‡ä»¶": [
            # é‡è¦çš„é…ç½®æ–‡ä»¶
            "docker-compose.prod.yml", "docker-compose.yml", "Dockerfile.api",
            
            # æ ¸å¿ƒè„šæœ¬
            "éƒ¨ç½²è„šæœ¬.sh", "å…¨é¢é‡å¯æ‰€æœ‰æœåŠ¡.py", "æœåŠ¡çŠ¶æ€æ£€æŸ¥.py",
            
            # é‡è¦æ–‡æ¡£
            "README.md", "å®Œæ•´éƒ¨ç½²ä¼˜åŒ–æ–¹æ¡ˆ.md", "ç³»ç»Ÿä¼˜åŒ–æ€»ç»“æŠ¥å‘Š.md",
            
            # æ ¸å¿ƒç›®å½•
            "api/", "apps/", "config/", "data/", "nginx/",
        ]
    }
    
    for category, files in recommendations.items():
        print(f"\n{category}:")
        for file in files[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            print(f"   - {file}")
        if len(files) > 10:
            print(f"   ... è¿˜æœ‰{len(files) - 10}ä¸ªæ–‡ä»¶")
    
    return recommendations

def create_cleanup_script():
    """åˆ›å»ºæ¸…ç†è„šæœ¬"""
    print("\nğŸ“ ç”Ÿæˆæ¸…ç†è„šæœ¬")
    print("=" * 60)
    
    cleanup_script = '''#!/bin/bash
# ç›®å½•æ¸…ç†è„šæœ¬ - åˆ é™¤é‡å¤ã€å¤šä½™å’Œå†²çªçš„æ–‡ä»¶

set -e

echo "ğŸ§¹ å¼€å§‹æ¸…ç†ç›®å½•..."

# åˆ›å»ºå¤‡ä»½ç›®å½•
BACKUP_DIR="cleanup_backup_$(date +%Y%m%d_%H%M%S)"
mkdir -p $BACKUP_DIR

echo "ğŸ“¦ åˆ›å»ºå¤‡ä»½: $BACKUP_DIR"

# 1. åˆ é™¤æ˜æ˜¾çš„ä¸´æ—¶å’Œé‡å¤æ–‡ä»¶
echo "ğŸ—‘ï¸ åˆ é™¤ä¸´æ—¶å’Œé‡å¤æ–‡ä»¶..."

# ä¸´æ—¶æµ‹è¯•æ–‡ä»¶
rm -f simple_test.* test.docx simple_test_debug.txt

# æ—§å¤‡ä»½æ–‡ä»¶
rm -f graph_backup_*.json backup_data.sh

# é‡å¤çš„å¯åŠ¨è„šæœ¬
rm -f å¯åŠ¨APIæœåŠ¡.bat å¯åŠ¨æ‰€æœ‰æœåŠ¡.bat å¿«é€Ÿä¿®å¤å¹¶å¯åŠ¨.bat

# é‡å¤çš„æ£€æŸ¥è„šæœ¬
rm -f check_api_status.py check_neo4j_data.py check_node_structure.py
rm -f comprehensive_system_check.py

# é‡å¤çš„ä¿®å¤è„šæœ¬
rm -f ä¿®å¤APIæŸ¥è¯¢é€»è¾‘.py ä¿®å¤Labelåˆ†ç±».py ä¿®å¤neo4jæ˜¾ç¤º.py
rm -f ä¿®å¤neo4jè¿æ¥.py ä¿®å¤å…³ç³»åˆ›å»º.py ä¿®å¤å‰ç«¯é”™è¯¯.py

# 2. æ•´ç†æŠ¥å‘Šæ–‡ä»¶
echo "ğŸ“‹ æ•´ç†æŠ¥å‘Šæ–‡ä»¶..."
mkdir -p reports/legacy
mv *REPORT*.md reports/legacy/ 2>/dev/null || true
mv *æŠ¥å‘Š*.md reports/legacy/ 2>/dev/null || true
mv *æ€»ç»“*.md reports/legacy/ 2>/dev/null || true

# 3. æ•´ç†è„šæœ¬æ–‡ä»¶
echo "ğŸ”§ æ•´ç†è„šæœ¬æ–‡ä»¶..."
mkdir -p scripts/legacy
mv append_*.py scripts/legacy/ 2>/dev/null || true
mv clean_*.py scripts/legacy/ 2>/dev/null || true
mv cleanup_*.py scripts/legacy/ 2>/dev/null || true
mv debug_*.py scripts/legacy/ 2>/dev/null || true
mv fix_*.py scripts/legacy/ 2>/dev/null || true

# 4. æ•´ç†æ•°æ®å¯¼å…¥æ–‡ä»¶
echo "ğŸ“Š æ•´ç†æ•°æ®å¯¼å…¥æ–‡ä»¶..."
mkdir -p data/import/legacy
mv å¯¼å…¥æ‰¹æ¬¡_*.cypher data/import/legacy/ 2>/dev/null || true
mv åˆå¹¶å…¨éƒ¨*æ¨¡å—æ•°æ®.py data/import/legacy/ 2>/dev/null || true
mv å¤„ç†*æ¨¡å—æ•°æ®.py data/import/legacy/ 2>/dev/null || true

# 5. æ•´ç†è¯å…¸æ•°æ®æ–‡ä»¶
echo "ğŸ“š æ•´ç†è¯å…¸æ•°æ®æ–‡ä»¶..."
mkdir -p data/dictionary/modules
mv ç¡¬ä»¶æ¨¡å—è¯å…¸æ•°æ®_*.csv data/dictionary/modules/ 2>/dev/null || true

# 6. æ•´ç†é…ç½®å’ŒæŠ¥å‘ŠJSONæ–‡ä»¶
echo "âš™ï¸ æ•´ç†é…ç½®æ–‡ä»¶..."
mkdir -p config/legacy
mv *æŠ¥å‘Š*.json config/legacy/ 2>/dev/null || true
mv *ç»Ÿè®¡*.json config/legacy/ 2>/dev/null || true

# 7. æ¸…ç†ç©ºç›®å½•
echo "ğŸ§½ æ¸…ç†ç©ºç›®å½•..."
find . -type d -empty -delete 2>/dev/null || true

echo "âœ… æ¸…ç†å®Œæˆ!"
echo "ğŸ“¦ å¤‡ä»½ä½ç½®: $BACKUP_DIR"
echo "ğŸ“ æ•´ç†åçš„ç›®å½•ç»“æ„æ›´åŠ æ¸…æ™°"

# æ˜¾ç¤ºæ¸…ç†åçš„æ ¹ç›®å½•æ–‡ä»¶æ•°é‡
echo "ğŸ“Š æ ¹ç›®å½•æ–‡ä»¶ç»Ÿè®¡:"
echo "   Pythonè„šæœ¬: $(ls -1 *.py 2>/dev/null | wc -l)"
echo "   é…ç½®æ–‡ä»¶: $(ls -1 *.yml *.yaml *.json 2>/dev/null | wc -l)"
echo "   æ–‡æ¡£æ–‡ä»¶: $(ls -1 *.md *.txt 2>/dev/null | wc -l)"
echo "   æ€»æ–‡ä»¶æ•°: $(ls -1 * 2>/dev/null | grep -v ":" | wc -l)"
'''
    
    with open("ç›®å½•æ¸…ç†è„šæœ¬.sh", "w", encoding="utf-8") as f:
        f.write(cleanup_script)
    
    print("ğŸ’¾ æ¸…ç†è„šæœ¬å·²ç”Ÿæˆ: ç›®å½•æ¸…ç†è„šæœ¬.sh")
    
    return cleanup_script

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” ç›®å½•æ¸…ç†åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    
    # 1. åˆ†æç›®å½•ç»“æ„
    file_categories = analyze_directory_structure()
    
    # 2. è¯†åˆ«é‡å¤æ–‡ä»¶
    duplicate_groups = identify_duplicate_files()
    
    # 3. è¯†åˆ«è¿‡æ—¶æ–‡ä»¶
    obsolete_files = identify_obsolete_files()
    
    # 4. åˆ†æç›®å½•å†²çª
    conflicts = analyze_directory_conflicts()
    
    # 5. ç”Ÿæˆæ¸…ç†å»ºè®®
    recommendations = generate_cleanup_recommendations()
    
    # 6. åˆ›å»ºæ¸…ç†è„šæœ¬
    cleanup_script = create_cleanup_script()
    
    # 7. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print(f"\nğŸ“Š æ¸…ç†åˆ†ææ€»ç»“")
    print("=" * 60)
    
    total_files = sum(len(files) for files in file_categories.values())
    duplicate_count = sum(len(files) for files in duplicate_groups.values() if len(files) > 1)
    
    print(f"ğŸ“ æ ¹ç›®å½•æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"ğŸ”„ é‡å¤æ–‡ä»¶ç»„æ•°: {len([g for g in duplicate_groups.values() if len(g) > 1])}")
    print(f"ğŸ—‘ï¸ å¯åˆ é™¤æ–‡ä»¶æ•°: {len(obsolete_files)}")
    print(f"âš ï¸ ç›®å½•å†²çªæ•°: {len(conflicts)}")
    
    print(f"\nğŸ’¡ å»ºè®®æ“ä½œ:")
    print("   1. æ‰§è¡Œ chmod +x ç›®å½•æ¸…ç†è„šæœ¬.sh")
    print("   2. è¿è¡Œ ./ç›®å½•æ¸…ç†è„šæœ¬.sh")
    print("   3. éªŒè¯æ¸…ç†ç»“æœ")
    print("   4. åˆ é™¤å¤‡ä»½ç›®å½•(å¦‚æœç¡®è®¤æ— è¯¯)")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report = {
        "åˆ†ææ—¶é—´": "2025-09-30",
        "æ–‡ä»¶åˆ†ç±»": {k: len(v) for k, v in file_categories.items()},
        "é‡å¤æ–‡ä»¶ç»„": {k: len(v) for k, v in duplicate_groups.items() if len(v) > 1},
        "è¿‡æ—¶æ–‡ä»¶æ•°": len(obsolete_files),
        "ç›®å½•å†²çªæ•°": len(conflicts),
        "æ¸…ç†å»ºè®®": {k: len(v) for k, v in recommendations.items()}
    }
    
    with open("ç›®å½•æ¸…ç†åˆ†ææŠ¥å‘Š.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: ç›®å½•æ¸…ç†åˆ†ææŠ¥å‘Š.json")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
