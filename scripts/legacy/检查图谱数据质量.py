#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
from neo4j import GraphDatabase
from collections import defaultdict, Counter
from pathlib import Path

class GraphDataQualityChecker:
    def __init__(self, uri="bolt://localhost:7687", user="neo4j", password="password123"):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        
    def close(self):
        self.driver.close()
    
    def check_node_duplicates(self):
        """æ£€æŸ¥èŠ‚ç‚¹é‡å¤æƒ…å†µ"""
        print("ğŸ” æ£€æŸ¥èŠ‚ç‚¹é‡å¤æƒ…å†µ")
        print("=" * 60)
        
        with self.driver.session() as session:
            # æ£€æŸ¥åŒåèŠ‚ç‚¹
            duplicate_query = """
            MATCH (n)
            WHERE n:Component OR n:Symptom OR n:Tool OR n:Process OR n:TestCase OR n:Material OR n:Role OR n:Metric
            WITH n.name as name, labels(n)[0] as label, count(n) as count, collect(id(n)) as node_ids
            WHERE count > 1
            RETURN name, label, count, node_ids
            ORDER BY count DESC
            """
            
            result = session.run(duplicate_query)
            duplicates = list(result)
            
            if duplicates:
                print(f"âŒ å‘ç° {len(duplicates)} ç»„é‡å¤èŠ‚ç‚¹:")
                total_duplicate_nodes = 0
                for record in duplicates:
                    name = record['name']
                    label = record['label']
                    count = record['count']
                    node_ids = record['node_ids']
                    total_duplicate_nodes += count - 1  # å‡å»1ä¸ªä¿ç•™çš„
                    print(f"  '{name}' ({label}): {count} ä¸ªé‡å¤ - IDs: {node_ids}")
                
                print(f"\nğŸ“Š é‡å¤ç»Ÿè®¡:")
                print(f"  é‡å¤ç»„æ•°: {len(duplicates)}")
                print(f"  å¤šä½™èŠ‚ç‚¹æ•°: {total_duplicate_nodes}")
                print(f"  å»é‡åé¢„æœŸèŠ‚ç‚¹æ•°: {1350 - total_duplicate_nodes}")
                
                return duplicates, total_duplicate_nodes
            else:
                print("âœ… æ²¡æœ‰å‘ç°é‡å¤èŠ‚ç‚¹")
                return [], 0
    
    def check_dictionary_coverage(self):
        """æ£€æŸ¥è¯å…¸æ•°æ®è¦†ç›–æƒ…å†µ"""
        print("\nğŸ” æ£€æŸ¥è¯å…¸æ•°æ®è¦†ç›–æƒ…å†µ")
        print("=" * 60)
        
        # åŠ è½½è¯å…¸æ•°æ®
        dict_file = Path("api/data/dictionary.json")
        if not dict_file.exists():
            print("âŒ è¯å…¸æ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        with open(dict_file, 'r', encoding='utf-8') as f:
            dictionary_data = json.load(f)
        
        print(f"ğŸ“š è¯å…¸æ•°æ®: {len(dictionary_data)} æ¡")
        
        # ç»Ÿè®¡è¯å…¸ä¸­çš„ç±»åˆ«
        dict_categories = Counter()
        dict_terms = set()
        for item in dictionary_data:
            category = item.get('category', 'Unknown')
            term = item.get('term', '')
            dict_categories[category] += 1
            dict_terms.add(term)
        
        print(f"ğŸ“Š è¯å…¸ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in dict_categories.most_common():
            print(f"  {category}: {count} æ¡")
        
        # æ£€æŸ¥å›¾è°±ä¸­çš„è¦†ç›–æƒ…å†µ
        with self.driver.session() as session:
            graph_categories = Counter()
            graph_terms = set()
            
            for category in dict_categories.keys():
                if category in ['Component', 'Symptom', 'Tool', 'Process', 'TestCase', 'Material', 'Role', 'Metric']:
                    query = f"""
                    MATCH (n:{category})
                    RETURN n.name as name
                    """
                    result = session.run(query)
                    category_terms = [record['name'] for record in result]
                    graph_categories[category] = len(category_terms)
                    graph_terms.update(category_terms)
        
        print(f"\nğŸ“Š å›¾è°±ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in graph_categories.most_common():
            print(f"  {category}: {count} ä¸ªèŠ‚ç‚¹")
        
        # åˆ†æè¦†ç›–æƒ…å†µ
        print(f"\nğŸ“ˆ è¦†ç›–æƒ…å†µåˆ†æ:")
        missing_terms = dict_terms - graph_terms
        extra_terms = graph_terms - dict_terms
        
        print(f"  è¯å…¸æœ¯è¯­æ€»æ•°: {len(dict_terms)}")
        print(f"  å›¾è°±æœ¯è¯­æ€»æ•°: {len(graph_terms)}")
        print(f"  ç¼ºå¤±æœ¯è¯­æ•°: {len(missing_terms)}")
        print(f"  é¢å¤–æœ¯è¯­æ•°: {len(extra_terms)}")
        
        if missing_terms:
            print(f"\nâŒ ç¼ºå¤±çš„æœ¯è¯­ (å‰10ä¸ª):")
            for term in list(missing_terms)[:10]:
                print(f"    - {term}")
        
        if extra_terms:
            print(f"\nâš ï¸ é¢å¤–çš„æœ¯è¯­ (å‰10ä¸ª):")
            for term in list(extra_terms)[:10]:
                print(f"    - {term}")
        
        return missing_terms, extra_terms
    
    def check_relationship_types(self):
        """æ£€æŸ¥å…³ç³»ç±»å‹æƒ…å†µ"""
        print("\nğŸ” æ£€æŸ¥å…³ç³»ç±»å‹æƒ…å†µ")
        print("=" * 60)
        
        with self.driver.session() as session:
            # è·å–æ‰€æœ‰å…³ç³»ç±»å‹
            rel_query = """
            MATCH ()-[r]->()
            RETURN type(r) as rel_type, count(r) as count
            ORDER BY count DESC
            """
            
            result = session.run(rel_query)
            relationships = list(result)
            
            total_rels = sum(record['count'] for record in relationships)
            
            print(f"ğŸ“Š å…³ç³»ç±»å‹ç»Ÿè®¡ (æ€»è®¡ {total_rels} ä¸ªå…³ç³»):")
            for record in relationships:
                rel_type = record['rel_type']
                count = record['count']
                percentage = (count / total_rels * 100) if total_rels > 0 else 0
                print(f"  {rel_type}: {count} ä¸ª ({percentage:.1f}%)")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–ç±»å‹çš„å…³ç³»åº”è¯¥å­˜åœ¨
            print(f"\nğŸ” æ£€æŸ¥é¢„æœŸçš„å…³ç³»ç±»å‹:")
            expected_relations = [
                'HAS_SYMPTOM', 'CAUSES', 'USED_IN', 'TESTS', 'MEASURES',
                'BELONGS_TO', 'RELATED_TO', 'PART_OF', 'REQUIRES'
            ]
            
            existing_types = {record['rel_type'] for record in relationships}
            missing_types = set(expected_relations) - existing_types
            
            if missing_types:
                print(f"âŒ ç¼ºå¤±çš„å…³ç³»ç±»å‹:")
                for rel_type in missing_types:
                    print(f"    - {rel_type}")
            else:
                print(f"âœ… æ‰€æœ‰é¢„æœŸå…³ç³»ç±»å‹éƒ½å­˜åœ¨")
            
            return relationships, missing_types
    
    def analyze_relationship_distribution(self):
        """åˆ†æå…³ç³»åˆ†å¸ƒçš„åˆç†æ€§"""
        print("\nğŸ” åˆ†æå…³ç³»åˆ†å¸ƒåˆç†æ€§")
        print("=" * 60)
        
        with self.driver.session() as session:
            # æ£€æŸ¥å„ç±»åˆ«èŠ‚ç‚¹çš„å…³ç³»æ•°é‡
            categories = ['Component', 'Symptom', 'Tool', 'Process', 'TestCase', 'Material', 'Role', 'Metric']
            
            for category in categories:
                # å‡ºåº¦å…³ç³»
                out_query = f"""
                MATCH (n:{category})-[r]->()
                RETURN count(r) as out_count
                """
                out_result = session.run(out_query)
                out_count = out_result.single()['out_count'] if out_result.single() else 0
                
                # å…¥åº¦å…³ç³»
                in_query = f"""
                MATCH ()-[r]->(n:{category})
                RETURN count(r) as in_count
                """
                in_result = session.run(in_query)
                in_count = in_result.single()['in_count'] if in_result.single() else 0
                
                # èŠ‚ç‚¹æ•°é‡
                node_query = f"""
                MATCH (n:{category})
                RETURN count(n) as node_count
                """
                node_result = session.run(node_query)
                node_count = node_result.single()['node_count'] if node_result.single() else 0
                
                avg_out = out_count / node_count if node_count > 0 else 0
                avg_in = in_count / node_count if node_count > 0 else 0
                
                print(f"  {category} ({node_count} ä¸ªèŠ‚ç‚¹):")
                print(f"    å‡ºåº¦å…³ç³»: {out_count} ä¸ª (å¹³å‡ {avg_out:.1f}/èŠ‚ç‚¹)")
                print(f"    å…¥åº¦å…³ç³»: {in_count} ä¸ª (å¹³å‡ {avg_in:.1f}/èŠ‚ç‚¹)")
    
    def suggest_cleanup_actions(self, duplicates, missing_types):
        """å»ºè®®æ¸…ç†æ“ä½œ"""
        print("\nğŸ’¡ å»ºè®®çš„æ¸…ç†æ“ä½œ")
        print("=" * 60)
        
        actions = []
        
        if duplicates:
            actions.append("1. æ¸…ç†é‡å¤èŠ‚ç‚¹")
            print("1. æ¸…ç†é‡å¤èŠ‚ç‚¹:")
            print("   - ä¿ç•™æ¯ç»„é‡å¤èŠ‚ç‚¹ä¸­çš„ä¸€ä¸ª")
            print("   - åˆå¹¶é‡å¤èŠ‚ç‚¹çš„å…³ç³»")
            print("   - åˆ é™¤å¤šä½™çš„èŠ‚ç‚¹")
        
        if missing_types:
            actions.append("2. æ¢å¤ç¼ºå¤±çš„å…³ç³»ç±»å‹")
            print("2. æ¢å¤ç¼ºå¤±çš„å…³ç³»ç±»å‹:")
            for rel_type in missing_types:
                print(f"   - é‡æ–°åˆ›å»º {rel_type} å…³ç³»")
        
        if len(actions) == 0:
            print("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šæ¸…ç†æ“ä½œ")
        
        return actions

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å›¾è°±æ•°æ®è´¨é‡æ£€æŸ¥")
    print("=" * 80)
    
    checker = GraphDataQualityChecker()
    
    try:
        # 1. æ£€æŸ¥èŠ‚ç‚¹é‡å¤
        duplicates, duplicate_count = checker.check_node_duplicates()
        
        # 2. æ£€æŸ¥è¯å…¸è¦†ç›–
        missing_terms, extra_terms = checker.check_dictionary_coverage()
        
        # 3. æ£€æŸ¥å…³ç³»ç±»å‹
        relationships, missing_rel_types = checker.check_relationship_types()
        
        # 4. åˆ†æå…³ç³»åˆ†å¸ƒ
        checker.analyze_relationship_distribution()
        
        # 5. å»ºè®®æ¸…ç†æ“ä½œ
        actions = checker.suggest_cleanup_actions(duplicates, missing_rel_types)
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        report = {
            'check_time': '2025-09-28',
            'node_analysis': {
                'total_nodes': 1350,
                'expected_from_dict': 1124,
                'duplicates_found': len(duplicates),
                'duplicate_nodes_count': duplicate_count,
                'missing_terms_count': len(missing_terms),
                'extra_terms_count': len(extra_terms)
            },
            'relationship_analysis': {
                'total_relationships': sum(r['count'] for r in relationships),
                'relationship_types': len(relationships),
                'missing_rel_types': list(missing_rel_types)
            },
            'recommended_actions': actions
        }
        
        with open('å›¾è°±æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # 7. æ€»ç»“
        print(f"\n" + "=" * 80)
        print(f"ğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥æ€»ç»“")
        print(f"=" * 80)
        
        print(f"ğŸ”¢ èŠ‚ç‚¹åˆ†æ:")
        print(f"  å½“å‰èŠ‚ç‚¹æ•°: 1350")
        print(f"  è¯å…¸æ•°æ®: 1124 æ¡")
        print(f"  é‡å¤èŠ‚ç‚¹: {duplicate_count} ä¸ª")
        print(f"  é¢„æœŸå»é‡å: {1350 - duplicate_count} ä¸ª")
        
        print(f"\nğŸ”— å…³ç³»åˆ†æ:")
        total_rels = sum(r['count'] for r in relationships)
        print(f"  å½“å‰å…³ç³»æ•°: {total_rels}")
        print(f"  å…³ç³»ç±»å‹æ•°: {len(relationships)}")
        print(f"  ç¼ºå¤±å…³ç³»ç±»å‹: {len(missing_rel_types)} ä¸ª")
        
        if duplicate_count > 0 or missing_rel_types:
            print(f"\nâš ï¸ å‘ç°æ•°æ®è´¨é‡é—®é¢˜ï¼Œå»ºè®®æ‰§è¡Œæ¸…ç†æ“ä½œ")
        else:
            print(f"\nâœ… æ•°æ®è´¨é‡è‰¯å¥½")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: å›¾è°±æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š.json")
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        checker.close()

if __name__ == "__main__":
    main()
